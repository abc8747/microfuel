{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"microfuel","text":"<p>Documentation</p> <p><code>microfuel</code> is a lightweight, end-to-end differentiable surrogate model for aircraft fuel consumption.</p> <p>While the prevalent approach involves using Gradient Boosted Decision Trees (LightGBM/XGBoost) fed by extensive manual feature engineering and external data, they are non-differentiable.</p> <p>We ask the question: How accurate can we be at fuel flow prediction using raw kinematics alone, while maintaining the smoothness required for gradient-based trajectory optimisation?</p>"},{"location":"#performance","title":"Performance","text":"<p>We strongly prioritise simplicity over marginal gains in predictive accuracy derived from exogenous data dependencies.</p> <ul> <li>Inputs: <code>aircraft_type</code>, <code>flight_duration</code> and smoothed time series of {<code>altitude</code>, <code>groundspeed</code>, <code>vertical_rate</code>}.</li> <li>Output: Average fuel burn rate \\(\\dot{m}_f\\) (kg/s) in a specified segment.</li> </ul> Model Parameters Test RMSE Capability <code>microfuel-v1.0-realtime</code> 20,021 238.92 kg Only relies on information within the segment. Suitable for real-time inference. <code>microfuel-v1.0-offline</code> 66,689 222.54 kg \u00b9 Processes the entire flight history. Suitable for high-fidelity post-flight analysis. <p>\u00b9 As of 2025-11-10, this model is ranked #4 out of 164 teams in Phase 1 of the PRC Data Challenge 2025. No changes to the model were made in Phase 2.</p> <p>For a detailed comparison of the architecture and methodology against SOTA models, see the comparison section of our documentation.</p>"},{"location":"#installation","title":"Installation","text":"<p>The repository is currently in a pre-alpha state and not ready for production use. While the ultimate goal is a NumPy-only inference engine for maximum portability, the current training code relies on <code>torch</code> and <code>triton</code>.</p> <p>By the end of December 2025, the code is expected to be stabilised, with model weights released and a PyPI package published.</p> <p>For a step-by-step reproduction guide, refer to the Quickstart section of our documentation.</p> <p>A <code>tangram</code> plugin will also be developed to demonstrate real-time fuel inference from live <code>jet1090</code> data.</p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#microfuel","title":"microfuel","text":""},{"location":"api/#microfuel.PATH_ROOT","title":"PATH_ROOT  <code>module-attribute</code>","text":"<pre><code>PATH_ROOT = Path(__file__).parent.parent.parent\n</code></pre>"},{"location":"api/#microfuel.PATH_DATA","title":"PATH_DATA  <code>module-attribute</code>","text":"<pre><code>PATH_DATA = PATH_ROOT / 'data'\n</code></pre>"},{"location":"api/#microfuel.PATH_DATA_RAW","title":"PATH_DATA_RAW  <code>module-attribute</code>","text":"<pre><code>PATH_DATA_RAW = PATH_DATA / 'raw'\n</code></pre>"},{"location":"api/#microfuel.PATH_PREPROCESSED","title":"PATH_PREPROCESSED  <code>module-attribute</code>","text":"<pre><code>PATH_PREPROCESSED = PATH_DATA / 'preprocessed'\n</code></pre>"},{"location":"api/#microfuel.PATH_PLOTS_OUTPUT","title":"PATH_PLOTS_OUTPUT  <code>module-attribute</code>","text":"<pre><code>PATH_PLOTS_OUTPUT = PATH_DATA / 'plots'\n</code></pre>"},{"location":"api/#microfuel.PATH_CHECKPOINTS","title":"PATH_CHECKPOINTS  <code>module-attribute</code>","text":"<pre><code>PATH_CHECKPOINTS = PATH_DATA / 'checkpoints'\n</code></pre>"},{"location":"api/#microfuel.PATH_PREDICTIONS","title":"PATH_PREDICTIONS  <code>module-attribute</code>","text":"<pre><code>PATH_PREDICTIONS = PATH_DATA / 'predictions'\n</code></pre>"},{"location":"api/#microfuel.FlightId","title":"FlightId  <code>module-attribute</code>","text":"<pre><code>FlightId: TypeAlias = str\n</code></pre> <p>Unique flight identifier: <code>prc_{}</code></p>"},{"location":"api/#microfuel.SegmentId","title":"SegmentId  <code>module-attribute</code>","text":"<pre><code>SegmentId: TypeAlias = int\n</code></pre>"},{"location":"api/#microfuel.AircraftType","title":"AircraftType  <code>module-attribute</code>","text":"<pre><code>AircraftType: TypeAlias = Literal[\n    \"A20N\",\n    \"A320\",\n    \"A359\",\n    \"B788\",\n    \"B738\",\n    \"A332\",\n    \"A21N\",\n    \"A321\",\n    \"B789\",\n    \"B77W\",\n    \"A333\",\n    \"B772\",\n    \"B744\",\n    \"B737\",\n    \"B739\",\n    \"B38M\",\n    \"A319\",\n    \"A306\",\n    \"A388\",\n    \"B752\",\n    \"B748\",\n    \"B77L\",\n    \"B763\",\n    \"MD11\",\n    \"B39M\",\n    \"A318\",\n]\n</code></pre>"},{"location":"api/#microfuel.AIRCRAFT_TYPES","title":"AIRCRAFT_TYPES  <code>module-attribute</code>","text":"<pre><code>AIRCRAFT_TYPES: tuple[AircraftType, ...] = get_args(\n    AircraftType\n)\n</code></pre>"},{"location":"api/#microfuel.AirportIcao","title":"AirportIcao  <code>module-attribute</code>","text":"<pre><code>AirportIcao: TypeAlias = str\n</code></pre>"},{"location":"api/#microfuel.Partition","title":"Partition  <code>module-attribute</code>","text":"<pre><code>Partition: TypeAlias = Literal[\n    \"phase1\", \"phase1_rank\", \"phase2_rank\"\n]\n</code></pre>"},{"location":"api/#microfuel.Split","title":"Split  <code>module-attribute</code>","text":"<pre><code>Split: TypeAlias = Literal['train', 'validation']\n</code></pre>"},{"location":"api/#microfuel.SPLITS","title":"SPLITS  <code>module-attribute</code>","text":"<pre><code>SPLITS: tuple[Split, ...] = get_args(Split)\n</code></pre>"},{"location":"api/#microfuel.deg2rad","title":"deg2rad  <code>module-attribute</code>","text":"<pre><code>deg2rad = isqx.convert(isqx.DEG, isqx.RAD)\n</code></pre>"},{"location":"api/#microfuel.ft2m","title":"ft2m  <code>module-attribute</code>","text":"<pre><code>ft2m = isqx.convert(isqx.usc.FT, isqx.M)\n</code></pre>"},{"location":"api/#microfuel.knot2mps","title":"knot2mps  <code>module-attribute</code>","text":"<pre><code>knot2mps = isqx.convert(isqx.usc.KNOT, isqx.M_PERS)\n</code></pre>"},{"location":"api/#microfuel.fpm2mps","title":"fpm2mps  <code>module-attribute</code>","text":"<pre><code>fpm2mps = isqx.convert(\n    isqx.usc.FT * isqx.MIN**-1, isqx.M_PERS\n)\n</code></pre>"},{"location":"api/#microfuel.Coordinate2D","title":"Coordinate2D","text":"<p>               Bases: <code>NamedTuple</code>, <code>Generic[_T]</code></p> Source code in <code>src/microfuel/__init__.py</code> <pre><code>class Coordinate2D(NamedTuple, Generic[_T]):\n    lng: Annotated[_T, isqx.LONGITUDE(isqx.DEG)]\n    lat: Annotated[_T, isqx.LATITUDE(isqx.DEG)]\n</code></pre>"},{"location":"api/#microfuel.Coordinate2D.lng","title":"lng  <code>instance-attribute</code>","text":"<pre><code>lng: Annotated[_T, isqx.LONGITUDE(isqx.DEG)]\n</code></pre>"},{"location":"api/#microfuel.Coordinate2D.lat","title":"lat  <code>instance-attribute</code>","text":"<pre><code>lat: Annotated[_T, isqx.LATITUDE(isqx.DEG)]\n</code></pre>"},{"location":"api/#microfuel.dataloader","title":"dataloader","text":""},{"location":"api/#microfuel.dataloader.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(__name__)\n</code></pre>"},{"location":"api/#microfuel.dataloader.SequenceInfo","title":"SequenceInfo  <code>module-attribute</code>","text":"<pre><code>SequenceInfo = namedtuple(\n    \"SequenceInfo\",\n    [\n        \"flight_indices\",\n        \"segment_indices_relative\",\n        \"target\",\n        \"segment_id\",\n        \"aircraft_type_idx\",\n        \"duration_s\",\n        \"flight_id\",\n    ],\n)\n</code></pre>"},{"location":"api/#microfuel.dataloader.Sequence","title":"Sequence  <code>module-attribute</code>","text":"<pre><code>Sequence = namedtuple(\n    \"Sequence\",\n    [\n        \"features_flight\",\n        \"features_segment\",\n        \"target\",\n        \"segment_id\",\n        \"aircraft_type_idx\",\n        \"duration_s\",\n        \"flight_id\",\n    ],\n)\n</code></pre>"},{"location":"api/#microfuel.dataloader.VarlenBatch","title":"VarlenBatch  <code>module-attribute</code>","text":"<pre><code>VarlenBatch = namedtuple(\n    \"VarlenBatch\",\n    [\n        \"x_flight\",\n        \"cu_seqlens_flight\",\n        \"x_segment\",\n        \"cu_seqlens_segment\",\n        \"y\",\n        \"segment_ids\",\n        \"aircraft_type_idx\",\n        \"durations\",\n        \"flight_ids\",\n    ],\n)\n</code></pre>"},{"location":"api/#microfuel.dataloader.AC_TYPE_ALIASES","title":"AC_TYPE_ALIASES  <code>module-attribute</code>","text":"<pre><code>AC_TYPE_ALIASES = {'B734': 'B737'}\n</code></pre>"},{"location":"api/#microfuel.dataloader.VarlenDataset","title":"VarlenDataset","text":"<p>               Bases: <code>Dataset</code></p> Source code in <code>src/microfuel/dataloader.py</code> <pre><code>class VarlenDataset(Dataset):\n    def __init__(\n        self,\n        partition: Partition,\n        split: Split | None,\n        source_partitions: list[Partition] | None = None,\n    ):\n        if split:\n            splits = preprocessed.load_splits(partition)\n            segment_ids = splits[split]\n            flight_ids = (\n                raw.scan_fuel(partition)\n                .filter(pl.col(\"idx\").is_in(segment_ids))\n                .select(\"flight_id\")\n                .unique()\n                .collect()[\"flight_id\"]\n                .to_list()\n            )\n        else:\n            segment_ids = None\n            flight_ids = (\n                raw.scan_fuel(partition)\n                .select(\"flight_id\")\n                .unique()\n                .collect()[\"flight_id\"]\n                .to_list()\n            )\n\n        # always get train stats for submission\n        self.stats = preprocessed.load_standardisation_stats(\"phase1\")\n        self.ac_type_vocab = {ac_type: i for i, ac_type in enumerate(AIRCRAFT_TYPES)}\n\n        self.all_features, self.sequences = _prepare_tensors(\n            partition, flight_ids, segment_ids, self.stats, self.ac_type_vocab, source_partitions\n        )\n\n        counts = Counter(s.aircraft_type_idx for s in self.sequences)\n        self.class_counts = torch.tensor([counts[i] for i in range(len(self.ac_type_vocab))])\n\n    def __len__(self) -&gt; int:\n        return len(self.sequences)\n\n    def __getitem__(self, idx: int) -&gt; Sequence:\n        seq_info = self.sequences[idx]\n        flight_start_abs, flight_end_abs = seq_info.flight_indices\n        segment_start_rel, segment_end_rel = seq_info.segment_indices_relative\n\n        features_flight = self.all_features[flight_start_abs:flight_end_abs]\n        features_segment = features_flight[segment_start_rel:segment_end_rel]\n\n        return Sequence(\n            features_flight=features_flight,\n            features_segment=features_segment,\n            target=seq_info.target,\n            segment_id=seq_info.segment_id,\n            aircraft_type_idx=seq_info.aircraft_type_idx,\n            duration_s=seq_info.duration_s,\n            flight_id=seq_info.flight_id,\n        )\n</code></pre>"},{"location":"api/#microfuel.dataloader.VarlenDataset.stats","title":"stats  <code>instance-attribute</code>","text":"<pre><code>stats = preprocessed.load_standardisation_stats('phase1')\n</code></pre>"},{"location":"api/#microfuel.dataloader.VarlenDataset.ac_type_vocab","title":"ac_type_vocab  <code>instance-attribute</code>","text":"<pre><code>ac_type_vocab = {\n    ac_type: i\n    for i, ac_type in (enumerate(AIRCRAFT_TYPES))\n}\n</code></pre>"},{"location":"api/#microfuel.dataloader.VarlenDataset.class_counts","title":"class_counts  <code>instance-attribute</code>","text":"<pre><code>class_counts = torch.tensor(\n    [\n        (counts[i])\n        for i in (range(len(self.ac_type_vocab)))\n    ]\n)\n</code></pre>"},{"location":"api/#microfuel.dataloader.VarlenDataset.__init__","title":"__init__","text":"<pre><code>__init__(\n    partition: Partition,\n    split: Split | None,\n    source_partitions: list[Partition] | None = None,\n)\n</code></pre> Source code in <code>src/microfuel/dataloader.py</code> <pre><code>def __init__(\n    self,\n    partition: Partition,\n    split: Split | None,\n    source_partitions: list[Partition] | None = None,\n):\n    if split:\n        splits = preprocessed.load_splits(partition)\n        segment_ids = splits[split]\n        flight_ids = (\n            raw.scan_fuel(partition)\n            .filter(pl.col(\"idx\").is_in(segment_ids))\n            .select(\"flight_id\")\n            .unique()\n            .collect()[\"flight_id\"]\n            .to_list()\n        )\n    else:\n        segment_ids = None\n        flight_ids = (\n            raw.scan_fuel(partition)\n            .select(\"flight_id\")\n            .unique()\n            .collect()[\"flight_id\"]\n            .to_list()\n        )\n\n    # always get train stats for submission\n    self.stats = preprocessed.load_standardisation_stats(\"phase1\")\n    self.ac_type_vocab = {ac_type: i for i, ac_type in enumerate(AIRCRAFT_TYPES)}\n\n    self.all_features, self.sequences = _prepare_tensors(\n        partition, flight_ids, segment_ids, self.stats, self.ac_type_vocab, source_partitions\n    )\n\n    counts = Counter(s.aircraft_type_idx for s in self.sequences)\n    self.class_counts = torch.tensor([counts[i] for i in range(len(self.ac_type_vocab))])\n</code></pre>"},{"location":"api/#microfuel.dataloader.VarlenDataset.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> Source code in <code>src/microfuel/dataloader.py</code> <pre><code>def __len__(self) -&gt; int:\n    return len(self.sequences)\n</code></pre>"},{"location":"api/#microfuel.dataloader.VarlenDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx: int) -&gt; Sequence\n</code></pre> Source code in <code>src/microfuel/dataloader.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; Sequence:\n    seq_info = self.sequences[idx]\n    flight_start_abs, flight_end_abs = seq_info.flight_indices\n    segment_start_rel, segment_end_rel = seq_info.segment_indices_relative\n\n    features_flight = self.all_features[flight_start_abs:flight_end_abs]\n    features_segment = features_flight[segment_start_rel:segment_end_rel]\n\n    return Sequence(\n        features_flight=features_flight,\n        features_segment=features_segment,\n        target=seq_info.target,\n        segment_id=seq_info.segment_id,\n        aircraft_type_idx=seq_info.aircraft_type_idx,\n        duration_s=seq_info.duration_s,\n        flight_id=seq_info.flight_id,\n    )\n</code></pre>"},{"location":"api/#microfuel.dataloader.collate_fn","title":"collate_fn","text":"<pre><code>collate_fn(batch_sequences: list[Sequence]) -&gt; VarlenBatch\n</code></pre> Source code in <code>src/microfuel/dataloader.py</code> <pre><code>def collate_fn(batch_sequences: list[Sequence]) -&gt; VarlenBatch:\n    lengths_flight = [len(seq.features_flight) for seq in batch_sequences]\n    lengths_segment = [len(seq.features_segment) for seq in batch_sequences]\n\n    x_flight = torch.cat([seq.features_flight for seq in batch_sequences], dim=0)\n    x_segment = torch.cat([seq.features_segment for seq in batch_sequences], dim=0)\n    y = torch.tensor([seq.target for seq in batch_sequences], dtype=torch.float32).unsqueeze(1)\n\n    cu_seqlens_flight = torch.from_numpy(np.cumsum([0, *lengths_flight], dtype=np.int32))\n    cu_seqlens_segment = torch.from_numpy(np.cumsum([0, *lengths_segment], dtype=np.int32))\n\n    segment_ids = torch.tensor([seq.segment_id for seq in batch_sequences], dtype=torch.int32)\n    aircraft_type_idx = torch.tensor(\n        [seq.aircraft_type_idx for seq in batch_sequences], dtype=torch.long\n    )\n    durations = torch.tensor([seq.duration_s for seq in batch_sequences], dtype=torch.float32)\n    flight_ids = [seq.flight_id for seq in batch_sequences]\n\n    return VarlenBatch(\n        x_flight=x_flight,\n        cu_seqlens_flight=cu_seqlens_flight,\n        x_segment=x_segment,\n        cu_seqlens_segment=cu_seqlens_segment,\n        y=y,\n        segment_ids=segment_ids,\n        aircraft_type_idx=aircraft_type_idx,\n        durations=durations,\n        flight_ids=flight_ids,\n    )\n</code></pre>"},{"location":"api/#microfuel.datasets","title":"datasets","text":""},{"location":"api/#microfuel.datasets.preprocessed","title":"preprocessed","text":""},{"location":"api/#microfuel.datasets.preprocessed.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(__name__)\n</code></pre>"},{"location":"api/#microfuel.datasets.preprocessed.CoreFeature","title":"CoreFeature  <code>module-attribute</code>","text":"<pre><code>CoreFeature = Literal[\n    \"altitude\", \"groundspeed\", \"vertical_rate\"\n]\n</code></pre>"},{"location":"api/#microfuel.datasets.preprocessed.WEATHER_FEATURES","title":"WEATHER_FEATURES  <code>module-attribute</code>","text":"<pre><code>WEATHER_FEATURES = []\n</code></pre>"},{"location":"api/#microfuel.datasets.preprocessed.StateFeature","title":"StateFeature  <code>module-attribute</code>","text":"<pre><code>StateFeature = Literal[CoreFeature]\n</code></pre>"},{"location":"api/#microfuel.datasets.preprocessed.STATE_FEATURES","title":"STATE_FEATURES  <code>module-attribute</code>","text":"<pre><code>STATE_FEATURES = get_args(StateFeature)\n</code></pre>"},{"location":"api/#microfuel.datasets.preprocessed.FlightFeature","title":"FlightFeature  <code>module-attribute</code>","text":"<pre><code>FlightFeature = Literal[\n    \"flight_progress\", \"flight_duration\"\n]\n</code></pre>"},{"location":"api/#microfuel.datasets.preprocessed.FLIGHT_FEATURES","title":"FLIGHT_FEATURES  <code>module-attribute</code>","text":"<pre><code>FLIGHT_FEATURES = get_args(FlightFeature)\n</code></pre>"},{"location":"api/#microfuel.datasets.preprocessed.MODEL_INPUT_FEATURES","title":"MODEL_INPUT_FEATURES  <code>module-attribute</code>","text":"<pre><code>MODEL_INPUT_FEATURES: list[str] = [\n    *FLIGHT_FEATURES,\n    *STATE_FEATURES,\n]\n</code></pre>"},{"location":"api/#microfuel.datasets.preprocessed.SmoothResult","title":"SmoothResult  <code>module-attribute</code>","text":"<pre><code>SmoothResult = namedtuple(\n    \"SmoothResult\", [\"val\", \"val_d\", \"var_val\", \"var_val_d\"]\n)\n</code></pre>"},{"location":"api/#microfuel.datasets.preprocessed.Stats","title":"Stats  <code>module-attribute</code>","text":"<pre><code>Stats: TypeAlias = dict[StateFeature | FlightFeature, Stat]\n</code></pre>"},{"location":"api/#microfuel.datasets.preprocessed.Stat","title":"Stat","text":"<p>               Bases: <code>TypedDict</code></p> Source code in <code>src/microfuel/datasets/preprocessed.py</code> <pre><code>class Stat(TypedDict):\n    mean: float\n    std: float\n</code></pre>"},{"location":"api/#microfuel.datasets.preprocessed.Stat.mean","title":"mean  <code>instance-attribute</code>","text":"<pre><code>mean: float\n</code></pre>"},{"location":"api/#microfuel.datasets.preprocessed.Stat.std","title":"std  <code>instance-attribute</code>","text":"<pre><code>std: float\n</code></pre>"},{"location":"api/#microfuel.datasets.preprocessed.IteratorData","title":"IteratorData","text":"<p>               Bases: <code>NamedTuple</code></p> Source code in <code>src/microfuel/datasets/preprocessed.py</code> <pre><code>class IteratorData(NamedTuple):\n    segments_df: pl.DataFrame\n    traj_lf: pl.LazyFrame\n</code></pre>"},{"location":"api/#microfuel.datasets.preprocessed.IteratorData.segments_df","title":"segments_df  <code>instance-attribute</code>","text":"<pre><code>segments_df: pl.DataFrame\n</code></pre>"},{"location":"api/#microfuel.datasets.preprocessed.IteratorData.traj_lf","title":"traj_lf  <code>instance-attribute</code>","text":"<pre><code>traj_lf: pl.LazyFrame\n</code></pre>"},{"location":"api/#microfuel.datasets.preprocessed.TrajectoryInfo","title":"TrajectoryInfo","text":"<p>               Bases: <code>TypedDict</code></p> Source code in <code>src/microfuel/datasets/preprocessed.py</code> <pre><code>class TrajectoryInfo(TypedDict):\n    idx: int\n    flight_id: str\n    start: datetime\n    end: datetime\n    fuel_kg: float\n    takeoff: datetime\n    landed: datetime\n    aircraft_type: str\n</code></pre>"},{"location":"api/#microfuel.datasets.preprocessed.TrajectoryInfo.idx","title":"idx  <code>instance-attribute</code>","text":"<pre><code>idx: int\n</code></pre>"},{"location":"api/#microfuel.datasets.preprocessed.TrajectoryInfo.flight_id","title":"flight_id  <code>instance-attribute</code>","text":"<pre><code>flight_id: str\n</code></pre>"},{"location":"api/#microfuel.datasets.preprocessed.TrajectoryInfo.start","title":"start  <code>instance-attribute</code>","text":"<pre><code>start: datetime\n</code></pre>"},{"location":"api/#microfuel.datasets.preprocessed.TrajectoryInfo.end","title":"end  <code>instance-attribute</code>","text":"<pre><code>end: datetime\n</code></pre>"},{"location":"api/#microfuel.datasets.preprocessed.TrajectoryInfo.fuel_kg","title":"fuel_kg  <code>instance-attribute</code>","text":"<pre><code>fuel_kg: float\n</code></pre>"},{"location":"api/#microfuel.datasets.preprocessed.TrajectoryInfo.takeoff","title":"takeoff  <code>instance-attribute</code>","text":"<pre><code>takeoff: datetime\n</code></pre>"},{"location":"api/#microfuel.datasets.preprocessed.TrajectoryInfo.landed","title":"landed  <code>instance-attribute</code>","text":"<pre><code>landed: datetime\n</code></pre>"},{"location":"api/#microfuel.datasets.preprocessed.TrajectoryInfo.aircraft_type","title":"aircraft_type  <code>instance-attribute</code>","text":"<pre><code>aircraft_type: str\n</code></pre>"},{"location":"api/#microfuel.datasets.preprocessed.Trajectory","title":"Trajectory","text":"<p>               Bases: <code>NamedTuple</code></p> Source code in <code>src/microfuel/datasets/preprocessed.py</code> <pre><code>class Trajectory(NamedTuple):\n    features_df: pl.DataFrame\n    info: TrajectoryInfo\n</code></pre>"},{"location":"api/#microfuel.datasets.preprocessed.Trajectory.features_df","title":"features_df  <code>instance-attribute</code>","text":"<pre><code>features_df: pl.DataFrame\n</code></pre>"},{"location":"api/#microfuel.datasets.preprocessed.Trajectory.info","title":"info  <code>instance-attribute</code>","text":"<pre><code>info: TrajectoryInfo\n</code></pre>"},{"location":"api/#microfuel.datasets.preprocessed.TrajectoryIterator","title":"TrajectoryIterator","text":"<p>Yields the entire flight trajectory for each segment as polars DataFrames.</p> Source code in <code>src/microfuel/datasets/preprocessed.py</code> <pre><code>class TrajectoryIterator:\n    \"\"\"Yields the entire flight trajectory for each segment as polars DataFrames.\"\"\"\n\n    def __init__(\n        self,\n        partition: Partition,\n        *,\n        segment_ids: Collection[SegmentId] | None = None,\n        shuffle_seed: int | None = None,\n        stats: Stats | None = None,\n        start_to_end_only: bool = False,\n        path_base: Path = PATH_PREPROCESSED,\n    ):\n        \"\"\"\n        :param start_to_end_only: if False, yields the entire materialised flight trajectory.\n            Note that collecting this iterator will use a lot of memory due to duplicates!\n            Prefer using the torch iterator instead.\n        \"\"\"\n        it_data = prepare_iterator_data(partition, segment_ids, stats, path_base)\n        self.traj_lf = it_data.traj_lf\n        self.segment_infos: list[TrajectoryInfo] = it_data.segments_df.to_dicts()  # type: ignore\n        self.start_to_end_only = start_to_end_only\n        self.stats = stats\n\n        self.segments_by_flight: dict[FlightId, list[TrajectoryInfo]] = {}\n        for info in self.segment_infos:\n            self.segments_by_flight.setdefault(info[\"flight_id\"], []).append(info)\n\n        self.flight_ids_to_iterate = list(self.segments_by_flight.keys())\n        if shuffle_seed is not None:\n            rng = np.random.default_rng(shuffle_seed)\n            rng.shuffle(self.flight_ids_to_iterate)\n\n    def __len__(self) -&gt; int:\n        return len(self.segment_infos)\n\n    def __iter__(self) -&gt; Iterator[Trajectory]:\n        for flight_id in self.flight_ids_to_iterate:\n            flight_traj_df = self.traj_lf.filter(pl.col(\"flight_id\") == flight_id).collect()\n\n            for segment_info in self.segments_by_flight[flight_id]:\n                if self.start_to_end_only:\n                    start_ts = segment_info[\"start\"]\n                    end_ts = segment_info[\"end\"]\n\n                    start_idx, end_idx = find_segment_indices(\n                        flight_traj_df[\"timestamp\"].to_numpy(),\n                        np.datetime64(start_ts.isoformat()),\n                        np.datetime64(end_ts.isoformat()),\n                        xp=np,\n                    )\n                    segment_traj_df = flight_traj_df[start_idx:end_idx]\n                    if segment_traj_df.height &lt; 2:\n                        logger.error(\n                            f\"skipping {flight_id}: found &lt; 2 datapoints for segment \"\n                            f\"({start_ts} - {end_ts}): {start_idx}..={end_idx}\"\n                        )\n                        continue\n                else:\n                    segment_traj_df = flight_traj_df\n\n                yield Trajectory(\n                    features_df=segment_traj_df,\n                    info=segment_info,\n                )\n</code></pre>"},{"location":"api/#microfuel.datasets.preprocessed.TrajectoryIterator.traj_lf","title":"traj_lf  <code>instance-attribute</code>","text":"<pre><code>traj_lf = it_data.traj_lf\n</code></pre>"},{"location":"api/#microfuel.datasets.preprocessed.TrajectoryIterator.segment_infos","title":"segment_infos  <code>instance-attribute</code>","text":"<pre><code>segment_infos: list[TrajectoryInfo] = (\n    it_data.segments_df.to_dicts()\n)\n</code></pre>"},{"location":"api/#microfuel.datasets.preprocessed.TrajectoryIterator.start_to_end_only","title":"start_to_end_only  <code>instance-attribute</code>","text":"<pre><code>start_to_end_only = start_to_end_only\n</code></pre>"},{"location":"api/#microfuel.datasets.preprocessed.TrajectoryIterator.stats","title":"stats  <code>instance-attribute</code>","text":"<pre><code>stats = stats\n</code></pre>"},{"location":"api/#microfuel.datasets.preprocessed.TrajectoryIterator.segments_by_flight","title":"segments_by_flight  <code>instance-attribute</code>","text":"<pre><code>segments_by_flight: dict[\n    FlightId, list[TrajectoryInfo]\n] = {}\n</code></pre>"},{"location":"api/#microfuel.datasets.preprocessed.TrajectoryIterator.flight_ids_to_iterate","title":"flight_ids_to_iterate  <code>instance-attribute</code>","text":"<pre><code>flight_ids_to_iterate = list(self.segments_by_flight.keys())\n</code></pre>"},{"location":"api/#microfuel.datasets.preprocessed.TrajectoryIterator.__init__","title":"__init__","text":"<pre><code>__init__(\n    partition: Partition,\n    *,\n    segment_ids: Collection[SegmentId] | None = None,\n    shuffle_seed: int | None = None,\n    stats: Stats | None = None,\n    start_to_end_only: bool = False,\n    path_base: Path = PATH_PREPROCESSED,\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>start_to_end_only</code> <code>bool</code> <p>if False, yields the entire materialised flight trajectory. Note that collecting this iterator will use a lot of memory due to duplicates! Prefer using the torch iterator instead.</p> <code>False</code> Source code in <code>src/microfuel/datasets/preprocessed.py</code> <pre><code>def __init__(\n    self,\n    partition: Partition,\n    *,\n    segment_ids: Collection[SegmentId] | None = None,\n    shuffle_seed: int | None = None,\n    stats: Stats | None = None,\n    start_to_end_only: bool = False,\n    path_base: Path = PATH_PREPROCESSED,\n):\n    \"\"\"\n    :param start_to_end_only: if False, yields the entire materialised flight trajectory.\n        Note that collecting this iterator will use a lot of memory due to duplicates!\n        Prefer using the torch iterator instead.\n    \"\"\"\n    it_data = prepare_iterator_data(partition, segment_ids, stats, path_base)\n    self.traj_lf = it_data.traj_lf\n    self.segment_infos: list[TrajectoryInfo] = it_data.segments_df.to_dicts()  # type: ignore\n    self.start_to_end_only = start_to_end_only\n    self.stats = stats\n\n    self.segments_by_flight: dict[FlightId, list[TrajectoryInfo]] = {}\n    for info in self.segment_infos:\n        self.segments_by_flight.setdefault(info[\"flight_id\"], []).append(info)\n\n    self.flight_ids_to_iterate = list(self.segments_by_flight.keys())\n    if shuffle_seed is not None:\n        rng = np.random.default_rng(shuffle_seed)\n        rng.shuffle(self.flight_ids_to_iterate)\n</code></pre>"},{"location":"api/#microfuel.datasets.preprocessed.TrajectoryIterator.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> Source code in <code>src/microfuel/datasets/preprocessed.py</code> <pre><code>def __len__(self) -&gt; int:\n    return len(self.segment_infos)\n</code></pre>"},{"location":"api/#microfuel.datasets.preprocessed.TrajectoryIterator.__iter__","title":"__iter__","text":"<pre><code>__iter__() -&gt; Iterator[Trajectory]\n</code></pre> Source code in <code>src/microfuel/datasets/preprocessed.py</code> <pre><code>def __iter__(self) -&gt; Iterator[Trajectory]:\n    for flight_id in self.flight_ids_to_iterate:\n        flight_traj_df = self.traj_lf.filter(pl.col(\"flight_id\") == flight_id).collect()\n\n        for segment_info in self.segments_by_flight[flight_id]:\n            if self.start_to_end_only:\n                start_ts = segment_info[\"start\"]\n                end_ts = segment_info[\"end\"]\n\n                start_idx, end_idx = find_segment_indices(\n                    flight_traj_df[\"timestamp\"].to_numpy(),\n                    np.datetime64(start_ts.isoformat()),\n                    np.datetime64(end_ts.isoformat()),\n                    xp=np,\n                )\n                segment_traj_df = flight_traj_df[start_idx:end_idx]\n                if segment_traj_df.height &lt; 2:\n                    logger.error(\n                        f\"skipping {flight_id}: found &lt; 2 datapoints for segment \"\n                        f\"({start_ts} - {end_ts}): {start_idx}..={end_idx}\"\n                    )\n                    continue\n            else:\n                segment_traj_df = flight_traj_df\n\n            yield Trajectory(\n                features_df=segment_traj_df,\n                info=segment_info,\n            )\n</code></pre>"},{"location":"api/#microfuel.datasets.preprocessed.make_splits","title":"make_splits","text":"<pre><code>make_splits(\n    partition: Partition,\n    train_split: float = 0.8,\n    seed: int = 13,\n    *,\n    path_base: Path = PATH_PREPROCESSED,\n    max_bins: int = 30,\n    min_samples_for_binning: int = 2,\n)\n</code></pre> Source code in <code>src/microfuel/datasets/preprocessed.py</code> <pre><code>def make_splits(\n    partition: Partition,\n    train_split: float = 0.8,\n    seed: int = 13,\n    *,\n    path_base: Path = PATH_PREPROCESSED,\n    max_bins: int = 30,\n    min_samples_for_binning: int = 2,\n):  # TODO: allow k fold stratified splits\n    path_base.mkdir(exist_ok=True, parents=True)\n    flight_list_lf = raw.scan_flight_list(partition)\n    fuel_lf = raw.scan_fuel(partition)\n\n    segments_df = (\n        fuel_lf.with_columns(\n            (pl.col(\"end\") - pl.col(\"start\")).dt.total_seconds().alias(\"duration_s\")\n        )\n        .join(flight_list_lf.select(\"flight_id\", \"aircraft_type\"), on=\"flight_id\")\n        .select(\"idx\", \"aircraft_type\", \"duration_s\")\n        .sort(\"idx\")\n        .collect()\n    )\n    logger.info(f\"found {len(segments_df)} segments with fuel data in `{partition}`\")\n\n    bin_boundaries_data = []\n\n    def add_duration_bin(group_df: pl.DataFrame) -&gt; pl.DataFrame:\n        ac_type = group_df[\"aircraft_type\"][0]\n        duration_series = group_df[\"duration_s\"]\n        n_samples = duration_series.len()\n        assert n_samples &gt; 0\n\n        duration_quantiles = (\n            min(max_bins, int(1 + np.log2(n_samples)))\n            if n_samples &gt;= min_samples_for_binning\n            else 1\n        )\n\n        quantile_points = np.linspace(0, 1, duration_quantiles + 1)\n        breaks_set: set[float] = set()\n        for q in quantile_points:\n            b = duration_series.quantile(q, interpolation=\"linear\")\n            assert b is not None\n            breaks_set.add(b)\n        breaks = sorted(breaks_set)\n\n        if len(breaks) &lt; 2:\n            min_val = float(duration_series.min() or 0)  # type: ignore\n            max_val = float(duration_series.max() or 1)  # type: ignore\n            breaks = [min_val, max_val] if min_val != max_val else [min_val, min_val + 1]\n\n        # last break should be inclusive of the max value\n        max_dur = float(duration_series.max())  # type: ignore\n        if max_dur is not None and breaks[-1] &lt; max_dur:\n            breaks[-1] = max_dur\n\n        labels = [f\"d_q{i}\" for i in range(len(breaks) - 1)]\n        for i, label in enumerate(labels):\n            bin_boundaries_data.append(\n                {\n                    \"aircraft_type\": ac_type,\n                    \"duration_bin\": label,\n                    \"lower_bound\": breaks[i],\n                    \"upper_bound\": breaks[i + 1],\n                }\n            )\n\n        bin_expr = pl.when(pl.col(\"duration_s\") &lt;= breaks[1]).then(pl.lit(labels[0]))\n        for i in range(2, len(breaks) - 1):\n            bin_expr = bin_expr.when(pl.col(\"duration_s\") &lt;= breaks[i]).then(pl.lit(labels[i - 1]))\n        bin_expr = bin_expr.otherwise(pl.lit(labels[-1]))\n\n        return group_df.with_columns(bin_expr.alias(\"duration_bin\"))\n\n    segments_binned_df = segments_df.group_by(\"aircraft_type\", maintain_order=True).map_groups(\n        add_duration_bin\n    )\n    bin_boundaries_df = pl.DataFrame(bin_boundaries_data)\n\n    stratify_cols = [\"aircraft_type\", \"duration_bin\"]\n    n_train_samples_expr = pl.max_horizontal(1, (pl.count() * train_split).floor())\n    train_df = segments_binned_df.filter(\n        pl.int_range(0, pl.count()).shuffle(seed=seed).over(stratify_cols)\n        &lt; n_train_samples_expr.over(stratify_cols)\n    )\n\n    train_segment_ids_set = set(train_df[\"idx\"].to_list())\n    all_segment_ids_set = set(segments_binned_df[\"idx\"].to_list())\n    validation_segment_ids_set = all_segment_ids_set - train_segment_ids_set\n\n    segment_ids_train = sorted(list(train_segment_ids_set))\n    segment_ids_validation = sorted(list(validation_segment_ids_set))\n\n    logger.info(\n        f\"stratified split by {stratify_cols}: {len(segment_ids_train)} train, {len(segment_ids_validation)} validation\"\n    )\n\n    train_counts_df = train_df.group_by(stratify_cols).len().rename({\"len\": \"train_count\"})\n    validation_df = segments_binned_df.filter(pl.col(\"idx\").is_in(validation_segment_ids_set))\n    validation_counts_df = (\n        validation_df.group_by(stratify_cols).len().rename({\"len\": \"validation_count\"})\n    )\n\n    all_groups_df = segments_binned_df.select(stratify_cols).unique().sort(stratify_cols)\n    combined_counts_df = (\n        all_groups_df.join(train_counts_df, on=stratify_cols, how=\"left\")\n        .join(validation_counts_df, on=stratify_cols, how=\"left\")\n        .fill_null(0)\n    )\n\n    logging_df = combined_counts_df.join(bin_boundaries_df, on=stratify_cols, how=\"left\")\n\n    logger.info(\"split counts by stratification groups:\")\n    _ac_types: set[str] = set()\n    for row in logging_df.sort([\"aircraft_type\", \"duration_bin\"]).iter_rows(named=True):\n        ac_type = t if (t := row[\"aircraft_type\"]) not in _ac_types else \"\"\n        _ac_types.add(row[\"aircraft_type\"])\n        lower = row[\"lower_bound\"]\n        upper = row[\"upper_bound\"]\n        train_count = row[\"train_count\"]\n        validation_count = row[\"validation_count\"]\n        total = train_count + validation_count\n        train_pct = train_count / total if total &gt; 0 else 0\n        duration_str = f\"({lower or 0:.0f}s-{upper or 0:.0f}s]\"\n        logger.info(\n            f\"  {ac_type:&lt;5}{duration_str:&lt;14}: {train_count:&gt;5}/{validation_count:&gt;5} ({train_pct:5.1%})\"\n        )\n\n    splits: dict[Split, list[SegmentId]] = {\n        \"train\": segment_ids_train,\n        \"validation\": segment_ids_validation,\n    }\n    output_path = path_base / f\"splits_{partition}.json\"\n    with open(output_path, \"w\") as f:\n        json.dump(splits, f)\n    logger.info(f\"wrote splits to {output_path}\")\n</code></pre>"},{"location":"api/#microfuel.datasets.preprocessed.find_segment_indices","title":"find_segment_indices","text":"<pre><code>find_segment_indices(\n    timestamps, start_time, end_time, *, xp=np\n)\n</code></pre> Source code in <code>src/microfuel/datasets/preprocessed.py</code> <pre><code>def find_segment_indices(timestamps, start_time, end_time, *, xp=np):\n    start_idx = xp.searchsorted(timestamps, start_time, side=\"left\")\n    end_idx = xp.searchsorted(timestamps, end_time, side=\"right\")\n    return start_idx, end_idx\n</code></pre>"},{"location":"api/#microfuel.datasets.preprocessed.smooth_time_series","title":"smooth_time_series","text":"<pre><code>smooth_time_series(\n    values,\n    dts_s,\n    process_noise_variances: tuple[float, float],\n    observation_noise_variance: float,\n    gap_threshold: float = 30.0,\n) -&gt; SmoothResult\n</code></pre> <p>Applies a Kalman filter and RTS smoother to a 1D time series, handling large gaps.</p> <p>Assumes the time series follow a Constant Velocity (CV) system: \\(x_k = F x_{k-1} + w_k\\).</p> <p>Parameters:</p> Name Type Description Default <code>process_noise_variances</code> <code>tuple[float, float]</code> <p>(pos, vel) variances for the model's state transition noise (Q).</p> required <code>observation_noise_variance</code> <code>float</code> <p>variance for the measurement noise (R).</p> required <code>gap_threshold</code> <code>float</code> <p>time gap (in seconds) above which to split the time series into chunks.</p> <code>30.0</code> Source code in <code>src/microfuel/datasets/preprocessed.py</code> <pre><code>def smooth_time_series(\n    values,\n    dts_s,\n    process_noise_variances: tuple[float, float],\n    observation_noise_variance: float,\n    gap_threshold: float = 30.0,\n) -&gt; SmoothResult:\n    \"\"\"Applies a Kalman filter and RTS smoother to a 1D time series, handling large gaps.\n\n    Assumes the time series follow a Constant Velocity (CV) system:\n    $x_k = F x_{k-1} + w_k$.\n\n    :param process_noise_variances: (pos, vel) variances for the model's state transition noise (Q).\n    :param observation_noise_variance: variance for the measurement noise (R).\n    :param gap_threshold: time gap (in seconds) above which to split the time series into chunks.\n    \"\"\"\n    gap_indices = np.where(dts_s &gt; gap_threshold)[0]\n    chunk_boundaries = np.concatenate(([0], gap_indices + 1, [len(values)]))\n\n    all_smoothed_values = np.full_like(values, np.nan)\n    all_smoothed_derivatives = np.full_like(values, np.nan)\n    all_smoothed_value_variances = np.full_like(values, np.nan)\n    all_smoothed_derivative_variances = np.full_like(values, np.nan)\n\n    transition_matrix_template = np.array([[1.0, 0.0], [0.0, 1.0]])\n    observation_matrix = np.array([[1.0, 0.0]])\n    process_noise_covariance = np.diag(np.array(process_noise_variances, dtype=np.float64))\n    observation_noise_covariance = np.array([[observation_noise_variance]])\n\n    for i in range(len(chunk_boundaries) - 1):\n        start, end = chunk_boundaries[i], chunk_boundaries[i + 1]\n        chunk_values = values[start:end]\n        chunk_dts = dts_s[start : end - 1]\n\n        if len(chunk_values) &lt; 2:\n            continue\n\n        first_valid_idx = np.where(~np.isnan(chunk_values))[0]\n        if len(first_valid_idx) == 0:\n            continue\n        initial_value = chunk_values[first_valid_idx[0]]\n        initial_state_mean = np.array([initial_value, 0.0])\n        initial_state_covariance = np.eye(2) * 1e5\n\n        filtered_means, filtered_covs = _kalman_filter(\n            measurements=chunk_values,\n            dts=chunk_dts,\n            initial_state_mean=initial_state_mean,\n            initial_state_covariance=initial_state_covariance,\n            transition_matrix_fn_val=transition_matrix_template,\n            observation_matrix=observation_matrix,\n            process_noise_covariance=process_noise_covariance,\n            observation_noise_covariance=observation_noise_covariance,\n        )\n\n        smoothed_means, smoothed_covs = _rts_smoother_numba(\n            filtered_means,\n            filtered_covs,\n            chunk_dts,\n            transition_matrix_template,\n            process_noise_covariance,\n        )\n\n        all_smoothed_values[start:end] = smoothed_means[:, 0]\n        all_smoothed_derivatives[start:end] = smoothed_means[:, 1]\n        all_smoothed_value_variances[start:end] = smoothed_covs[:, 0, 0]\n        all_smoothed_derivative_variances[start:end] = smoothed_covs[:, 1, 1]\n\n    return SmoothResult(\n        all_smoothed_values,\n        all_smoothed_derivatives,\n        all_smoothed_value_variances,\n        all_smoothed_derivative_variances,\n    )\n</code></pre>"},{"location":"api/#microfuel.datasets.preprocessed.make_trajectories","title":"make_trajectories","text":"<pre><code>make_trajectories(\n    partition: Partition,\n    seed: int = 13,\n    *,\n    path_base: Path = PATH_PREPROCESSED,\n    altitude_max: Annotated[\n        float, isqx.aerospace.PRESSURE_ALTITUDE(isqx.M)\n    ] = ft2m(50000),\n    speed_max: Annotated[\n        float, isqx.SPEED(isqx.M_PERS)\n    ] = knot2mps(800),\n    vertical_speed_max: Annotated[\n        float, isqx.aerospace.VERTICAL_RATE(isqx.M_PERS)\n    ] = fpm2mps(8000),\n    track_rate_max: Annotated[float, isqx.RAD_PERS] = 0.003,\n    plot_every_n_flights: int | None = None,\n)\n</code></pre> <p>Creates train/validation split of preprocessed trajectories.</p> <p>Handles the alignment of asynchronous data sources:</p> <ol> <li>Flight List: [takeoff, landing] constraints.</li> <li>Fuel Data: segment boundaries.</li> <li>ADS-B + ACARS: raw state observations.</li> </ol> <p>It produces the standard state vector \\(x_t\\) required by <code>microfuel.model.FuelBurnPredictor</code>.</p> <p>Everything related to segments (e.g. whether a particular state vector is within [start, end]) should be handled elsewhere. This function processes the entire trajectory.</p> Source code in <code>src/microfuel/datasets/preprocessed.py</code> <pre><code>def make_trajectories(\n    partition: Partition,\n    seed: int = 13,\n    *,\n    path_base: Path = PATH_PREPROCESSED,\n    altitude_max: Annotated[float, isqx.aerospace.PRESSURE_ALTITUDE(isqx.M)] = ft2m(50000),\n    speed_max: Annotated[float, isqx.SPEED(isqx.M_PERS)] = knot2mps(800),\n    vertical_speed_max: Annotated[float, isqx.aerospace.VERTICAL_RATE(isqx.M_PERS)] = fpm2mps(8000),\n    track_rate_max: Annotated[float, isqx.RAD_PERS] = 0.003,\n    plot_every_n_flights: int | None = None,\n):\n    \"\"\"Creates train/validation split of preprocessed trajectories.\n\n    Handles the alignment of asynchronous data sources:\n\n    1. Flight List: [takeoff, landing] constraints.\n    2. Fuel Data: segment boundaries.\n    3. ADS-B + ACARS: raw state observations.\n\n    It produces the standard state vector $x_t$ required by\n    [`microfuel.model.FuelBurnPredictor`][].\n\n    Everything related to segments (e.g. whether a particular state vector is within [start, end])\n    should be handled elsewhere. This function processes the *entire* trajectory.\n    \"\"\"\n    path_base.mkdir(exist_ok=True, parents=True)\n\n    flight_list_lf = raw.scan_flight_list(partition)\n    fuel_lf = raw.scan_fuel(partition)\n\n    flight_ids_with_fuel = (\n        flight_list_lf.join(fuel_lf, on=\"flight_id\")\n        .select(\"flight_id\")\n        .unique()\n        .sort(\"flight_id\")\n        .collect()\n        .to_series()\n        .shuffle(seed=seed)\n        .to_list()\n    )\n    logger.info(\n        f\"found {len(flight_ids_with_fuel)} flights with fuel data in partition `{partition}`\"\n    )\n\n    flight_id_to_flight: dict[FlightId, raw.FlightListRecord] = {\n        row[\"flight_id\"]: row  # type: ignore\n        for row in flight_list_lf.collect().iter_rows(named=True)\n    }\n    flight_id_to_segment: dict[FlightId, tuple[pl.Series, pl.Series]] = {\n        flight_id: (data[\"start\"], data[\"end\"])\n        for (flight_id,), data in fuel_lf.collect().group_by(\"flight_id\")\n    }\n    icao_to_coords: dict[AirportIcao, Coordinate2D[float]] = {\n        row[\"icao\"]: Coordinate2D(lng=row[\"longitude\"], lat=row[\"latitude\"])\n        for row in raw.scan_airports().collect().iter_rows(named=True)\n    }  # type: ignore\n\n    trajectories_all: list[pl.LazyFrame] = []\n    for i, flight_id in enumerate(track(flight_ids_with_fuel, description=\"processing flights\")):\n        traj_lf = raw.scan_trajectory(flight_id, partition).with_columns(\n            pl.col(\"timestamp\").dt.replace_time_zone(\"UTC\")\n        )  # raw file has naiive timestamps, cast early to avoid issues in era5 interpolation\n        flight = flight_id_to_flight[flight_id]\n        timestamp_takeoff = flight[\"takeoff\"]\n        timestamp_landed = flight[\"landed\"]\n        ac_type = flight[\"aircraft_type\"]\n\n        # segments can cover timestamps that are missing from the trajectory data, including\n        # timestamps that start before takeoff or end after landing.\n        # so we want to make sure one segment has at least 2 points (start and end) present\n        timestamps_segment_start, timestamps_segment_end = flight_id_to_segment[flight_id]\n        timestamps_required = (\n            pl.concat(\n                (\n                    pl.Series((timestamp_takeoff, timestamp_landed)).dt.cast_time_unit(\"ns\"),\n                    timestamps_segment_start,\n                    timestamps_segment_end,\n                )\n            )\n            .unique()\n            .sort()\n        )\n\n        # NOTE: discarding duplicate timestamps is a bad idea! sometimes the time gets \"stuck\"\n        # and we lose a lot of useful information.\n        traj_df = traj_lf.unique(subset=[\"timestamp\"], keep=\"first\").sort(\"timestamp\").collect()\n        timestamps_existing = traj_df.select(\"timestamp\").to_series()\n        # takeoff time in flight list usually precedes the first timestamp in trajectory data\n        timestamps_missing = timestamps_required.filter(\n            ~timestamps_required.is_in(timestamps_existing)\n        )\n\n        if timestamps_missing.len() &gt; 0:\n            # for required timestamps that are beyond what the trajectory data provides,\n            # we assume they are stationary on the ground, zero filling features\n            timestamp_gnd_start: datetime = min(timestamp_takeoff, timestamps_existing.min())  # type: ignore\n            timestamp_gnd_end: datetime = max(timestamp_landed, timestamps_existing.max())  # type: ignore\n            coord_origin, coord_dest = (\n                icao_to_coords[flight[\"origin_icao\"]],\n                icao_to_coords[flight[\"destination_icao\"]],\n            )  # we dont need elevation since altitude is barometric\n            trks: list[float] = traj_df.select(\"track\").drop_nulls().to_series().to_list()\n            trk_start, trk_end = (trks[0], trks[-1]) if len(trks) else (0.0, 0.0)  # ffill/bfill\n\n            def _artificial(ts: datetime) -&gt; raw.TrajectoryRecord:\n                if ts &lt;= timestamp_gnd_start:\n                    val, lng, lat, trk = 0.0, coord_origin.lng, coord_origin.lat, trk_start\n                elif ts &gt;= timestamp_gnd_end:\n                    val, lng, lat, trk = 0.0, coord_dest.lng, coord_dest.lat, trk_end\n                else:\n                    val, lng, lat, trk = None, None, None, None\n                return raw.TrajectoryRecord(\n                    timestamp=ts,\n                    flight_id=flight_id,\n                    typecode=ac_type,\n                    latitude=lat,\n                    longitude=lng,\n                    altitude=val,\n                    groundspeed=val,\n                    track=trk,\n                    vertical_rate=val,\n                    mach=val,\n                    TAS=val,\n                    CAS=val,\n                    source=\"artificial\",\n                )\n\n            artificial_df = pl.DataFrame(_artificial(ts) for ts in timestamps_missing).with_columns(\n                pl.col(\"timestamp\").dt.cast_time_unit(\"ns\")\n            )\n            full_traj_df = traj_df.vstack(artificial_df).sort(\"timestamp\")\n        else:\n            full_traj_df = traj_df.sort(\"timestamp\")\n\n        timestamp_s = full_traj_df[\"timestamp\"].dt.epoch(time_unit=\"ms\").to_numpy() / 1000.0\n        vs_raw = fpm2mps(full_traj_df[\"vertical_rate\"].to_numpy())\n        alt_raw = ft2m(full_traj_df[\"altitude\"].to_numpy())\n        gs_raw = knot2mps(full_traj_df[\"groundspeed\"].to_numpy())\n        track_raw_rad = np.deg2rad(full_traj_df[\"track\"].to_numpy())\n        lat_raw = full_traj_df[\"latitude\"].to_numpy()\n        lng_raw = full_traj_df[\"longitude\"].to_numpy()\n\n        vs_outlier_mask = (np.abs(vs_raw) &gt; vertical_speed_max) | np.isnan(vs_raw)\n        alt_outlier_mask = (alt_raw &gt; altitude_max) | np.isnan(alt_raw)\n        gs_outlier_mask = (gs_raw &gt; speed_max) | np.isnan(gs_raw)\n        track_outlier_mask = np.isnan(track_raw_rad)\n\n        vs_with_nan = np.where(vs_outlier_mask, np.nan, vs_raw)\n        alt_with_nan = np.where(alt_outlier_mask, np.nan, alt_raw)\n\n        v_east_raw = gs_raw * np.sin(track_raw_rad)\n        v_north_raw = gs_raw * np.cos(track_raw_rad)\n        v_east_with_nan = np.where(gs_outlier_mask | track_outlier_mask, np.nan, v_east_raw)\n        v_north_with_nan = np.where(gs_outlier_mask | track_outlier_mask, np.nan, v_north_raw)\n\n        dts_s = np.diff(timestamp_s)\n        alt_res = smooth_time_series(\n            values=alt_with_nan,\n            dts_s=dts_s,\n            process_noise_variances=(1.0**2, 0.3**2),\n            observation_noise_variance=4.0**2,\n        )\n        vs_res = smooth_time_series(\n            values=vs_with_nan,\n            dts_s=dts_s,\n            process_noise_variances=(0.3**2, 0.1**2),\n            observation_noise_variance=1.0**2,\n        )\n        v_east_res = smooth_time_series(\n            values=v_east_with_nan,\n            dts_s=dts_s,\n            process_noise_variances=(1.0**2, 0.1**2),\n            observation_noise_variance=6.0**2,\n        )\n        v_north_res = smooth_time_series(\n            values=v_north_with_nan,\n            dts_s=dts_s,\n            process_noise_variances=(1.0**2, 0.1**2),\n            observation_noise_variance=6.0**2,\n        )\n\n        v_east_smooth, v_east_dot_smooth = v_east_res.val, v_east_res.val_d\n        v_north_smooth, v_north_dot_smooth = v_north_res.val, v_north_res.val_d\n        gs_smooth = np.sqrt(v_east_smooth**2 + v_north_smooth**2)\n        gs_smooth_outlier_mask = (gs_smooth &gt; speed_max) | (gs_smooth &lt; 0.0)\n        track_rate_smooth = np.abs(\n            (v_north_smooth * v_east_dot_smooth - v_east_smooth * v_north_dot_smooth)\n            / np.clip(v_east_smooth**2 + v_north_smooth**2, 1e-6, None)\n        )\n        track_rate_outlier_mask = track_rate_smooth &gt; track_rate_max\n        gs_track_outlier_mask = gs_smooth_outlier_mask | track_rate_outlier_mask\n        # ground speed and track rate are derived from ve and vn if either fails, set as outlier.\n        gs_smooth[gs_track_outlier_mask] = np.nan\n        track_rate_smooth[gs_track_outlier_mask] = np.nan\n\n        v_east_interp = _np_interpolate(v_east_smooth, timestamp_s)\n        v_north_interp = _np_interpolate(v_north_smooth, timestamp_s)\n\n        # 0=N, 90=E\n        track_interp_rad = np.arctan2(v_east_interp, v_north_interp)\n        track_interp_deg = np.rad2deg(track_interp_rad)\n        track_interp_deg = np.where(track_interp_deg &lt; 0, track_interp_deg + 360, track_interp_deg)\n\n        if i &lt; 100 or (plot_every_n_flights is not None and i % plot_every_n_flights == 0):\n            # import matplotlib\n            import matplotlib.pyplot as plt\n            from matplotlib.gridspec import GridSpec\n\n            # matplotlib.use(\"WebAgg\")\n            from .. import PATH_PLOTS_OUTPUT\n\n            N_PLOTS = 4\n            fig = plt.figure(figsize=(9, 9 * N_PLOTS * 0.3), layout=\"tight\")\n            gs = GridSpec(N_PLOTS, 1, figure=fig)\n\n            ax_alt = fig.add_subplot(gs[0])\n            ax_vs = fig.add_subplot(gs[1], sharex=ax_alt)\n            ax_gs = fig.add_subplot(gs[2], sharex=ax_alt)\n            ax_track = fig.add_subplot(gs[3], sharex=ax_alt)\n\n            for ax in [ax_alt, ax_vs, ax_gs, ax_track]:\n                if ax != ax_track:\n                    plt.setp(ax.get_xticklabels(), visible=False)\n                ax.axvline(timestamp_takeoff.timestamp(), color=\"green\", linewidth=0.5)\n                ax.axvline(timestamp_landed.timestamp(), color=\"blue\", linewidth=0.5)\n                for j, (start_ts, end_ts) in enumerate(\n                    zip(timestamps_segment_start, timestamps_segment_end)\n                ):\n                    ax.axvspan(start_ts.timestamp(), end_ts.timestamp(), color=f\"C{j}\", alpha=0.1)\n                ax.grid(True, linewidth=0.2)\n\n            ax_alt.plot(timestamp_s, alt_raw, \"k.\", markersize=2, alpha=0.3, label=\"raw altitude\")\n            ax_alt.plot(timestamp_s, alt_res.val, \"r-\", linewidth=0.5, label=\"smoothed altitude\")\n            alt_std = np.sqrt(alt_res.var_val)\n            ax_alt.fill_between(\n                timestamp_s,\n                alt_res.val - alt_std,\n                alt_res.val + alt_std,\n                color=\"r\",\n                alpha=0.2,\n                label=r\"$\\pm 1 \\sigma$\",\n            )\n            ax_alt.set_ylabel(\"altitude (m)\")\n            ax_alt.set_ylim(0, altitude_max)\n            ax_alt.legend()\n\n            ax_vs.plot(\n                timestamp_s, vs_raw, \"k.\", markersize=2, alpha=0.3, label=\"raw vertical rate\"\n            )\n            ax_vs.plot(\n                timestamp_s,\n                vs_res.val,\n                \"r-\",\n                linewidth=0.5,\n                label=\"smoothed vertical rate\",\n            )\n            ax_vs.plot(\n                timestamp_s,\n                alt_res.val_d,\n                \"b--\",\n                linewidth=0.5,\n                label=\"smoothed altitude derivative\",\n            )\n            vs_std = np.sqrt(vs_res.var_val)\n            ax_vs.fill_between(\n                timestamp_s,\n                vs_res.val - vs_std,\n                vs_res.val + vs_std,\n                color=\"r\",\n                alpha=0.2,\n                label=r\"$\\pm 1 \\sigma$ (vs)\",\n            )\n            ax_vs.set_ylabel(\"vertical rate (m/s)\")\n            ax_vs.set_ylim(-vertical_speed_max, vertical_speed_max)\n            ax_vs.legend()\n\n            ax_gs.plot(timestamp_s, gs_raw, \"k.\", markersize=2, alpha=0.3, label=\"raw groundspeed\")\n            ax_gs.plot(\n                timestamp_s,\n                gs_smooth,\n                \"r-\",\n                linewidth=0.5,\n                label=\"smoothed groundspeed\",\n            )\n            ax_gs.set_ylabel(\"groundspeed (m/s)\")\n            ax_gs.set_ylim(0, speed_max)\n            ax_gs.legend()\n\n            ax_track.plot(\n                timestamp_s, track_interp_deg, \"r.\", markersize=0.5, label=\"smoothed track\"\n            )\n            ax_track.set_ylabel(\"track (deg)\")\n            ax_track.set_ylim(0, 360)\n            ax_track.legend(loc=\"upper left\")\n            ax_track_rate = ax_track.twinx()\n            ax_track_rate.plot(\n                timestamp_s,\n                np.rad2deg(track_rate_smooth),\n                \"b.\",\n                markersize=2,\n                label=\"track rate\",\n            )\n            ax_track_rate.set_ylabel(\"track rate (deg/s)\", color=\"b\")\n            ax_track_rate.tick_params(axis=\"y\", labelcolor=\"b\")\n            ax_track_rate.legend(loc=\"lower right\")\n            ax_track_rate.set_ylim(-np.rad2deg(track_rate_max), np.rad2deg(track_rate_max))\n\n            ax_track.set_xlabel(\"time (s)\")\n            fig.suptitle(f\"flight id: {flight_id}\")\n\n            # plt.show()\n            output_dir = PATH_PLOTS_OUTPUT / \"preprocessed_trajectories\"\n            output_dir.mkdir(exist_ok=True, parents=True)\n            output_path = output_dir / f\"{partition}_{flight_id}.png\"\n            fig.savefig(output_path, dpi=300)\n            plt.close(fig)\n\n        # its possible that the very start of the smoothed data isn't processed\n        # so we must interpolate.\n        processed_traj_df = pl.DataFrame(\n            {\n                \"timestamp\": full_traj_df[\"timestamp\"],\n                \"time_since_takeoff\": (\n                    full_traj_df[\"timestamp\"] - timestamp_takeoff\n                ).dt.total_seconds(fractional=True),\n                \"time_till_arrival\": (\n                    timestamp_landed - full_traj_df[\"timestamp\"]\n                ).dt.total_seconds(fractional=True),\n                \"latitude\": _np_interpolate(lat_raw, timestamp_s),\n                \"longitude\": _np_interpolate(lng_raw, timestamp_s),\n                \"vertical_rate\": _np_interpolate(vs_res.val, timestamp_s),\n                \"vertical_rate_is_outlier\": (vs_outlier_mask | np.isnan(vs_res.val)),\n                \"altitude\": _np_interpolate(alt_res.val, timestamp_s),\n                \"altitude_is_outlier\": (alt_outlier_mask | np.isnan(alt_res.val)),\n                \"groundspeed\": _np_interpolate(gs_smooth, timestamp_s),\n                \"groundspeed_is_outlier\": (\n                    gs_outlier_mask | gs_smooth_outlier_mask | np.isnan(gs_smooth)\n                ),\n                \"track\": track_interp_deg,\n                \"track_rate\": _np_interpolate(track_rate_smooth, timestamp_s),\n                \"track_rate_is_outlier\": (track_rate_outlier_mask | np.isnan(track_rate_smooth)),\n            }\n        ).with_columns(pl.lit(flight_id).alias(\"flight_id\"))\n\n        trajectories_all.append(processed_traj_df.lazy())\n\n    output_path = path_base / f\"trajectories_{partition}.parquet\"\n    stop_event = multiprocessing.Event()\n    monitor_process = multiprocessing.Process(\n        target=_monitor_file_size, args=(output_path, stop_event)\n    )\n    monitor_process.start()\n    try:\n        pl.concat(trajectories_all).sink_parquet(output_path)\n    finally:\n        stop_event.set()\n        monitor_process.join()\n    logger.info(f\"wrote state vectors to {output_path}\")\n</code></pre>"},{"location":"api/#microfuel.datasets.preprocessed.altitude_to_pressure_std","title":"altitude_to_pressure_std","text":"<pre><code>altitude_to_pressure_std(altitude_m)\n</code></pre> Source code in <code>src/microfuel/datasets/preprocessed.py</code> <pre><code>def altitude_to_pressure_std(altitude_m):\n    # P ~ P0 * (1 - L*h/T0)^(gM/RL)\n    return 1013.25 * (1 - 2.25577e-5 * altitude_m).pow(5.25588)\n</code></pre>"},{"location":"api/#microfuel.datasets.preprocessed.make_era5","title":"make_era5","text":"<pre><code>make_era5(\n    partition: Partition,\n    *,\n    path_base: Path = PATH_PREPROCESSED,\n    path_raw_weather: Path = PATH_DATA_RAW / \"era5\",\n)\n</code></pre> Source code in <code>src/microfuel/datasets/preprocessed.py</code> <pre><code>def make_era5(\n    partition: Partition,\n    *,\n    path_base: Path = PATH_PREPROCESSED,\n    path_raw_weather: Path = PATH_DATA_RAW / \"era5\",\n):\n    import xarray as xr\n\n    path_base.mkdir(exist_ok=True, parents=True)\n    path_out_dir = path_base / f\"weather_{partition}\"\n    path_out_dir.mkdir(exist_ok=True, parents=True)\n\n    traj_lf = pl.scan_parquet(path_base / f\"trajectories_{partition}.parquet\")\n\n    df_coords = (\n        traj_lf.select(\n            \"flight_id\",\n            \"timestamp\",\n            \"latitude\",\n            \"longitude\",\n            \"altitude\",\n        )\n        .with_columns(\n            date_key=pl.col(\"timestamp\").dt.convert_time_zone(\"UTC\").dt.date(),\n            pressure_level=altitude_to_pressure_std(pl.col(\"altitude\")),\n            longitude_era5=(pl.col(\"longitude\") + 360) % 360,\n        )\n        .sort(\"timestamp\")\n        .collect()\n    )\n\n    unique_dates = df_coords[\"date_key\"].unique().sort()\n    logger.info(f\"extracting weather for {len(unique_dates)} unique days\")\n\n    def _process_variable(\n        variable_dir: Path,\n        variable_names: list[str],\n        batch_times: tuple,\n        targets: dict,\n    ) -&gt; np.ndarray | None:\n        files = sorted(variable_dir.glob(\"*.nc\"), key=lambda p: int(p.stem))\n        if not files:\n            return None\n\n        levels = [int(p.stem) for p in files]\n\n        try:\n            ds = xr.open_mfdataset(\n                files,\n                combine=\"nested\",\n                concat_dim=\"level\",\n                parallel=False,\n                chunks={\"time\": 1},\n            )\n            ds.coords[\"level\"] = levels\n\n            var_name = next((v for v in variable_names if v in ds), None)\n            if not var_name:\n                raise ValueError(f\"vars {variable_names} not found in {variable_dir}\")\n\n            ds = ds.sortby([\"time\", \"latitude\", \"level\"])\n\n            min_t, max_t = batch_times\n            da_sliced = ds[var_name].sel(time=slice(min_t, max_t))\n\n            da_loaded = da_sliced.load()\n            ds.close()\n\n            # NOTE: fill_value enables extrapolation.\n            # consider a point at 23:55,\n            # we would need 23:00 and 00:00 (the latter is located in a\n            # different file) to interpolate. however, we cannot afford spending\n            # 2x RAM, so we just have to deal with it for now.\n            interp_res = da_loaded.interp(\n                time=targets[\"time\"],\n                latitude=targets[\"latitude\"],\n                longitude=targets[\"longitude\"],\n                level=targets[\"level\"],\n                method=\"linear\",\n                kwargs={\"bounds_error\": False, \"fill_value\": None},\n            )\n\n            result = interp_res.values\n\n            del da_loaded\n            del ds\n            gc.collect()\n\n            return result\n\n        except Exception:\n            logger.error(f\"failed to process {variable_dir}:\\n{traceback.format_exc()}\")\n            return None\n\n    for date_key in track(unique_dates, description=\"processing daily weather\"):\n        output_path = path_out_dir / f\"{date_key}.parquet\"\n        if output_path.exists():\n            continue\n\n        batch = df_coords.filter(pl.col(\"date_key\") == date_key)\n        if batch.height == 0:\n            continue\n\n        logger.info(f\"processing {date_key}: {batch.height} points\")\n\n        year = f\"{date_key.year}\"\n        month = f\"{date_key.month:02d}\"\n        day = f\"{date_key.day:02d}\"\n        day_path = path_raw_weather / year / month / day\n\n        if not day_path.exists():\n            continue\n\n        target_time = xr.DataArray(batch[\"timestamp\"].to_numpy(), dims=\"points\")\n        target_lats = xr.DataArray(batch[\"latitude\"].to_numpy(), dims=\"points\")\n        target_lons = xr.DataArray(batch[\"longitude_era5\"].to_numpy(), dims=\"points\")\n        target_level = xr.DataArray(batch[\"pressure_level\"].to_numpy(), dims=\"points\")\n\n        targets = {\n            \"time\": target_time,\n            \"latitude\": target_lats,\n            \"longitude\": target_lons,\n            \"level\": target_level,\n        }\n\n        min_time = batch[\"timestamp\"].min().astimezone(timezone.utc).replace(  # type: ignore\n            tzinfo=None\n        ) - timedelta(hours=2)\n        max_time = batch[\"timestamp\"].max().astimezone(timezone.utc).replace(  # type: ignore\n            tzinfo=None\n        ) + timedelta(hours=2)\n\n        u_values = _process_variable(\n            day_path / \"u_component_of_wind\",\n            [\"u\", \"u_component_of_wind\", \"var131\"],\n            (min_time, max_time),\n            targets,\n        )\n        if u_values is None:\n            continue\n\n        v_values = _process_variable(\n            day_path / \"v_component_of_wind\",\n            [\"v\", \"v_component_of_wind\", \"var132\"],\n            (min_time, max_time),\n            targets,\n        )\n        if v_values is None:\n            continue\n\n        chunk_res = pl.DataFrame(\n            {\n                \"flight_id\": batch[\"flight_id\"],\n                \"timestamp\": batch[\"timestamp\"],\n                \"u_wind\": u_values,\n                \"v_wind\": v_values,\n            }\n        )\n        chunk_res.write_parquet(output_path)\n\n        del chunk_res\n        del u_values\n        del v_values\n        gc.collect()\n\n    logger.info(f\"wrote daily weather chunks to {path_out_dir}\")\n</code></pre>"},{"location":"api/#microfuel.datasets.preprocessed.make_derived_features","title":"make_derived_features","text":"<pre><code>make_derived_features(\n    partition: Partition,\n    *,\n    path_base: Path = PATH_PREPROCESSED,\n)\n</code></pre> <p>Warning</p> <p>This function is unused. Integration of weather features is planned for the future</p> Source code in <code>src/microfuel/datasets/preprocessed.py</code> <pre><code>def make_derived_features(partition: Partition, *, path_base: Path = PATH_PREPROCESSED):\n    \"\"\"\n    !!! warning\n        This function is unused. Integration of weather features is planned for the future\n    \"\"\"\n    traj_lf = (\n        pl.scan_parquet(path_base / f\"trajectories_{partition}.parquet\")\n        .select(\"flight_id\", \"timestamp\", \"groundspeed\", \"track\")\n        .with_row_index(\"row_idx\")\n    )\n    weather_lf = (\n        pl.scan_parquet(path_base / f\"weather_{partition}/*.parquet\")\n        .select(\"flight_id\", \"timestamp\", \"u_wind\", \"v_wind\")\n        .unique(subset=[\"flight_id\", \"timestamp\"])\n    )\n\n    ve = pl.col(\"groundspeed\") * (deg2rad(pl.col(\"track\"))).sin()\n    vn = pl.col(\"groundspeed\") * (deg2rad(pl.col(\"track\"))).cos()\n\n    u, v = pl.col(\"u_wind\"), pl.col(\"v_wind\")\n\n    va_e = ve - u\n    va_n = vn - v\n\n    tas = (va_e.pow(2) + va_n.pow(2)).sqrt()\n    wind_dot = ve * u + vn * v\n\n    derived_lf = (\n        traj_lf.join(weather_lf, on=[\"flight_id\", \"timestamp\"], how=\"left\")\n        .sort(\"row_idx\")\n        .select(\n            \"flight_id\",\n            \"timestamp\",\n            tas.alias(\"true_airspeed\"),\n            wind_dot.alias(\"wind_dot_ground\"),\n        )\n    )\n\n    path_out = path_base / f\"derived_{partition}.parquet\"\n    derived_lf.sink_parquet(path_out)\n    logger.info(f\"wrote derived features to {path_out}\")\n</code></pre>"},{"location":"api/#microfuel.datasets.preprocessed.make_standardisation_stats","title":"make_standardisation_stats","text":"<pre><code>make_standardisation_stats(\n    partition: Partition,\n    *,\n    path_base: Path = PATH_PREPROCESSED,\n)\n</code></pre> Source code in <code>src/microfuel/datasets/preprocessed.py</code> <pre><code>def make_standardisation_stats(\n    partition: Partition,\n    *,\n    path_base: Path = PATH_PREPROCESSED,\n):\n    splits = load_splits(partition, path_base=path_base)\n    train_segment_ids = splits[\"train\"]\n    logger.info(f\"computing standardisation stats from {len(train_segment_ids)} train segments\")\n\n    fuel_lf = raw.scan_fuel(partition).filter(pl.col(\"idx\").is_in(train_segment_ids))\n    train_flight_ids_df = fuel_lf.select(\"flight_id\").unique().collect()\n    flight_list_df = (\n        raw.scan_flight_list(partition)\n        .filter(pl.col(\"flight_id\").is_in(train_flight_ids_df[\"flight_id\"]))\n        .collect()\n    )\n\n    flight_duration_s = (flight_list_df[\"landed\"] - flight_list_df[\"takeoff\"]).dt.total_seconds(\n        fractional=True\n    )\n    standardisation_stats: Stats = {\n        \"flight_duration\": {\n            \"mean\": flight_duration_s.mean(),\n            \"std\": flight_duration_s.std(),\n        }\n    }  # type: ignore\n\n    trajectory_iterator = TrajectoryIterator(\n        partition=partition,\n        segment_ids=train_segment_ids,\n        start_to_end_only=True,\n    )\n\n    features_to_stat = [*STATE_FEATURES, \"flight_progress\"]\n    running_stats = {\n        feature: {\"sum\": 0.0, \"sum_sq\": 0.0, \"count\": 0} for feature in features_to_stat\n    }\n\n    flight_id_to_duration = {\n        row[\"flight_id\"]: (row[\"landed\"] - row[\"takeoff\"]).total_seconds()\n        for row in flight_list_df.iter_rows(named=True)\n    }\n\n    for trajectory in track(trajectory_iterator, description=\"computing stats from train segments\"):\n        segment_df = trajectory.features_df\n        count = len(segment_df)\n        if count == 0:\n            continue\n\n        duration_s = flight_id_to_duration.get(trajectory.info[\"flight_id\"])\n        if duration_s is not None and duration_s &gt; 0:\n            progress = (segment_df[\"timestamp\"] - trajectory.info[\"takeoff\"]).dt.total_seconds(\n                fractional=True\n            ) / duration_s\n            running_stats[\"flight_progress\"][\"sum\"] += progress.sum()\n            running_stats[\"flight_progress\"][\"sum_sq\"] += (progress**2).sum()\n            running_stats[\"flight_progress\"][\"count\"] += count\n\n        stats_for_segment = segment_df.select(\n            [pl.sum(col).alias(f\"{col}_sum\") for col in STATE_FEATURES]\n            + [(pl.col(col).pow(2)).sum().alias(f\"{col}_sum_sq\") for col in STATE_FEATURES]\n        ).row(0, named=True)\n\n        for feature in STATE_FEATURES:\n            running_stats[feature][\"sum\"] += stats_for_segment[f\"{feature}_sum\"] or 0\n            running_stats[feature][\"sum_sq\"] += stats_for_segment[f\"{feature}_sum_sq\"] or 0\n            running_stats[feature][\"count\"] += count\n\n    for feature, stats in running_stats.items():\n        count = stats[\"count\"]\n        assert count &gt; 2, f\"not enough data for feature {feature}\"\n        mean = stats[\"sum\"] / count\n        variance = (stats[\"sum_sq\"] / count) - (mean**2)\n        assert variance &gt;= 1e-9, f\"variance for {feature} is negative or too small\"\n        std = np.sqrt(variance)\n\n        standardisation_stats[feature] = {\"mean\": mean, \"std\": std}\n\n    output_path = path_base / f\"stats_{partition}.json\"\n    with open(output_path, \"w\") as f:\n        json.dump(standardisation_stats, f, indent=2)\n    logger.info(f\"wrote stats to {output_path}\")\n</code></pre>"},{"location":"api/#microfuel.datasets.preprocessed.load_splits","title":"load_splits","text":"<pre><code>load_splits(\n    partition: Partition,\n    *,\n    path_base: Path = PATH_PREPROCESSED,\n) -&gt; dict[Split, list[SegmentId]]\n</code></pre> Source code in <code>src/microfuel/datasets/preprocessed.py</code> <pre><code>def load_splits(\n    partition: Partition, *, path_base: Path = PATH_PREPROCESSED\n) -&gt; dict[Split, list[SegmentId]]:\n    with open(path_base / f\"splits_{partition}.json\") as f:\n        return json.load(f)\n</code></pre>"},{"location":"api/#microfuel.datasets.preprocessed.load_standardisation_stats","title":"load_standardisation_stats","text":"<pre><code>load_standardisation_stats(\n    partition: Partition,\n    *,\n    path_base: Path = PATH_PREPROCESSED,\n) -&gt; Stats\n</code></pre> Source code in <code>src/microfuel/datasets/preprocessed.py</code> <pre><code>def load_standardisation_stats(\n    partition: Partition, *, path_base: Path = PATH_PREPROCESSED\n) -&gt; Stats:\n    with open(path_base / f\"stats_{partition}.json\") as f:\n        return json.load(f)\n</code></pre>"},{"location":"api/#microfuel.datasets.preprocessed.prepare_iterator_data","title":"prepare_iterator_data","text":"<pre><code>prepare_iterator_data(\n    partition: Partition,\n    segment_ids: Collection[SegmentId] | None = None,\n    stats: Stats | None = None,\n    path_base: Path = PATH_PREPROCESSED,\n    source_partitions: list[Partition] | None = None,\n) -&gt; IteratorData\n</code></pre> <p>Prepares data required by the dataloader.</p> Source code in <code>src/microfuel/datasets/preprocessed.py</code> <pre><code>def prepare_iterator_data(\n    partition: Partition,\n    segment_ids: Collection[SegmentId] | None = None,\n    stats: Stats | None = None,\n    path_base: Path = PATH_PREPROCESSED,\n    source_partitions: list[Partition] | None = None,\n) -&gt; IteratorData:\n    \"\"\"Prepares data required by the dataloader.\"\"\"\n    fuel_lf = raw.scan_fuel(partition)\n    if segment_ids:\n        fuel_lf = fuel_lf.filter(pl.col(\"idx\").is_in(segment_ids))\n\n    if source_partitions is None:\n        source_partitions = [partition]\n\n    flight_lists = [raw.scan_flight_list(p) for p in source_partitions]\n    assert flight_lists\n    flight_list_lf = pl.concat(flight_lists)\n    segments_df = (\n        fuel_lf.join(\n            flight_list_lf.select(\"flight_id\", \"takeoff\", \"landed\", \"aircraft_type\"),\n            on=\"flight_id\",\n        )\n        .sort(\"flight_id\")\n        .collect()\n    )\n\n    segments_df = (\n        fuel_lf.join(\n            flight_list_lf.select(\"flight_id\", \"takeoff\", \"landed\", \"aircraft_type\"),\n            on=\"flight_id\",\n        )\n        .sort(\"flight_id\")\n        .collect()\n    )\n\n    traj_paths = [path_base / f\"trajectories_{p}.parquet\" for p in source_partitions]\n    traj_paths = [p for p in traj_paths if p.exists()]\n    assert traj_paths\n    logger.info(f\"loading trajectories from {len(traj_paths)} partitions: {source_partitions}\")\n\n    # optimisation: select specific columns early to avoid OOM during large joins\n    traj_cols = [\"flight_id\", \"timestamp\", *STATE_FEATURES]\n    traj_lf = pl.scan_parquet(traj_paths).select(traj_cols)\n    derived_path = path_base / f\"derived_{partition}.parquet\"\n    if derived_path.exists() and WEATHER_FEATURES:\n        derived_lf = pl.scan_parquet(derived_path)\n        traj_lf = pl.concat(\n            [traj_lf, derived_lf.select(WEATHER_FEATURES)], how=\"horizontal\"\n        )  # NOTE: we do not do a join here to avoid OOM: we already made sure rows align perfectly\n\n    if stats is not None:\n        standardisation_exprs = [\n            ((pl.col(f) - stats[f][\"mean\"]) / stats[f][\"std\"]).alias(f) for f in STATE_FEATURES\n        ]\n        traj_lf = traj_lf.with_columns(standardisation_exprs)\n\n    return IteratorData(segments_df=segments_df, traj_lf=traj_lf)\n</code></pre>"},{"location":"api/#microfuel.datasets.raw","title":"raw","text":""},{"location":"api/#microfuel.datasets.raw.Config","title":"Config","text":"<p>               Bases: <code>TypedDict</code></p> Source code in <code>src/microfuel/datasets/raw.py</code> <pre><code>class Config(TypedDict):\n    team_id: int\n    team_name: str\n    bucket_access_key: str\n    bucket_access_secret: str\n</code></pre>"},{"location":"api/#microfuel.datasets.raw.Config.team_id","title":"team_id  <code>instance-attribute</code>","text":"<pre><code>team_id: int\n</code></pre>"},{"location":"api/#microfuel.datasets.raw.Config.team_name","title":"team_name  <code>instance-attribute</code>","text":"<pre><code>team_name: str\n</code></pre>"},{"location":"api/#microfuel.datasets.raw.Config.bucket_access_key","title":"bucket_access_key  <code>instance-attribute</code>","text":"<pre><code>bucket_access_key: str\n</code></pre>"},{"location":"api/#microfuel.datasets.raw.Config.bucket_access_secret","title":"bucket_access_secret  <code>instance-attribute</code>","text":"<pre><code>bucket_access_secret: str\n</code></pre>"},{"location":"api/#microfuel.datasets.raw.FuelRecord","title":"FuelRecord","text":"<p>               Bases: <code>TypedDict</code></p> <p>Fuel consumption data for a given flight interval. Path: <code>fuel_{partition}.parquet</code>.</p> Source code in <code>src/microfuel/datasets/raw.py</code> <pre><code>class FuelRecord(TypedDict):\n    \"\"\"Fuel consumption data for a given flight interval. Path: `fuel_{partition}.parquet`.\"\"\"\n\n    idx: Annotated[int, pl.Int64]\n    \"\"\"Unique row identifier.\"\"\"\n    flight_id: Annotated[FlightId, pl.Utf8]\n    \"\"\"Links to the flight list and trajectory.\"\"\"\n    start: Annotated[datetime, pl.Datetime(time_zone=\"UTC\")]\n    \"\"\"The start timestamp of the interval (usually an ACARS report).\"\"\"\n    end: Annotated[datetime, pl.Datetime(time_zone=\"UTC\")]\n    \"\"\"The end timestamp of the interval.\"\"\"\n    fuel_kg: Annotated[float, pl.Float64, isqx.MASS(isqx.KG)]\n    \"\"\"The target variable.\n\n    !!! warning\n        Note that this variable has quantisation artifacts: data is not a simple continuous\n        distribution but a composite from at least two distinct sources:\n        imperial (pounds) and metric (kilograms) units with a 2sf rounding step.\n    \"\"\"\n</code></pre>"},{"location":"api/#microfuel.datasets.raw.FuelRecord.idx","title":"idx  <code>instance-attribute</code>","text":"<pre><code>idx: Annotated[int, pl.Int64]\n</code></pre> <p>Unique row identifier.</p>"},{"location":"api/#microfuel.datasets.raw.FuelRecord.flight_id","title":"flight_id  <code>instance-attribute</code>","text":"<pre><code>flight_id: Annotated[FlightId, pl.Utf8]\n</code></pre> <p>Links to the flight list and trajectory.</p>"},{"location":"api/#microfuel.datasets.raw.FuelRecord.start","title":"start  <code>instance-attribute</code>","text":"<pre><code>start: Annotated[datetime, pl.Datetime(time_zone='UTC')]\n</code></pre> <p>The start timestamp of the interval (usually an ACARS report).</p>"},{"location":"api/#microfuel.datasets.raw.FuelRecord.end","title":"end  <code>instance-attribute</code>","text":"<pre><code>end: Annotated[datetime, pl.Datetime(time_zone='UTC')]\n</code></pre> <p>The end timestamp of the interval.</p>"},{"location":"api/#microfuel.datasets.raw.FuelRecord.fuel_kg","title":"fuel_kg  <code>instance-attribute</code>","text":"<pre><code>fuel_kg: Annotated[float, pl.Float64, isqx.MASS(isqx.KG)]\n</code></pre> <p>The target variable.</p> <p>Warning</p> <p>Note that this variable has quantisation artifacts: data is not a simple continuous distribution but a composite from at least two distinct sources: imperial (pounds) and metric (kilograms) units with a 2sf rounding step.</p>"},{"location":"api/#microfuel.datasets.raw.FlightListRecord","title":"FlightListRecord","text":"<p>               Bases: <code>TypedDict</code></p> <p>Metadata for each flight in the dataset. Path: <code>flight_list_{partition}.parquet</code>.</p> Source code in <code>src/microfuel/datasets/raw.py</code> <pre><code>class FlightListRecord(TypedDict):\n    \"\"\"Metadata for each flight in the dataset. Path: `flight_list_{partition}.parquet`.\"\"\"\n\n    flight_id: Annotated[FlightId, pl.Utf8]\n    \"\"\"A unique identifier for the flight.\"\"\"\n    flight_date: Annotated[datetime, pl.Date]\n    \"\"\"The date of the flight.\"\"\"\n    takeoff: Annotated[datetime, pl.Datetime(time_zone=\"UTC\")]\n    \"\"\"The timestamp of takeoff.\"\"\"\n    landed: Annotated[datetime, pl.Datetime(time_zone=\"UTC\")]\n    \"\"\"The timestamp of landing.\"\"\"\n    origin_icao: Annotated[AirportIcao, pl.Utf8]\n    \"\"\"ICAO code for the departure airport.\"\"\"\n    destination_icao: Annotated[AirportIcao, pl.Utf8]\n    \"\"\"ICAO code for the destination airport.\"\"\"\n    aircraft_type: Annotated[AircraftType, pl.Utf8]\n</code></pre>"},{"location":"api/#microfuel.datasets.raw.FlightListRecord.flight_id","title":"flight_id  <code>instance-attribute</code>","text":"<pre><code>flight_id: Annotated[FlightId, pl.Utf8]\n</code></pre> <p>A unique identifier for the flight.</p>"},{"location":"api/#microfuel.datasets.raw.FlightListRecord.flight_date","title":"flight_date  <code>instance-attribute</code>","text":"<pre><code>flight_date: Annotated[datetime, pl.Date]\n</code></pre> <p>The date of the flight.</p>"},{"location":"api/#microfuel.datasets.raw.FlightListRecord.takeoff","title":"takeoff  <code>instance-attribute</code>","text":"<pre><code>takeoff: Annotated[datetime, pl.Datetime(time_zone='UTC')]\n</code></pre> <p>The timestamp of takeoff.</p>"},{"location":"api/#microfuel.datasets.raw.FlightListRecord.landed","title":"landed  <code>instance-attribute</code>","text":"<pre><code>landed: Annotated[datetime, pl.Datetime(time_zone='UTC')]\n</code></pre> <p>The timestamp of landing.</p>"},{"location":"api/#microfuel.datasets.raw.FlightListRecord.origin_icao","title":"origin_icao  <code>instance-attribute</code>","text":"<pre><code>origin_icao: Annotated[AirportIcao, pl.Utf8]\n</code></pre> <p>ICAO code for the departure airport.</p>"},{"location":"api/#microfuel.datasets.raw.FlightListRecord.destination_icao","title":"destination_icao  <code>instance-attribute</code>","text":"<pre><code>destination_icao: Annotated[AirportIcao, pl.Utf8]\n</code></pre> <p>ICAO code for the destination airport.</p>"},{"location":"api/#microfuel.datasets.raw.FlightListRecord.aircraft_type","title":"aircraft_type  <code>instance-attribute</code>","text":"<pre><code>aircraft_type: Annotated[AircraftType, pl.Utf8]\n</code></pre>"},{"location":"api/#microfuel.datasets.raw.AirportRecord","title":"AirportRecord","text":"<p>               Bases: <code>TypedDict</code></p> <p>Airport metadata. Path: <code>apt.parquet</code>.</p> Source code in <code>src/microfuel/datasets/raw.py</code> <pre><code>class AirportRecord(TypedDict):\n    \"\"\"Airport metadata. Path: `apt.parquet`.\"\"\"\n\n    icao: Annotated[AirportIcao, pl.Utf8]\n    latitude: Annotated[float, pl.Float64, isqx.LATITUDE(isqx.DEG)]\n    longitude: Annotated[float, pl.Float64, isqx.LONGITUDE(isqx.DEG)]\n    elevation: Annotated[float, pl.Float64, isqx.aerospace.GEOMETRIC_ALTITUDE(isqx.usc.FT)] | None\n</code></pre>"},{"location":"api/#microfuel.datasets.raw.AirportRecord.icao","title":"icao  <code>instance-attribute</code>","text":"<pre><code>icao: Annotated[AirportIcao, pl.Utf8]\n</code></pre>"},{"location":"api/#microfuel.datasets.raw.AirportRecord.latitude","title":"latitude  <code>instance-attribute</code>","text":"<pre><code>latitude: Annotated[\n    float, pl.Float64, isqx.LATITUDE(isqx.DEG)\n]\n</code></pre>"},{"location":"api/#microfuel.datasets.raw.AirportRecord.longitude","title":"longitude  <code>instance-attribute</code>","text":"<pre><code>longitude: Annotated[\n    float, pl.Float64, isqx.LONGITUDE(isqx.DEG)\n]\n</code></pre>"},{"location":"api/#microfuel.datasets.raw.AirportRecord.elevation","title":"elevation  <code>instance-attribute</code>","text":"<pre><code>elevation: (\n    Annotated[\n        float,\n        pl.Float64,\n        isqx.aerospace.GEOMETRIC_ALTITUDE(isqx.usc.FT),\n    ]\n    | None\n)\n</code></pre>"},{"location":"api/#microfuel.datasets.raw.TrajectoryRecord","title":"TrajectoryRecord","text":"<p>               Bases: <code>TypedDict</code></p> <p>Flight trajectory data points. Path: <code>flights_{partition}/{flight_id}.parquet</code>.</p> Source code in <code>src/microfuel/datasets/raw.py</code> <pre><code>class TrajectoryRecord(TypedDict):\n    \"\"\"Flight trajectory data points. Path: `flights_{partition}/{flight_id}.parquet`.\"\"\"\n\n    timestamp: Annotated[datetime, pl.Datetime(time_unit=\"ns\", time_zone=\"UTC\")]\n    flight_id: Annotated[FlightId, pl.Utf8]\n    typecode: Annotated[AircraftType, pl.Utf8]\n    latitude: Annotated[float, pl.Float64, isqx.LATITUDE(isqx.DEG)] | None\n    \"\"\"Latitude, encoded via Compact Positional Reporting (CPR, tc=9-18, 20-22)\n    We do not have access to uncertainty/quantisation, can be anywhere from:\n\n    - navigational integrity category: nic=11 (rc &lt; 7.5m)..nic=8 (rc &lt; 185m)\n    - navigational accuracy category: nacp=10 (epu &lt; 10m)..nacp=8 (epu &lt; 93m)\"\"\"\n    longitude: Annotated[float, pl.Float64, isqx.LONGITUDE(isqx.DEG)] | None  # see above.\n    altitude: Annotated[float, pl.Float64, isqx.aerospace.PRESSURE_ALTITUDE(isqx.usc.FT)] | None\n    \"\"\"Barometric altitude (tc=9-18, 12-bit field). Not to be confused with GNSS altitude (tc=20-22)\n\n    Quantisation: 'q' bit (bit 8 of the field):\n    - q=1: 25-foot increments. altitude = (decimal value of 11 bits) * 25 - 1000 ft.\n    - q=0: 100-foot increments, using gray code for altitudes &gt; 50,175 ft.\n\n    Uncertainty: depends on barometric altitude quality (baq).\"\"\"\n    groundspeed: Annotated[float, pl.Float64, isqx.aerospace.GROUND_SPEED(isqx.usc.KNOT)] | None\n    \"\"\"Ground speed (GNSS or inertial reference system, tc=19, subtypes1-2).\n\n    Not transmitted directly, encoded as two signed velocity components (east-west velocity,\n    north-south velocity):\n\n    - groundspeed = sqrt(vew^2 + vns^2)\n    - track angle = atan2(vew, vns)\n\n    Quantisation: 1 kt (subsonic), 4 kt (supersonic).\n    Uncertainty: nacv=4 (&lt; 0.3m/s), nacv=3 (&lt; 1.0m/s), nacv=2 (&lt; 3.0m/s), nacv=1 (&lt; 10.0m/s)\"\"\"\n    track: Annotated[float, pl.Float64, isqx.DEG] | None  # see above.\n    vertical_rate: (\n        Annotated[float, pl.Float64, isqx.aerospace.VERTICAL_RATE(isqx.usc.FT * isqx.MIN**-1)]\n        | None\n    )\n    \"\"\"Vertical rate (`vrsrc` specifies origin: GNSS or barometric, tc=19).\n\n    a sign bit indicates climb or descent. a 9-bit value (vr) encodes the magnitude.\n    vertical speed (ft/min) = 64 * (vr - 1).\n\n    Uncertainty: linked to vertical component of nacv.\"\"\"\n    mach: Annotated[float, pl.Float64, isqx.MACH_NUMBER] | None\n    \"\"\"Mach number (Mode S, BDS 6,0, 10 bits, mb 25-34).\n\n    Quantisation: 0.004.\"\"\"\n    TAS: Annotated[float, pl.Float64, isqx.aerospace.TRUE_AIRSPEED(isqx.usc.KNOT)] | None\n    \"\"\"True airspeed.\n\n    - ADS-B (tc=19, subtype 3/4) - Quantisation: 1 kt (subtype 3), 4 kt (subtype 4).\n    - Mode S (BDS 5,0 track and turn report, 10 bits, mb 47-56) - Quantisation: 2 kt\"\"\"\n    CAS: Annotated[float, pl.Float64, isqx.aerospace.CALIBRATED_AIRSPEED(isqx.usc.KNOT)] | None\n    \"\"\"Calibrated airspeed. Not broadcast, but likely derived from indicated airspeed (BDS 6,0).\n\n    Quantisation: 1 kt.\"\"\"\n    source: Annotated[Literal[\"adsb\", \"acars\", \"artificial\"], pl.Utf8]\n    \"\"\"Data source.\n\n    Data from `adsb` and `acars` have different characteristics.\n    `acars` data, for instance, may include `mach`, `TAS`, and `CAS`,\n    which are not present in standard ADS-B reports.\n\n    `artificial` data points are inserted from [microfuel.datasets.raw.FlightListRecord][] to aid\n    interpolation.\n    \"\"\"\n</code></pre>"},{"location":"api/#microfuel.datasets.raw.TrajectoryRecord.timestamp","title":"timestamp  <code>instance-attribute</code>","text":"<pre><code>timestamp: Annotated[\n    datetime, pl.Datetime(time_unit=\"ns\", time_zone=\"UTC\")\n]\n</code></pre>"},{"location":"api/#microfuel.datasets.raw.TrajectoryRecord.flight_id","title":"flight_id  <code>instance-attribute</code>","text":"<pre><code>flight_id: Annotated[FlightId, pl.Utf8]\n</code></pre>"},{"location":"api/#microfuel.datasets.raw.TrajectoryRecord.typecode","title":"typecode  <code>instance-attribute</code>","text":"<pre><code>typecode: Annotated[AircraftType, pl.Utf8]\n</code></pre>"},{"location":"api/#microfuel.datasets.raw.TrajectoryRecord.latitude","title":"latitude  <code>instance-attribute</code>","text":"<pre><code>latitude: (\n    Annotated[float, pl.Float64, isqx.LATITUDE(isqx.DEG)]\n    | None\n)\n</code></pre> <p>Latitude, encoded via Compact Positional Reporting (CPR, tc=9-18, 20-22) We do not have access to uncertainty/quantisation, can be anywhere from:</p> <ul> <li>navigational integrity category: nic=11 (rc &lt; 7.5m)..nic=8 (rc &lt; 185m)</li> <li>navigational accuracy category: nacp=10 (epu &lt; 10m)..nacp=8 (epu &lt; 93m)</li> </ul>"},{"location":"api/#microfuel.datasets.raw.TrajectoryRecord.longitude","title":"longitude  <code>instance-attribute</code>","text":"<pre><code>longitude: (\n    Annotated[float, pl.Float64, isqx.LONGITUDE(isqx.DEG)]\n    | None\n)\n</code></pre>"},{"location":"api/#microfuel.datasets.raw.TrajectoryRecord.altitude","title":"altitude  <code>instance-attribute</code>","text":"<pre><code>altitude: (\n    Annotated[\n        float,\n        pl.Float64,\n        isqx.aerospace.PRESSURE_ALTITUDE(isqx.usc.FT),\n    ]\n    | None\n)\n</code></pre> <p>Barometric altitude (tc=9-18, 12-bit field). Not to be confused with GNSS altitude (tc=20-22)</p> <p>Quantisation: 'q' bit (bit 8 of the field): - q=1: 25-foot increments. altitude = (decimal value of 11 bits) * 25 - 1000 ft. - q=0: 100-foot increments, using gray code for altitudes &gt; 50,175 ft.</p> <p>Uncertainty: depends on barometric altitude quality (baq).</p>"},{"location":"api/#microfuel.datasets.raw.TrajectoryRecord.groundspeed","title":"groundspeed  <code>instance-attribute</code>","text":"<pre><code>groundspeed: (\n    Annotated[\n        float,\n        pl.Float64,\n        isqx.aerospace.GROUND_SPEED(isqx.usc.KNOT),\n    ]\n    | None\n)\n</code></pre> <p>Ground speed (GNSS or inertial reference system, tc=19, subtypes1-2).</p> <p>Not transmitted directly, encoded as two signed velocity components (east-west velocity, north-south velocity):</p> <ul> <li>groundspeed = sqrt(vew^2 + vns^2)</li> <li>track angle = atan2(vew, vns)</li> </ul> <p>Quantisation: 1 kt (subsonic), 4 kt (supersonic). Uncertainty: nacv=4 (&lt; 0.3m/s), nacv=3 (&lt; 1.0m/s), nacv=2 (&lt; 3.0m/s), nacv=1 (&lt; 10.0m/s)</p>"},{"location":"api/#microfuel.datasets.raw.TrajectoryRecord.track","title":"track  <code>instance-attribute</code>","text":"<pre><code>track: Annotated[float, pl.Float64, isqx.DEG] | None\n</code></pre>"},{"location":"api/#microfuel.datasets.raw.TrajectoryRecord.vertical_rate","title":"vertical_rate  <code>instance-attribute</code>","text":"<pre><code>vertical_rate: (\n    Annotated[\n        float,\n        pl.Float64,\n        isqx.aerospace.VERTICAL_RATE(\n            isqx.usc.FT * isqx.MIN**-1\n        ),\n    ]\n    | None\n)\n</code></pre> <p>Vertical rate (<code>vrsrc</code> specifies origin: GNSS or barometric, tc=19).</p> <p>a sign bit indicates climb or descent. a 9-bit value (vr) encodes the magnitude. vertical speed (ft/min) = 64 * (vr - 1).</p> <p>Uncertainty: linked to vertical component of nacv.</p>"},{"location":"api/#microfuel.datasets.raw.TrajectoryRecord.mach","title":"mach  <code>instance-attribute</code>","text":"<pre><code>mach: Annotated[float, pl.Float64, isqx.MACH_NUMBER] | None\n</code></pre> <p>Mach number (Mode S, BDS 6,0, 10 bits, mb 25-34).</p> <p>Quantisation: 0.004.</p>"},{"location":"api/#microfuel.datasets.raw.TrajectoryRecord.TAS","title":"TAS  <code>instance-attribute</code>","text":"<pre><code>TAS: (\n    Annotated[\n        float,\n        pl.Float64,\n        isqx.aerospace.TRUE_AIRSPEED(isqx.usc.KNOT),\n    ]\n    | None\n)\n</code></pre> <p>True airspeed.</p> <ul> <li>ADS-B (tc=19, subtype 3/4) - Quantisation: 1 kt (subtype 3), 4 kt (subtype 4).</li> <li>Mode S (BDS 5,0 track and turn report, 10 bits, mb 47-56) - Quantisation: 2 kt</li> </ul>"},{"location":"api/#microfuel.datasets.raw.TrajectoryRecord.CAS","title":"CAS  <code>instance-attribute</code>","text":"<pre><code>CAS: (\n    Annotated[\n        float,\n        pl.Float64,\n        isqx.aerospace.CALIBRATED_AIRSPEED(isqx.usc.KNOT),\n    ]\n    | None\n)\n</code></pre> <p>Calibrated airspeed. Not broadcast, but likely derived from indicated airspeed (BDS 6,0).</p> <p>Quantisation: 1 kt.</p>"},{"location":"api/#microfuel.datasets.raw.TrajectoryRecord.source","title":"source  <code>instance-attribute</code>","text":"<pre><code>source: Annotated[\n    Literal[\"adsb\", \"acars\", \"artificial\"], pl.Utf8\n]\n</code></pre> <p>Data source.</p> <p>Data from <code>adsb</code> and <code>acars</code> have different characteristics. <code>acars</code> data, for instance, may include <code>mach</code>, <code>TAS</code>, and <code>CAS</code>, which are not present in standard ADS-B reports.</p> <p><code>artificial</code> data points are inserted from microfuel.datasets.raw.FlightListRecord to aid interpolation.</p>"},{"location":"api/#microfuel.datasets.raw.SubmissionRecord","title":"SubmissionRecord","text":"<p>               Bases: <code>TypedDict</code></p> Source code in <code>src/microfuel/datasets/raw.py</code> <pre><code>class SubmissionRecord(TypedDict):\n    idx: Annotated[FlightId, pl.Utf8]\n    \"\"\"The row identifier.\"\"\"\n    predicted_fuel_kg: Annotated[float, pl.Float64, isqx.MASS(isqx.KG)]\n    \"\"\"Predicted fuel consumption (kilograms) for the given interval.\"\"\"\n</code></pre>"},{"location":"api/#microfuel.datasets.raw.SubmissionRecord.idx","title":"idx  <code>instance-attribute</code>","text":"<pre><code>idx: Annotated[FlightId, pl.Utf8]\n</code></pre> <p>The row identifier.</p>"},{"location":"api/#microfuel.datasets.raw.SubmissionRecord.predicted_fuel_kg","title":"predicted_fuel_kg  <code>instance-attribute</code>","text":"<pre><code>predicted_fuel_kg: Annotated[\n    float, pl.Float64, isqx.MASS(isqx.KG)\n]\n</code></pre> <p>Predicted fuel consumption (kilograms) for the given interval.</p>"},{"location":"api/#microfuel.datasets.raw.load_config","title":"load_config","text":"<pre><code>load_config(fp: Path = PATH_DATA / 'config.toml') -&gt; Config\n</code></pre> Source code in <code>src/microfuel/datasets/raw.py</code> <pre><code>def load_config(fp: Path = PATH_DATA / \"config.toml\") -&gt; Config:\n    if sys.version_info &gt;= (3, 11):\n        import tomllib\n    else:\n        import tomli as tomllib\n    with open(fp, \"rb\") as f:\n        return tomllib.load(f)  # type: ignore\n</code></pre>"},{"location":"api/#microfuel.datasets.raw.setup_mc_alias","title":"setup_mc_alias","text":"<pre><code>setup_mc_alias(\n    bucket_access_key: str,\n    bucket_access_secret: str,\n    endpoint_url: str = \"https://s3.opensky-network.org:443\",\n    alias_name: str = \"prc25\",\n) -&gt; int\n</code></pre> Source code in <code>src/microfuel/datasets/raw.py</code> <pre><code>def setup_mc_alias(\n    bucket_access_key: str,\n    bucket_access_secret: str,\n    endpoint_url: str = \"https://s3.opensky-network.org:443\",\n    alias_name: str = \"prc25\",\n) -&gt; int:\n    cmd = f\"mc alias set {alias_name} {endpoint_url} {bucket_access_key} {bucket_access_secret}\"\n    return os.system(cmd)\n</code></pre>"},{"location":"api/#microfuel.datasets.raw.download_from_s3","title":"download_from_s3","text":"<pre><code>download_from_s3(\n    bucket_access_key: str,\n    bucket_access_secret: str,\n    *,\n    path_out: Path = PATH_DATA_RAW,\n    bucket_name: str = \"prc-2025-datasets\",\n    endpoint_url: str = \"https://s3.opensky-network.org:443\",\n    alias_name: str = \"prc2025\",\n) -&gt; int\n</code></pre> <p>Download data from S3 using MinIO client. Not using boto3 because it is extremely slow.</p> Source code in <code>src/microfuel/datasets/raw.py</code> <pre><code>def download_from_s3(\n    bucket_access_key: str,\n    bucket_access_secret: str,\n    *,\n    path_out: Path = PATH_DATA_RAW,\n    bucket_name: str = \"prc-2025-datasets\",\n    endpoint_url: str = \"https://s3.opensky-network.org:443\",\n    alias_name: str = \"prc2025\",\n) -&gt; int:\n    \"\"\"Download data from S3 using MinIO client.\n    Not using boto3 because it is extremely slow.\"\"\"\n    path_out.mkdir(parents=True, exist_ok=True)\n    setup_mc_alias(bucket_access_key, bucket_access_secret, endpoint_url, alias_name)\n    cmd = f\"mc cp --recursive {alias_name}/{bucket_name}/ {path_out}/\"\n    return os.system(cmd)\n</code></pre>"},{"location":"api/#microfuel.datasets.raw.scan_fuel","title":"scan_fuel","text":"<pre><code>scan_fuel(\n    partition: Partition = \"phase1\",\n    *,\n    path_base: Path = PATH_DATA_RAW,\n) -&gt; Annotated[pl.LazyFrame, FuelRecord]\n</code></pre> Source code in <code>src/microfuel/datasets/raw.py</code> <pre><code>def scan_fuel(\n    partition: Partition = \"phase1\", *, path_base: Path = PATH_DATA_RAW\n) -&gt; Annotated[pl.LazyFrame, FuelRecord]:\n    fp = path_base / f\"fuel_{partition}.parquet\"\n    # timestamps are in nanoseconds\n    return pl.scan_parquet(fp).with_columns(\n        pl.col(\"start\").dt.replace_time_zone(\"UTC\"), pl.col(\"end\").dt.replace_time_zone(\"UTC\")\n    )\n</code></pre>"},{"location":"api/#microfuel.datasets.raw.scan_flight_list","title":"scan_flight_list","text":"<pre><code>scan_flight_list(\n    partition: Partition = \"phase1\",\n    *,\n    path_base: Path = PATH_DATA_RAW,\n) -&gt; Annotated[pl.LazyFrame, FlightListRecord]\n</code></pre> Source code in <code>src/microfuel/datasets/raw.py</code> <pre><code>def scan_flight_list(\n    partition: Partition = \"phase1\", *, path_base: Path = PATH_DATA_RAW\n) -&gt; Annotated[pl.LazyFrame, FlightListRecord]:\n    fp = path_base / f\"flight_list_{partition}.parquet\"\n    # timestamp are in seconds\n    return pl.scan_parquet(fp).with_columns(\n        pl.col(\"takeoff\").dt.replace_time_zone(\"UTC\"), pl.col(\"landed\").dt.replace_time_zone(\"UTC\")\n    )\n</code></pre>"},{"location":"api/#microfuel.datasets.raw.scan_airports","title":"scan_airports","text":"<pre><code>scan_airports(\n    *, path_base: Path = PATH_DATA_RAW\n) -&gt; Annotated[pl.LazyFrame, AirportRecord]\n</code></pre> Source code in <code>src/microfuel/datasets/raw.py</code> <pre><code>def scan_airports(*, path_base: Path = PATH_DATA_RAW) -&gt; Annotated[pl.LazyFrame, AirportRecord]:\n    fp = path_base / \"apt.parquet\"\n    return pl.scan_parquet(fp)\n</code></pre>"},{"location":"api/#microfuel.datasets.raw.scan_all_trajectories","title":"scan_all_trajectories","text":"<pre><code>scan_all_trajectories(\n    partition: Partition = \"phase1\",\n    *,\n    path_base: Path = PATH_DATA_RAW,\n) -&gt; Annotated[pl.LazyFrame, TrajectoryRecord]\n</code></pre> Source code in <code>src/microfuel/datasets/raw.py</code> <pre><code>def scan_all_trajectories(\n    partition: Partition = \"phase1\", *, path_base: Path = PATH_DATA_RAW\n) -&gt; Annotated[pl.LazyFrame, TrajectoryRecord]:\n    fp = path_base / f\"flights_{partition}\"\n    return pl.scan_parquet(f\"{fp}/*.parquet\").with_columns(\n        pl.col(\"timestamp\").dt.replace_time_zone(\"UTC\"),\n    )\n</code></pre>"},{"location":"api/#microfuel.datasets.raw.scan_trajectory","title":"scan_trajectory","text":"<pre><code>scan_trajectory(\n    flight_id: str,\n    partition: Partition = \"phase1\",\n    *,\n    path_base: Path = PATH_DATA_RAW,\n) -&gt; Annotated[pl.LazyFrame, TrajectoryRecord]\n</code></pre> Source code in <code>src/microfuel/datasets/raw.py</code> <pre><code>def scan_trajectory(\n    flight_id: str, partition: Partition = \"phase1\", *, path_base: Path = PATH_DATA_RAW\n) -&gt; Annotated[pl.LazyFrame, TrajectoryRecord]:\n    fp = path_base / f\"flights_{partition}\" / f\"{flight_id}.parquet\"\n    return pl.scan_parquet(fp).with_columns(\n        pl.col(\"timestamp\").dt.replace_time_zone(\"UTC\"),\n    )\n</code></pre>"},{"location":"api/#microfuel.hacks","title":"hacks","text":"<p>Vendored and patched versions of <code>flash-linear-attention</code>.</p> <p>To avoid recompilation during variable-length training, we make the following changes:</p> <ol> <li> <p>Causal Conv1D (https://github.com/fla-org/flash-linear-attention/blob/main/fla/modules/convolution.py)</p> </li> <li> <p><code>NB</code> removed from autotune key</p> </li> <li><code>NB</code> removed from constexpr list in signature (kept as scalar)</li> <li> <p><code>BT</code> fixed to 64 in wrappers</p> </li> <li> <p>L2Norm (https://github.com/fla-org/flash-linear-attention/blob/main/fla/modules/l2norm.py)</p> </li> <li> <p><code>NB</code> removed from autotune key</p> </li> <li><code>NB</code> removed from constexpr in signature</li> <li> <p><code>T</code> removed from constexpr in signature</p> </li> <li> <p>GatedNorm (https://github.com/fla-org/flash-linear-attention/blob/main/fla/modules/fused_norm_gate.py)</p> </li> <li> <p><code>NB</code> removed from autotune key</p> </li> <li><code>NB</code> removed from constexpr in signature</li> </ol>"},{"location":"api/#microfuel.hacks.NUM_WARPS_AUTOTUNE","title":"NUM_WARPS_AUTOTUNE  <code>module-attribute</code>","text":"<pre><code>NUM_WARPS_AUTOTUNE = (\n    [2, 4, 8, 16] if is_amd else [4, 8, 16, 32]\n)\n</code></pre>"},{"location":"api/#microfuel.hacks.FIXED_BT_CONV","title":"FIXED_BT_CONV  <code>module-attribute</code>","text":"<pre><code>FIXED_BT_CONV = 64\n</code></pre>"},{"location":"api/#microfuel.hacks.BT_LIST","title":"BT_LIST  <code>module-attribute</code>","text":"<pre><code>BT_LIST = [8, 16, 32, 64, 128]\n</code></pre>"},{"location":"api/#microfuel.hacks.causal_conv1d_fwd_kernel","title":"causal_conv1d_fwd_kernel","text":"<pre><code>causal_conv1d_fwd_kernel(\n    x,\n    y,\n    weight,\n    bias,\n    residual,\n    cu_seqlens,\n    initial_state,\n    chunk_indices,\n    B,\n    T,\n    D: tl.constexpr,\n    W: tl.constexpr,\n    BT: tl.constexpr,\n    BW: tl.constexpr,\n    BD: tl.constexpr,\n    NB,\n    ACTIVATION: tl.constexpr,\n    HAS_WEIGHT: tl.constexpr,\n    HAS_BIAS: tl.constexpr,\n    HAS_RESIDUAL: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n)\n</code></pre> Source code in <code>src/microfuel/hacks.py</code> <pre><code>@triton.heuristics({\n    'HAS_WEIGHT': lambda args: args['weight'] is not None,\n    'HAS_BIAS': lambda args: args['bias'] is not None,\n    'HAS_RESIDUAL': lambda args: args['residual'] is not None,\n    'USE_INITIAL_STATE': lambda args: args['initial_state'] is not None,\n    'IS_VARLEN': lambda args: args['cu_seqlens'] is not None,\n})\n@triton.autotune(\n    configs=[\n        triton.Config({'BD': BD}, num_warps=num_warps)\n        for BD in [16, 32, 64, 128]\n        for num_warps in NUM_WARPS_AUTOTUNE\n    ],\n    key=['D', 'W'],  # removed NB\n    **autotune_cache_kwargs,\n)\n@triton.jit\ndef causal_conv1d_fwd_kernel(\n    x, y, weight, bias, residual, cu_seqlens, initial_state, chunk_indices,\n    B, T,\n    D: tl.constexpr, W: tl.constexpr,\n    BT: tl.constexpr, BW: tl.constexpr, BD: tl.constexpr,\n    NB,  # removed constexpr\n    ACTIVATION: tl.constexpr,\n    HAS_WEIGHT: tl.constexpr, HAS_BIAS: tl.constexpr, HAS_RESIDUAL: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr, IS_VARLEN: tl.constexpr,\n):\n    i_d, i_t, i_b = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    if IS_VARLEN:\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int64), tl.load(cu_seqlens + i_n + 1).to(tl.int64)\n        T = eos - bos\n    else:\n        i_n = i_b\n        bos, eos = (i_b * T).to(tl.int64), (i_b * T + T).to(tl.int64)\n\n    o_d = i_d * BD + tl.arange(0, BD)\n    o_w = tl.arange(0, BW) + W - BW\n    m_d = o_d &lt; D\n    m_w = o_w &gt;= 0\n\n    if HAS_WEIGHT:\n        b_w = tl.load(weight + o_d[:, None] * W + o_w, mask=m_d[:, None] &amp; m_w, other=0).to(tl.float32)\n\n    b_y = tl.zeros((BT, BD), dtype=tl.float32)\n    if not USE_INITIAL_STATE:\n        for i_w in tl.static_range(-W + 1, 1):\n            p_yi = tl.make_block_ptr(x + bos * D, (T, D), (D, 1), (i_t * BT + i_w, i_d * BD), (BT, BD), (1, 0))\n            b_yi = tl.load(p_yi, boundary_check=(0, 1)).to(tl.float32)\n            if HAS_WEIGHT:\n                b_yi *= tl.sum(b_w * (o_w == (i_w + W - 1)), 1)\n            b_y += b_yi\n    elif i_t * BT &gt;= W:\n        for i_w in tl.static_range(-W + 1, 1):\n            p_yi = tl.make_block_ptr(x + bos * D, (T, D), (D, 1), (i_t * BT + i_w, i_d * BD), (BT, BD), (1, 0))\n            b_yi = tl.load(p_yi, boundary_check=(0, 1)).to(tl.float32)\n            if HAS_WEIGHT:\n                b_yi *= tl.sum(b_w * (o_w == (i_w + W - 1)), 1)\n            b_y += b_yi\n    else:\n        o_t = i_t * BT + tl.arange(0, BT)\n        for i_w in tl.static_range(-W + 1, 1):\n            o_x = o_t + i_w\n            m_x = ((o_x &gt;= 0) &amp; (o_x &lt; T))[:, None] &amp; m_d\n            m_c = ((o_x + W &gt;= 0) &amp; (o_x &lt; 0))[:, None] &amp; m_d\n            b_yi = tl.load(x + bos * D + o_x[:, None] * D + o_d, mask=m_x, other=0).to(tl.float32)\n            b_yi += tl.load(initial_state + i_n * D*W + o_d * W + (o_x + W)[:, None], mask=m_c, other=0).to(tl.float32)\n            if HAS_WEIGHT:\n                b_yi *= tl.sum(b_w * (o_w == (i_w + W - 1)), 1)\n            b_y += b_yi\n\n    if HAS_BIAS:\n        b_y += tl.load(bias + o_d, mask=m_d).to(tl.float32)\n    if ACTIVATION == 'swish' or ACTIVATION == 'silu':\n        b_y = b_y * tl.sigmoid(b_y)\n    if HAS_RESIDUAL:\n        p_residual = tl.make_block_ptr(residual + bos * D, (T, D), (D, 1), (i_t * BT, i_d * BD), (BT, BD), (1, 0))\n        b_residual = tl.load(p_residual, boundary_check=(0, 1))\n        b_y += b_residual\n\n    p_y = tl.make_block_ptr(y + bos * D, (T, D), (D, 1), (i_t * BT, i_d * BD), (BT, BD), (1, 0))\n    tl.store(p_y, tl.cast(b_y, dtype=p_y.dtype.element_ty, fp_downcast_rounding='rtne'), boundary_check=(0, 1))\n</code></pre>"},{"location":"api/#microfuel.hacks.causal_conv1d_bwd_kernel","title":"causal_conv1d_bwd_kernel","text":"<pre><code>causal_conv1d_bwd_kernel(\n    x,\n    y,\n    weight,\n    initial_state,\n    dh0,\n    dht,\n    dy,\n    dx,\n    dw,\n    db,\n    cu_seqlens,\n    chunk_indices,\n    B,\n    T,\n    D: tl.constexpr,\n    W: tl.constexpr,\n    BT: tl.constexpr,\n    BW: tl.constexpr,\n    BD: tl.constexpr,\n    NB,\n    ACTIVATION: tl.constexpr,\n    HAS_WEIGHT: tl.constexpr,\n    HAS_BIAS: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr,\n    USE_FINAL_STATE: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n)\n</code></pre> Source code in <code>src/microfuel/hacks.py</code> <pre><code>@triton.heuristics({\n    'HAS_WEIGHT': lambda args: args['dw'] is not None,\n    'HAS_BIAS': lambda args: args['db'] is not None,\n    'USE_INITIAL_STATE': lambda args: args['dh0'] is not None,\n    'USE_FINAL_STATE': lambda args: args['dht'] is not None,\n    'IS_VARLEN': lambda args: args['cu_seqlens'] is not None,\n})\n@triton.autotune(\n    configs=[\n        triton.Config({'BD': BD}, num_warps=num_warps)\n        for BD in [16, 32, 64, 128]\n        for num_warps in [4, 8, 16, 32]\n    ],\n    key=['D', 'W'],  # removed NB\n    **autotune_cache_kwargs,\n)\n@triton.jit\ndef causal_conv1d_bwd_kernel(\n    x, y, weight, initial_state, dh0, dht, dy, dx, dw, db, cu_seqlens, chunk_indices,\n    B, T,\n    D: tl.constexpr, W: tl.constexpr,\n    BT: tl.constexpr, BW: tl.constexpr, BD: tl.constexpr,\n    NB,  # removed constexpr\n    ACTIVATION: tl.constexpr,\n    HAS_WEIGHT: tl.constexpr, HAS_BIAS: tl.constexpr,\n    USE_INITIAL_STATE: tl.constexpr, USE_FINAL_STATE: tl.constexpr,\n    IS_VARLEN: tl.constexpr,\n):\n    i_d, i_t, i_b = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    if IS_VARLEN:\n        i_tg = i_t\n        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32)\n        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int64), tl.load(cu_seqlens + i_n + 1).to(tl.int64)\n        T = eos - bos\n    else:\n        i_tg = i_b * tl.num_programs(1) + i_t\n        i_n = i_b\n        bos, eos = (i_b * T).to(tl.int64), (i_b * T + T).to(tl.int64)\n\n    o_d = i_d * BD + tl.arange(0, BD)\n    o_w = tl.arange(0, BW) + W - BW\n    m_d = o_d &lt; D\n    m_w = o_w &gt;= 0\n\n    if HAS_WEIGHT:\n        p_x = tl.make_block_ptr(x + bos * D, (T, D), (D, 1), (i_t * BT, i_d * BD), (BT, BD), (1, 0))\n        b_x = tl.load(p_x, boundary_check=(0, 1))\n        b_w = tl.load(weight + o_d[:, None] * W + o_w, mask=m_d[:, None] &amp; m_w, other=0)\n\n    b_dx = tl.zeros((BT, BD), dtype=tl.float32)\n    if HAS_BIAS:\n        b_db = tl.zeros((BD,), dtype=tl.float32)\n\n    if not USE_FINAL_STATE:\n        for i_w in tl.static_range(0, W):\n            p_dy = tl.make_block_ptr(dy + bos * D, (T, D), (D, 1), (i_t * BT + i_w, i_d * BD), (BT, BD), (1, 0))\n            b_dy = tl.load(p_dy, boundary_check=(0, 1)).to(tl.float32)\n            if ACTIVATION == 'swish' or ACTIVATION == 'silu':\n                p_y = tl.make_block_ptr(y + bos * D, (T, D), (D, 1), (i_t * BT + i_w, i_d * BD), (BT, BD), (1, 0))\n                b_y = tl.load(p_y, boundary_check=(0, 1)).to(tl.float32)\n                b_ys = tl.sigmoid(b_y)\n                b_dy = b_dy * b_ys * (1 + b_y * (1 - b_ys))\n            b_wdy = b_dy\n            if HAS_WEIGHT:\n                b_wdy = b_wdy * tl.sum(b_w * (o_w == (W - i_w - 1)), 1)\n                b_dw = tl.sum(b_dy * b_x, 0)\n                tl.store(dw + i_tg * D*W + o_d * W + W - i_w - 1, b_dw.to(dw.dtype.element_ty), mask=m_d)\n            if HAS_BIAS and i_w == 0:\n                b_db += tl.sum(b_dy, 0)\n            b_dx += b_wdy\n    elif i_t * BT &gt;= W:\n        for i_w in tl.static_range(0, W):\n            p_dy = tl.make_block_ptr(dy + bos * D, (T, D), (D, 1), (i_t * BT + i_w, i_d * BD), (BT, BD), (1, 0))\n            b_dy = tl.load(p_dy, boundary_check=(0, 1)).to(tl.float32)\n            if ACTIVATION == 'swish' or ACTIVATION == 'silu':\n                p_y = tl.make_block_ptr(y + bos * D, (T, D), (D, 1), (i_t * BT + i_w, i_d * BD), (BT, BD), (1, 0))\n                b_y = tl.load(p_y, boundary_check=(0, 1)).to(tl.float32)\n                b_ys = tl.sigmoid(b_y)\n                b_dy = b_dy * b_ys * (1 + b_y * (1 - b_ys))\n            b_wdy = b_dy\n            if HAS_WEIGHT:\n                b_wdy = b_wdy * tl.sum(b_w * (o_w == (W - i_w - 1)), 1)\n                b_dw = tl.sum(b_dy * b_x, 0)\n                tl.store(dw + i_tg * D*W + o_d * W + W - i_w - 1, b_dw.to(dw.dtype.element_ty), mask=m_d)\n            if HAS_BIAS and i_w == 0:\n                b_db += tl.sum(b_dy, 0)\n            b_dx += b_wdy\n    else:\n        o_t = i_t * BT + tl.arange(0, BT)\n        for i_w in tl.static_range(0, W):\n            p_dy = tl.make_block_ptr(dy + bos * D, (T, D), (D, 1), (i_t * BT + i_w, i_d * BD), (BT, BD), (1, 0))\n            b_dy_shift = tl.load(p_dy, boundary_check=(0, 1)).to(tl.float32)\n            if ACTIVATION == 'swish' or ACTIVATION == 'silu':\n                p_y = tl.make_block_ptr(y + bos * D, (T, D), (D, 1), (i_t * BT + i_w, i_d * BD), (BT, BD), (1, 0))\n                b_y_shift = tl.load(p_y, boundary_check=(0, 1)).to(tl.float32)\n                b_ys = tl.sigmoid(b_y_shift)\n                b_dy_shift = b_dy_shift * b_ys * (1 + b_y_shift * (1 - b_ys))\n            if HAS_WEIGHT:\n                b_dw = tl.sum(b_dy_shift * b_x, 0)\n                if USE_INITIAL_STATE:\n                    mask_head_rows = (o_t &lt; i_w)\n                    b_dy_head = tl.load(dy + bos * D + o_t[:, None] * D + o_d, mask=(mask_head_rows[:, None] &amp; m_d[None, :]), other=0.0).to(tl.float32)\n                    if ACTIVATION == 'swish' or ACTIVATION == 'silu':\n                        b_y_head = tl.load(y + bos * D + o_t[:, None] * D + o_d, mask=(mask_head_rows[:, None] &amp; m_d[None, :]), other=0.0).to(tl.float32)\n                        b_ys_head = tl.sigmoid(b_y_head)\n                        b_dy_head = b_dy_head * b_ys_head * (1 + b_y_head * (1 - b_ys_head))\n                    o_c = W - i_w + o_t\n                    mask_c = (mask_head_rows &amp; (o_c &gt;= 1) &amp; (o_c &lt; W))\n                    b_xc = tl.load(initial_state + i_n * D * W + o_d[None, :] * W + o_c[:, None], mask=(mask_c[:, None] &amp; m_d[None, :]), other=0.0).to(tl.float32)\n                    b_dw += tl.sum(b_dy_head * b_xc, 0)\n                tl.store(dw + i_tg * D * W + o_d * W + W - i_w - 1, b_dw.to(dw.dtype.element_ty), mask=m_d)\n\n            if HAS_BIAS and i_w == 0:\n                b_db += tl.sum(b_dy_shift, 0)\n            b_wdy = b_dy_shift if not HAS_WEIGHT else (b_dy_shift * tl.sum(b_w * (o_w == (W - i_w - 1)), 1))\n            b_dx += b_wdy\n\n        if USE_INITIAL_STATE:\n            p_dy0 = tl.make_block_ptr(dy + bos * D, (T, D), (D, 1), (i_t * BT, i_d * BD), (BT, BD), (1, 0))\n            b_dy0 = tl.load(p_dy0, boundary_check=(0, 1)).to(tl.float32)\n            if ACTIVATION == 'swish' or ACTIVATION == 'silu':\n                p_y0 = tl.make_block_ptr(y + bos * D, (T, D), (D, 1), (i_t * BT, i_d * BD), (BT, BD), (1, 0))\n                b_y0 = tl.load(p_y0, boundary_check=(0, 1)).to(tl.float32)\n                b_ys0 = tl.sigmoid(b_y0)\n                b_dy0 = b_dy0 * b_ys0 * (1 + b_y0 * (1 - b_ys0))\n            for i_w in tl.static_range(1, W):\n                m_rows = (o_t &lt; i_w)\n                if HAS_WEIGHT:\n                    w_idx_rows = i_w - 1 - o_t\n                    w_mask = (o_w[None, :] == w_idx_rows[:, None])\n                    w_pick = tl.sum(b_w[None, :, :] * w_mask[:, None, :], 2)\n                else:\n                    w_pick = 1.0\n                contrib = (b_dy0 * w_pick).to(tl.float32)\n                contrib = tl.where(m_rows[:, None] &amp; m_d[None, :], contrib, 0.0)\n                b_dh0_s = tl.sum(contrib, 0)\n                tl.store(dh0 + i_t * B * D * W + i_n * D * W + o_d * W + i_w, b_dh0_s.to(dh0.dtype.element_ty, fp_downcast_rounding='rtne'), mask=m_d)\n\n    if HAS_BIAS:\n        b_db = tl.cast(b_db, dtype=db.dtype.element_ty, fp_downcast_rounding='rtne')\n        tl.store(db + i_tg * D + o_d, b_db, mask=m_d)\n\n    if USE_FINAL_STATE:\n        if i_t * BT + BT &gt;= T-W:\n            start_tok = max(0, T - (W - 1))\n            offset = i_t * BT + tl.arange(0, BT)\n            tok_idx = offset - start_tok\n            mask = (offset &gt;= start_tok) &amp; (offset &lt; T)\n            w_idx = 1 + tok_idx\n            dht_off = i_n * D * W + o_d[None, :] * W + w_idx[:, None]\n            b_dht = tl.load(dht + dht_off, mask=mask[:, None] &amp; m_d[None, :], other=0.).to(tl.float32)\n            b_dx += b_dht\n\n    p_dx = tl.make_block_ptr(dx + bos * D, (T, D), (D, 1), (i_t * BT, i_d * BD), (BT, BD), (1, 0))\n    tl.store(p_dx, tl.cast(b_dx, dtype=p_dx.dtype.element_ty, fp_downcast_rounding='rtne'), boundary_check=(0, 1))\n</code></pre>"},{"location":"api/#microfuel.hacks.causal_conv1d_fwd","title":"causal_conv1d_fwd","text":"<pre><code>causal_conv1d_fwd(\n    x: torch.Tensor,\n    weight: torch.Tensor,\n    bias: torch.Tensor,\n    residual: torch.Tensor,\n    initial_state: torch.Tensor | None = None,\n    output_final_state: bool = False,\n    activation: str | None = None,\n    cu_seqlens: torch.Tensor | None = None,\n) -&gt; torch.Tensor\n</code></pre> Source code in <code>src/microfuel/hacks.py</code> <pre><code>@input_guard\ndef causal_conv1d_fwd(\n    x: torch.Tensor,\n    weight: torch.Tensor,\n    bias: torch.Tensor,\n    residual: torch.Tensor,\n    initial_state: torch.Tensor | None = None,\n    output_final_state: bool = False,\n    activation: str | None = None,\n    cu_seqlens: torch.Tensor | None = None,\n) -&gt; torch.Tensor:\n    shape = x.shape\n    if x.shape[-1] != weight.shape[0]:\n        x = rearrange(x, 'b t ... -&gt; b t (...)')\n    B, T, D, W = *x.shape, weight.shape[1]\n\n    # HACK: fix BT to constant to avoid recompile on varlen batches\n    BT = FIXED_BT_CONV\n    BW = triton.next_power_of_2(W)\n    chunk_indices = prepare_chunk_indices(cu_seqlens, BT) if cu_seqlens is not None else None\n    NT = len(chunk_indices) if cu_seqlens is not None else triton.cdiv(T, BT)\n    NB = triton.cdiv(B*T, 1024)\n\n    y = torch.empty_like(x)\n    def grid(meta): return (triton.cdiv(D, meta['BD']), NT, B)\n    causal_conv1d_fwd_kernel[grid](\n        x=x, y=y, weight=weight, bias=bias, residual=residual,\n        cu_seqlens=cu_seqlens, initial_state=initial_state, chunk_indices=chunk_indices,\n        B=B, T=T, D=D, W=W, BT=BT, BW=BW, NB=NB,\n        ACTIVATION=activation,\n    )\n    final_state = None\n    if output_final_state:\n        # NOTE: we use the original util since it doesn't depend on the kernel modifications\n        final_state = convolution.causal_conv1d_update_states(\n            x=x, state_len=W, initial_state=initial_state, cu_seqlens=cu_seqlens,\n        )\n    return y.view(shape), final_state\n</code></pre>"},{"location":"api/#microfuel.hacks.causal_conv1d_bwd","title":"causal_conv1d_bwd","text":"<pre><code>causal_conv1d_bwd(\n    x: torch.Tensor,\n    dy: torch.Tensor,\n    dht: torch.Tensor,\n    weight: torch.Tensor | None = None,\n    bias: torch.Tensor | None = None,\n    residual: torch.Tensor | None = None,\n    initial_state: torch.Tensor | None = None,\n    activation: str | None = None,\n    cu_seqlens: torch.Tensor | None = None,\n)\n</code></pre> Source code in <code>src/microfuel/hacks.py</code> <pre><code>def causal_conv1d_bwd(\n    x: torch.Tensor,\n    dy: torch.Tensor,\n    dht: torch.Tensor,\n    weight: torch.Tensor | None = None,\n    bias: torch.Tensor | None = None,\n    residual: torch.Tensor | None = None,\n    initial_state: torch.Tensor | None = None,\n    activation: str | None = None,\n    cu_seqlens: torch.Tensor | None = None,\n):\n    shape = x.shape\n    if x.shape[-1] != weight.shape[0]:\n        x = rearrange(x, 'b t ... -&gt; b t (...)')\n    B, T, D = x.shape\n    W = weight.shape[1] if weight is not None else None\n\n    # HACK: fix BT\n    BT = FIXED_BT_CONV\n    BW = triton.next_power_of_2(W)\n    chunk_indices = prepare_chunk_indices(cu_seqlens, BT) if cu_seqlens is not None else None\n    NT = len(chunk_indices) if cu_seqlens is not None else triton.cdiv(T, BT)\n    NB = triton.cdiv(B*T, 1024)\n\n    y = None\n    if activation is not None:\n        y, _ = causal_conv1d_fwd(\n            x=x, weight=weight, bias=bias, residual=None, initial_state=initial_state,\n            activation=None, cu_seqlens=cu_seqlens, output_final_state=False,\n        )\n    dx = torch.empty_like(x)\n    dw = weight.new_empty(B*NT, *weight.shape, dtype=torch.float) if weight is not None else None\n    db = bias.new_empty(B*NT, *bias.shape, dtype=torch.float) if bias is not None else None\n    dr = dy if residual is not None else None\n    dh0 = initial_state.new_zeros(min(NT, triton.cdiv(W, BT)), *initial_state.shape) if initial_state is not None else None\n\n    def grid(meta): return (triton.cdiv(D, meta['BD']), NT, B)\n    causal_conv1d_bwd_kernel[grid](\n        x=x, y=y, weight=weight, initial_state=initial_state, dh0=dh0, dht=dht,\n        dy=dy, dx=dx, dw=dw, db=db, cu_seqlens=cu_seqlens, chunk_indices=chunk_indices,\n        B=B, T=T, D=D, W=W, BT=BT, BW=BW, NB=NB,\n        ACTIVATION=activation,\n    )\n    if weight is not None:\n        dw = dw.sum(0).to(weight)\n    if bias is not None:\n        db = db.sum(0).to(bias)\n    if initial_state is not None:\n        dh0 = dh0.sum(0, dtype=torch.float32).to(initial_state)\n\n    return dx.view(shape), dw, db, dr, dh0\n</code></pre>"},{"location":"api/#microfuel.hacks.l2norm_fwd_kernel","title":"l2norm_fwd_kernel","text":"<pre><code>l2norm_fwd_kernel(\n    x,\n    y,\n    rstd,\n    eps,\n    T,\n    D: tl.constexpr,\n    BD: tl.constexpr,\n    NB,\n    BT: tl.constexpr,\n)\n</code></pre> Source code in <code>src/microfuel/hacks.py</code> <pre><code>@triton.autotune(\n    configs=[\n        triton.Config({'BT': BT}, num_warps=num_warps)\n        for num_warps in [1, 2, 4, 8, 16]\n        for BT in BT_LIST\n    ],\n    key=['D'],  # NB removed\n    **autotune_cache_kwargs,\n)\n@triton.jit\ndef l2norm_fwd_kernel(\n    x, y, rstd, eps,\n    T,  # removed constexpr\n    D: tl.constexpr, BD: tl.constexpr, \n    NB,  # removed constexpr\n    BT: tl.constexpr,\n):\n    i_t = tl.program_id(0)\n    p_x = tl.make_block_ptr(x, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))\n    p_y = tl.make_block_ptr(y, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))\n    p_rstd = tl.make_block_ptr(rstd, (T,), (1,), (i_t * BT,), (BT,), (0,))\n\n    b_x = tl.load(p_x, boundary_check=(0, 1)).to(tl.float32)\n    b_rstd = 1 / tl.sqrt(tl.sum(b_x * b_x, 1) + eps)\n    b_y = b_x * b_rstd[:, None]\n\n    tl.store(p_y, b_y.to(p_y.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_rstd, b_rstd.to(p_rstd.dtype.element_ty), boundary_check=(0,))\n</code></pre>"},{"location":"api/#microfuel.hacks.l2norm_bwd_kernel","title":"l2norm_bwd_kernel","text":"<pre><code>l2norm_bwd_kernel(\n    y,\n    rstd,\n    dy,\n    dx,\n    eps,\n    T,\n    D: tl.constexpr,\n    BD: tl.constexpr,\n    NB,\n    BT: tl.constexpr,\n)\n</code></pre> Source code in <code>src/microfuel/hacks.py</code> <pre><code>@triton.autotune(\n    configs=[\n        triton.Config({'BT': BT}, num_warps=num_warps)\n        for num_warps in [1, 2, 4, 8, 16]\n        for BT in BT_LIST\n    ],\n    key=['D'],  # NB removed\n    **autotune_cache_kwargs,\n)\n@triton.jit\ndef l2norm_bwd_kernel(\n    y, rstd, dy, dx, eps,\n    T,  # removed constexpr\n    D: tl.constexpr, BD: tl.constexpr,\n    NB,  # removed constexpr\n    BT: tl.constexpr,\n):\n    i_t = tl.program_id(0)\n    p_y = tl.make_block_ptr(y, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))\n    p_rstd = tl.make_block_ptr(rstd, (T,), (1,), (i_t * BT,), (BT,), (0,))\n    p_dy = tl.make_block_ptr(dy, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))\n    p_dx = tl.make_block_ptr(dx, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))\n\n    b_y = tl.load(p_y, boundary_check=(0, 1)).to(tl.float32)\n    b_rstd = tl.load(p_rstd, boundary_check=(0,)).to(tl.float32)\n    b_dy = tl.load(p_dy, boundary_check=(0, 1)).to(tl.float32)\n    b_dx = b_dy * b_rstd[:, None] - tl.sum(b_dy * b_y, 1)[:, None] * b_y * b_rstd[:, None]\n    tl.store(p_dx, b_dx.to(p_dx.dtype.element_ty), boundary_check=(0, 1))\n</code></pre>"},{"location":"api/#microfuel.hacks.l2norm_fwd","title":"l2norm_fwd","text":"<pre><code>l2norm_fwd(\n    x: torch.Tensor,\n    eps: float = 1e-06,\n    output_dtype: torch.dtype | None = None,\n)\n</code></pre> Source code in <code>src/microfuel/hacks.py</code> <pre><code>def l2norm_fwd(\n    x: torch.Tensor,\n    eps: float = 1e-6,\n    output_dtype: torch.dtype | None = None,\n):\n    x_shape_og = x.shape\n    x = x.view(-1, x.shape[-1])\n    if output_dtype is None:\n        y = torch.empty_like(x)\n    else:\n        y = torch.empty_like(x, dtype=output_dtype)\n    assert y.stride(-1) == 1\n    T, D = x.shape[0], x.shape[-1]\n    MAX_FUSED_SIZE = 65536 // x.element_size()\n    BD = min(MAX_FUSED_SIZE, triton.next_power_of_2(D))\n    if D &gt; BD:\n        raise RuntimeError(\"This layer doesn't support feature dim &gt;= 64KB.\")\n\n    rstd = torch.empty((T,), dtype=torch.float32, device=x.device)\n    if D &lt;= 512:\n        NB = triton.cdiv(T, 2048)\n        def grid(meta): return (triton.cdiv(T, meta['BT']), )\n        l2norm_fwd_kernel[grid](\n            x=x, y=y, rstd=rstd, eps=eps, T=T, D=D, BD=BD, NB=NB,\n        )\n    else:\n        # fallback to original kernel1 for large D (no NB/T constexpr issues there)\n        l2norm.l2norm_fwd_kernel1[(T,)](\n            x=x, y=y, rstd=rstd, eps=eps, D=D, BD=BD,\n        )\n    return y.view(x_shape_og), rstd.view(x_shape_og[:-1])\n</code></pre>"},{"location":"api/#microfuel.hacks.l2norm_bwd","title":"l2norm_bwd","text":"<pre><code>l2norm_bwd(\n    y: torch.Tensor,\n    rstd: torch.Tensor,\n    dy: torch.Tensor,\n    eps: float = 1e-06,\n)\n</code></pre> Source code in <code>src/microfuel/hacks.py</code> <pre><code>def l2norm_bwd(\n    y: torch.Tensor,\n    rstd: torch.Tensor,\n    dy: torch.Tensor,\n    eps: float = 1e-6,\n):\n    y_shape_og = y.shape\n    y = y.view(-1, dy.shape[-1])\n    dy = dy.view(-1, dy.shape[-1])\n    assert dy.shape == y.shape\n    dx = torch.empty_like(y)\n    T, D = y.shape[0], y.shape[-1]\n    MAX_FUSED_SIZE = 65536 // y.element_size()\n    BD = min(MAX_FUSED_SIZE, triton.next_power_of_2(D))\n    if D &gt; BD:\n        raise RuntimeError(\"This layer norm doesn't support feature dim &gt;= 64KB.\")\n\n    if D &lt;= 512:\n        NB = triton.cdiv(T, 2048)\n        def grid(meta): return (triton.cdiv(T, meta['BT']), )\n        l2norm_bwd_kernel[grid](\n            y=y, rstd=rstd, dy=dy, dx=dx, eps=eps, T=T, D=D, BD=BD, NB=NB,\n        )\n    else:\n        l2norm.l2norm_bwd_kernel1[(T,)](\n            y=y, rstd=rstd, dy=dy, dx=dx, eps=eps, D=D, BD=BD,\n        )\n\n    return dx.view(y_shape_og)\n</code></pre>"},{"location":"api/#microfuel.hacks.layer_norm_gated_fwd_kernel","title":"layer_norm_gated_fwd_kernel","text":"<pre><code>layer_norm_gated_fwd_kernel(\n    x,\n    g,\n    y,\n    w,\n    b,\n    residual,\n    residual_out,\n    mean,\n    rstd,\n    eps,\n    T,\n    D: tl.constexpr,\n    BT: tl.constexpr,\n    BD: tl.constexpr,\n    NB,\n    ACTIVATION: tl.constexpr,\n    IS_RMS_NORM: tl.constexpr,\n    STORE_RESIDUAL_OUT: tl.constexpr,\n    HAS_RESIDUAL: tl.constexpr,\n    HAS_WEIGHT: tl.constexpr,\n    HAS_BIAS: tl.constexpr,\n)\n</code></pre> Source code in <code>src/microfuel/hacks.py</code> <pre><code>@triton.heuristics({\n    'STORE_RESIDUAL_OUT': lambda args: args['residual_out'] is not None,\n    'HAS_RESIDUAL': lambda args: args['residual'] is not None,\n    'HAS_WEIGHT': lambda args: args['w'] is not None,\n    'HAS_BIAS': lambda args: args['b'] is not None,\n})\n@triton.autotune(\n    configs=[\n        triton.Config({'BT': BT}, num_warps=num_warps)\n        for BT in [16, 32, 64]\n        for num_warps in [4, 8, 16]\n    ],\n    key=['D', 'IS_RMS_NORM', 'STORE_RESIDUAL_OUT', 'HAS_RESIDUAL', 'HAS_WEIGHT'],  # NB removed\n    **autotune_cache_kwargs,\n)\n@triton.jit\ndef layer_norm_gated_fwd_kernel(\n    x, g, y, w, b, residual, residual_out, mean, rstd, eps,\n    T,\n    D: tl.constexpr, BT: tl.constexpr, BD: tl.constexpr,\n    NB,  # removed constexpr\n    ACTIVATION: tl.constexpr, IS_RMS_NORM: tl.constexpr,\n    STORE_RESIDUAL_OUT: tl.constexpr, HAS_RESIDUAL: tl.constexpr,\n    HAS_WEIGHT: tl.constexpr, HAS_BIAS: tl.constexpr,\n):\n    i_t = tl.program_id(0)\n    o_d = tl.arange(0, BD)\n    m_d = o_d &lt; D\n\n    p_x = tl.make_block_ptr(x, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))\n    b_x = tl.load(p_x, boundary_check=(0, 1)).to(tl.float32)\n    if HAS_RESIDUAL:\n        p_res = tl.make_block_ptr(residual, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))\n        b_x += tl.load(p_res, boundary_check=(0, 1)).to(tl.float32)\n    if STORE_RESIDUAL_OUT:\n        p_res_out = tl.make_block_ptr(residual_out, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))\n        tl.store(p_res_out, b_x.to(p_res_out.dtype.element_ty), boundary_check=(0, 1))\n    if not IS_RMS_NORM:\n        b_mean = tl.sum(b_x, axis=1) / D\n        p_mean = tl.make_block_ptr(mean, (T,), (1,), (i_t * BT,), (BT,), (0,))\n        tl.store(p_mean, b_mean.to(p_mean.dtype.element_ty), boundary_check=(0,))\n        b_xbar = tl.where(m_d[None, :], b_x - b_mean[:, None], 0.0)\n        b_var = tl.sum(b_xbar * b_xbar, axis=1) / D\n    else:\n        b_xbar = tl.where(m_d[None, :], b_x, 0.0)\n        b_var = tl.sum(b_xbar * b_xbar, axis=1) / D\n    b_rstd = 1 / tl.sqrt(b_var + eps)\n    p_rstd = tl.make_block_ptr(rstd, (T,), (1,), (i_t * BT,), (BT,), (0,))\n    tl.store(p_rstd, b_rstd.to(p_rstd.dtype.element_ty), boundary_check=(0,))\n\n    if HAS_WEIGHT:\n        b_w = tl.load(w + o_d, mask=m_d).to(tl.float32)\n    if HAS_BIAS:\n        b_b = tl.load(b + o_d, mask=m_d).to(tl.float32)\n    b_x_hat = (b_x - b_mean[:, None]) * b_rstd[:, None] if not IS_RMS_NORM else b_x * b_rstd[:, None]\n    b_y = b_x_hat * b_w[None, :] if HAS_WEIGHT else b_x_hat\n    if HAS_BIAS:\n        b_y = b_y + b_b[None, :]\n\n    p_g = tl.make_block_ptr(g, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))\n    b_g = tl.load(p_g, boundary_check=(0, 1)).to(tl.float32)\n    if ACTIVATION == 'swish' or ACTIVATION == 'silu':\n        b_y = b_y * b_g * tl.sigmoid(b_g)\n    elif ACTIVATION == 'sigmoid':\n        b_y = b_y * tl.sigmoid(b_g)\n\n    p_y = tl.make_block_ptr(y, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))\n    tl.store(p_y, b_y.to(p_y.dtype.element_ty), boundary_check=(0, 1))\n</code></pre>"},{"location":"api/#microfuel.hacks.layer_norm_gated_bwd_kernel","title":"layer_norm_gated_bwd_kernel","text":"<pre><code>layer_norm_gated_bwd_kernel(\n    x,\n    g,\n    w,\n    b,\n    y,\n    dy,\n    dx,\n    dg,\n    dw,\n    db,\n    dresidual,\n    dresidual_in,\n    mean,\n    rstd,\n    T,\n    BS,\n    D: tl.constexpr,\n    BT: tl.constexpr,\n    BD: tl.constexpr,\n    NB,\n    ACTIVATION: tl.constexpr,\n    IS_RMS_NORM: tl.constexpr,\n    STORE_DRESIDUAL: tl.constexpr,\n    HAS_DRESIDUAL: tl.constexpr,\n    HAS_WEIGHT: tl.constexpr,\n    HAS_BIAS: tl.constexpr,\n    RECOMPUTE_OUTPUT: tl.constexpr,\n)\n</code></pre> Source code in <code>src/microfuel/hacks.py</code> <pre><code>@triton.heuristics({\n    'HAS_DRESIDUAL': lambda args: args['dresidual'] is not None,\n    'HAS_WEIGHT': lambda args: args['w'] is not None,\n    'HAS_BIAS': lambda args: args['b'] is not None,\n    'RECOMPUTE_OUTPUT': lambda args: args['y'] is not None,\n})\n@triton.autotune(\n    configs=[\n        triton.Config({'BT': BT}, num_warps=num_warps)\n        for BT in [16, 32, 64]\n        for num_warps in [4, 8, 16]\n    ],\n    key=['D', 'IS_RMS_NORM', 'HAS_DRESIDUAL', 'HAS_WEIGHT'],  # NB removed\n    **autotune_cache_kwargs,\n)\n@triton.jit\ndef layer_norm_gated_bwd_kernel(\n    x, g, w, b, y, dy, dx, dg, dw, db, dresidual, dresidual_in, mean, rstd,\n    T, BS,\n    D: tl.constexpr, BT: tl.constexpr, BD: tl.constexpr,\n    NB,  # removed constexpr\n    ACTIVATION: tl.constexpr, IS_RMS_NORM: tl.constexpr,\n    STORE_DRESIDUAL: tl.constexpr, HAS_DRESIDUAL: tl.constexpr,\n    HAS_WEIGHT: tl.constexpr, HAS_BIAS: tl.constexpr, RECOMPUTE_OUTPUT: tl.constexpr,\n):\n    i_s = tl.program_id(0)\n    o_d = tl.arange(0, BD)\n    m_d = o_d &lt; D\n    if HAS_WEIGHT:\n        b_w = tl.load(w + o_d, mask=m_d).to(tl.float32)\n        b_dw = tl.zeros((BT, BD), dtype=tl.float32)\n    if HAS_BIAS:\n        b_b = tl.load(b + o_d, mask=m_d, other=0.0).to(tl.float32)\n        b_db = tl.zeros((BT, BD), dtype=tl.float32)\n\n    T = min(i_s * BS + BS, T)\n    for i_t in range(i_s * BS, T, BT):\n        p_x = tl.make_block_ptr(x, (T, D), (D, 1), (i_t, 0), (BT, BD), (1, 0))\n        p_g = tl.make_block_ptr(g, (T, D), (D, 1), (i_t, 0), (BT, BD), (1, 0))\n        p_dy = tl.make_block_ptr(dy, (T, D), (D, 1), (i_t, 0), (BT, BD), (1, 0))\n        p_dx = tl.make_block_ptr(dx, (T, D), (D, 1), (i_t, 0), (BT, BD), (1, 0))\n        p_dg = tl.make_block_ptr(dg, (T, D), (D, 1), (i_t, 0), (BT, BD), (1, 0))\n        b_x = tl.load(p_x, boundary_check=(0, 1)).to(tl.float32)\n        b_g = tl.load(p_g, boundary_check=(0, 1)).to(tl.float32)\n        b_dy = tl.load(p_dy, boundary_check=(0, 1)).to(tl.float32)\n\n        if not IS_RMS_NORM:\n            p_mean = tl.make_block_ptr(mean, (T,), (1,), (i_t,), (BT,), (0,))\n            b_mean = tl.load(p_mean, boundary_check=(0,))\n        p_rstd = tl.make_block_ptr(rstd, (T,), (1,), (i_t,), (BT,), (0,))\n        b_rstd = tl.load(p_rstd, boundary_check=(0,))\n        b_xhat = (b_x - b_mean[:, None]) * b_rstd[:, None] if not IS_RMS_NORM else b_x * b_rstd[:, None]\n        b_xhat = tl.where(m_d[None, :], b_xhat, 0.0)\n\n        b_y = b_xhat * b_w[None, :] if HAS_WEIGHT else b_xhat\n        if HAS_BIAS:\n            b_y = b_y + b_b[None, :]\n        if RECOMPUTE_OUTPUT:\n            p_y = tl.make_block_ptr(y, (T, D), (D, 1), (i_t, 0), (BT, BD), (1, 0))\n            tl.store(p_y, b_y.to(p_y.dtype.element_ty), boundary_check=(0, 1))\n\n        b_sigmoid_g = tl.sigmoid(b_g)\n        if ACTIVATION == 'swish' or ACTIVATION == 'silu':\n            b_dg = b_dy * b_y * (b_sigmoid_g + b_g * b_sigmoid_g * (1 - b_sigmoid_g))\n            b_dy = b_dy * b_g * b_sigmoid_g\n        elif ACTIVATION == 'sigmoid':\n            b_dg = b_dy * b_y * b_sigmoid_g * (1 - b_sigmoid_g)\n            b_dy = b_dy * b_sigmoid_g\n        b_wdy = b_dy\n\n        if HAS_WEIGHT or HAS_BIAS:\n            m_t = (i_t + tl.arange(0, BT)) &lt; T\n        if HAS_WEIGHT:\n            b_wdy = b_dy * b_w\n            b_dw += tl.where(m_t[:, None], b_dy * b_xhat, 0.0)\n        if HAS_BIAS:\n            b_db += tl.where(m_t[:, None], b_dy, 0.0)\n        if not IS_RMS_NORM:\n            b_c1 = tl.sum(b_xhat * b_wdy, axis=1) / D\n            b_c2 = tl.sum(b_wdy, axis=1) / D\n            b_dx = (b_wdy - (b_xhat * b_c1[:, None] + b_c2[:, None])) * b_rstd[:, None]\n        else:\n            b_c1 = tl.sum(b_xhat * b_wdy, axis=1) / D\n            b_dx = (b_wdy - b_xhat * b_c1[:, None]) * b_rstd[:, None]\n        if HAS_DRESIDUAL:\n            p_dres = tl.make_block_ptr(dresidual, (T, D), (D, 1), (i_t, 0), (BT, BD), (1, 0))\n            b_dres = tl.load(p_dres, boundary_check=(0, 1)).to(tl.float32)\n            b_dx += b_dres\n        if STORE_DRESIDUAL:\n            p_dres_in = tl.make_block_ptr(dresidual_in, (T, D), (D, 1), (i_t, 0), (BT, BD), (1, 0))\n            tl.store(p_dres_in, b_dx.to(p_dres_in.dtype.element_ty), boundary_check=(0, 1))\n\n        tl.store(p_dx, b_dx.to(p_dx.dtype.element_ty), boundary_check=(0, 1))\n        tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), boundary_check=(0, 1))\n\n    if HAS_WEIGHT:\n        tl.store(dw + i_s * D + o_d, tl.sum(b_dw, axis=0), mask=m_d)\n    if HAS_BIAS:\n        tl.store(db + i_s * D + o_d, tl.sum(b_db, axis=0), mask=m_d)\n</code></pre>"},{"location":"api/#microfuel.hacks.layer_norm_gated_fwd","title":"layer_norm_gated_fwd","text":"<pre><code>layer_norm_gated_fwd(\n    x: torch.Tensor,\n    g: torch.Tensor,\n    weight: torch.Tensor,\n    bias: torch.Tensor,\n    activation: str = \"swish\",\n    eps: float = 1e-05,\n    residual: torch.Tensor = None,\n    out_dtype: torch.dtype = None,\n    residual_dtype: torch.dtype = None,\n    is_rms_norm: bool = False,\n)\n</code></pre> Source code in <code>src/microfuel/hacks.py</code> <pre><code>def layer_norm_gated_fwd(\n    x: torch.Tensor,\n    g: torch.Tensor,\n    weight: torch.Tensor,\n    bias: torch.Tensor,\n    activation: str = 'swish',\n    eps: float = 1e-5,\n    residual: torch.Tensor = None,\n    out_dtype: torch.dtype = None,\n    residual_dtype: torch.dtype = None,\n    is_rms_norm: bool = False,\n):\n    if residual is not None:\n        residual_dtype = residual.dtype\n    T, D = x.shape\n    if residual is not None:\n        assert residual.shape == (T, D)\n    if weight is not None:\n        assert weight.shape == (D,)\n    if bias is not None:\n        assert bias.shape == (D,)\n    y = torch.empty_like(x, dtype=x.dtype if out_dtype is None else out_dtype)\n    if residual is not None or (residual_dtype is not None and residual_dtype != x.dtype):\n        residual_out = torch.empty(T, D, device=x.device, dtype=residual_dtype)\n    else:\n        residual_out = None\n    mean = torch.empty((T,), dtype=torch.float, device=x.device) if not is_rms_norm else None\n    rstd = torch.empty((T,), dtype=torch.float, device=x.device)\n    MAX_FUSED_SIZE = 65536 // x.element_size()\n    BD = min(MAX_FUSED_SIZE, triton.next_power_of_2(D))\n    if D &gt; BD:\n        raise RuntimeError(\"This layer norm doesn't support feature dim &gt;= 64KB.\")\n\n    if D &lt;= 512:\n        NB = triton.cdiv(T, 2048)\n        def grid(meta): return (triton.cdiv(T, meta['BT']),)\n        layer_norm_gated_fwd_kernel[grid](\n            x=x, g=g, y=y, w=weight, b=bias, residual=residual, residual_out=residual_out,\n            mean=mean, rstd=rstd, eps=eps, T=T, D=D, BD=BD, NB=NB,\n            ACTIVATION=activation, IS_RMS_NORM=is_rms_norm,\n        )\n    else:\n        fused_norm_gate.layer_norm_gated_fwd_kernel1[(T,)](\n            x=x, g=g, y=y, w=weight, b=bias, residual=residual, residual_out=residual_out,\n            mean=mean, rstd=rstd, eps=eps, D=D, BD=BD,\n            ACTIVATION=activation, IS_RMS_NORM=is_rms_norm,\n        )\n    return y, mean, rstd, residual_out if residual_out is not None else x\n</code></pre>"},{"location":"api/#microfuel.hacks.layer_norm_gated_bwd","title":"layer_norm_gated_bwd","text":"<pre><code>layer_norm_gated_bwd(\n    dy: torch.Tensor,\n    x: torch.Tensor,\n    g: torch.Tensor,\n    weight: torch.Tensor,\n    bias: torch.Tensor,\n    activation: str = \"swish\",\n    eps: float = 1e-05,\n    mean: torch.Tensor = None,\n    rstd: torch.Tensor = None,\n    dresidual: torch.Tensor = None,\n    has_residual: bool = False,\n    is_rms_norm: bool = False,\n    x_dtype: torch.dtype = None,\n    recompute_output: bool = False,\n)\n</code></pre> Source code in <code>src/microfuel/hacks.py</code> <pre><code>def layer_norm_gated_bwd(\n    dy: torch.Tensor,\n    x: torch.Tensor,\n    g: torch.Tensor,\n    weight: torch.Tensor,\n    bias: torch.Tensor,\n    activation: str = 'swish',\n    eps: float = 1e-5,\n    mean: torch.Tensor = None,\n    rstd: torch.Tensor = None,\n    dresidual: torch.Tensor = None,\n    has_residual: bool = False,\n    is_rms_norm: bool = False,\n    x_dtype: torch.dtype = None,\n    recompute_output: bool = False,\n):\n    T, D = x.shape\n    dx = torch.empty_like(x) if x_dtype is None else torch.empty(T, D, dtype=x_dtype, device=x.device)\n    dg = torch.empty_like(g) if x_dtype is None else torch.empty(T, D, dtype=x_dtype, device=x.device)\n    dresidual_in = torch.empty_like(x) if has_residual and dx.dtype != x.dtype else None\n    y = torch.empty(T, D, dtype=dy.dtype, device=dy.device) if recompute_output else None\n\n    MAX_FUSED_SIZE = 65536 // x.element_size()\n    BD = min(MAX_FUSED_SIZE, triton.next_power_of_2(D))\n    if D &gt; BD:\n        raise RuntimeError(\"This layer norm doesn't support feature dim &gt;= 64KB.\")\n    NS = get_multiprocessor_count(x.device.index)\n    BS = math.ceil(T / NS)\n\n    dw = torch.empty((NS, D), dtype=torch.float, device=weight.device) if weight is not None else None\n    db = torch.empty((NS, D), dtype=torch.float, device=bias.device) if bias is not None else None\n    grid = (NS,)\n\n    if D &lt;= 512:\n        NB = triton.cdiv(T, 2048)\n        layer_norm_gated_bwd_kernel[grid](\n            x=x, g=g, w=weight, b=bias, y=y, dy=dy, dx=dx, dg=dg, dw=dw, db=db,\n            dresidual=dresidual, dresidual_in=dresidual_in, mean=mean, rstd=rstd,\n            T=T, D=D, BS=BS, BD=BD, NB=NB,\n            ACTIVATION=activation, IS_RMS_NORM=is_rms_norm,\n            STORE_DRESIDUAL=dresidual_in is not None,\n        )\n    else:\n        fused_norm_gate.layer_norm_gated_bwd_kernel1[grid](\n            x=x, g=g, w=weight, b=bias, y=y, dy=dy, dx=dx, dg=dg, dw=dw, db=db,\n            dresidual=dresidual, dresidual_in=dresidual_in, mean=mean, rstd=rstd,\n            T=T, D=D, BS=BS, BD=BD,\n            ACTIVATION=activation, IS_RMS_NORM=is_rms_norm,\n            STORE_DRESIDUAL=dresidual_in is not None,\n        )\n    dw = dw.sum(0).to(weight.dtype) if weight is not None else None\n    db = db.sum(0).to(bias.dtype) if bias is not None else None\n    if has_residual and dx.dtype == x.dtype:\n        dresidual_in = dx\n    return (dx, dg, dw, db, dresidual_in) if not recompute_output else (dx, dg, dw, db, dresidual_in, y)\n</code></pre>"},{"location":"api/#microfuel.hacks.install_optimized_kernels_","title":"install_optimized_kernels_","text":"<pre><code>install_optimized_kernels_()\n</code></pre> <p>Patches FLA modules with optimized versions to reduce JIT recompilation.</p> Source code in <code>src/microfuel/hacks.py</code> <pre><code>def install_optimized_kernels_():\n    \"\"\"Patches FLA modules with optimized versions to reduce JIT recompilation.\"\"\"\n    convolution.causal_conv1d_fwd_kernel = causal_conv1d_fwd_kernel\n    convolution.causal_conv1d_bwd_kernel = causal_conv1d_bwd_kernel\n    convolution.causal_conv1d_fwd = causal_conv1d_fwd\n    convolution.causal_conv1d_bwd = causal_conv1d_bwd\n\n    l2norm.l2norm_fwd_kernel = l2norm_fwd_kernel\n    l2norm.l2norm_bwd_kernel = l2norm_bwd_kernel\n    l2norm.l2norm_fwd = l2norm_fwd\n    l2norm.l2norm_bwd = l2norm_bwd\n\n    fused_norm_gate.layer_norm_gated_fwd_kernel = layer_norm_gated_fwd_kernel\n    fused_norm_gate.layer_norm_gated_bwd_kernel = layer_norm_gated_bwd_kernel\n    fused_norm_gate.layer_norm_gated_fwd = layer_norm_gated_fwd\n    fused_norm_gate.layer_norm_gated_bwd = layer_norm_gated_bwd\n</code></pre>"},{"location":"api/#microfuel.model","title":"model","text":""},{"location":"api/#microfuel.model.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(__name__)\n</code></pre>"},{"location":"api/#microfuel.model.ZeroCentredRMSNorm","title":"ZeroCentredRMSNorm","text":"<p>               Bases: <code>nn.Module</code></p> <p>Avoids abnormal amplification of some weights in the original QK-norm. During regularisation and weight decay, <code>weight</code> will be pushed near 0.</p> <p>See: https://ceramic.ai/blog/zerocentered</p> Source code in <code>src/microfuel/model.py</code> <pre><code>class ZeroCentredRMSNorm(nn.Module):\n    \"\"\"Avoids abnormal amplification of some weights in the original QK-norm.\n    During regularisation and weight decay, `weight` will be pushed near 0.\n\n    See: https://ceramic.ai/blog/zerocentered\"\"\"\n\n    def __init__(self, hidden_size: int, eps: float = 1e-6):\n        super().__init__()\n        self.weight = nn.Parameter(torch.zeros(hidden_size))\n        self.variance_epsilon = eps\n\n    def forward(self, hidden_states: torch.Tensor) -&gt; torch.Tensor:\n        input_dtype = hidden_states.dtype\n        hidden_states = hidden_states.to(torch.float32)\n        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n        return (hidden_states * (1.0 + self.weight)).to(input_dtype)\n</code></pre>"},{"location":"api/#microfuel.model.ZeroCentredRMSNorm.weight","title":"weight  <code>instance-attribute</code>","text":"<pre><code>weight = nn.Parameter(torch.zeros(hidden_size))\n</code></pre>"},{"location":"api/#microfuel.model.ZeroCentredRMSNorm.variance_epsilon","title":"variance_epsilon  <code>instance-attribute</code>","text":"<pre><code>variance_epsilon = eps\n</code></pre>"},{"location":"api/#microfuel.model.ZeroCentredRMSNorm.__init__","title":"__init__","text":"<pre><code>__init__(hidden_size: int, eps: float = 1e-06)\n</code></pre> Source code in <code>src/microfuel/model.py</code> <pre><code>def __init__(self, hidden_size: int, eps: float = 1e-6):\n    super().__init__()\n    self.weight = nn.Parameter(torch.zeros(hidden_size))\n    self.variance_epsilon = eps\n</code></pre>"},{"location":"api/#microfuel.model.ZeroCentredRMSNorm.forward","title":"forward","text":"<pre><code>forward(hidden_states: torch.Tensor) -&gt; torch.Tensor\n</code></pre> Source code in <code>src/microfuel/model.py</code> <pre><code>def forward(self, hidden_states: torch.Tensor) -&gt; torch.Tensor:\n    input_dtype = hidden_states.dtype\n    hidden_states = hidden_states.to(torch.float32)\n    variance = hidden_states.pow(2).mean(-1, keepdim=True)\n    hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n    return (hidden_states * (1.0 + self.weight)).to(input_dtype)\n</code></pre>"},{"location":"api/#microfuel.model.Pooler","title":"Pooler","text":"<p>               Bases: <code>nn.Module</code></p> Source code in <code>src/microfuel/model.py</code> <pre><code>class Pooler(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x: torch.Tensor, cu_seqlens: torch.Tensor) -&gt; torch.Tensor:\n        return x[cu_seqlens[1:] - 1]\n</code></pre>"},{"location":"api/#microfuel.model.Pooler.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> Source code in <code>src/microfuel/model.py</code> <pre><code>def __init__(self):\n    super().__init__()\n</code></pre>"},{"location":"api/#microfuel.model.Pooler.forward","title":"forward","text":"<pre><code>forward(\n    x: torch.Tensor, cu_seqlens: torch.Tensor\n) -&gt; torch.Tensor\n</code></pre> Source code in <code>src/microfuel/model.py</code> <pre><code>def forward(self, x: torch.Tensor, cu_seqlens: torch.Tensor) -&gt; torch.Tensor:\n    return x[cu_seqlens[1:] - 1]\n</code></pre>"},{"location":"api/#microfuel.model.LinearAttentionBlock","title":"LinearAttentionBlock","text":"<p>               Bases: <code>nn.Module</code></p> Source code in <code>src/microfuel/model.py</code> <pre><code>class LinearAttentionBlock(nn.Module):\n    def __init__(self, hidden_size: int, num_heads: int, head_dim: int):\n        super().__init__()\n        self.norm = ZeroCentredRMSNorm(hidden_size)\n        self.gdn = GatedDeltaNet(hidden_size=hidden_size, num_heads=num_heads, head_dim=head_dim)\n\n    def forward(self, x: torch.Tensor, cu_seqlens: torch.Tensor) -&gt; torch.Tensor:\n        residual = x\n        x = self.norm(x)\n        x, _, _ = self.gdn(x.unsqueeze(0), cu_seqlens=cu_seqlens)\n        x = x.squeeze(0)\n        x = x + residual\n        return x\n</code></pre>"},{"location":"api/#microfuel.model.LinearAttentionBlock.norm","title":"norm  <code>instance-attribute</code>","text":"<pre><code>norm = ZeroCentredRMSNorm(hidden_size)\n</code></pre>"},{"location":"api/#microfuel.model.LinearAttentionBlock.gdn","title":"gdn  <code>instance-attribute</code>","text":"<pre><code>gdn = GatedDeltaNet(\n    hidden_size=hidden_size,\n    num_heads=num_heads,\n    head_dim=head_dim,\n)\n</code></pre>"},{"location":"api/#microfuel.model.LinearAttentionBlock.__init__","title":"__init__","text":"<pre><code>__init__(hidden_size: int, num_heads: int, head_dim: int)\n</code></pre> Source code in <code>src/microfuel/model.py</code> <pre><code>def __init__(self, hidden_size: int, num_heads: int, head_dim: int):\n    super().__init__()\n    self.norm = ZeroCentredRMSNorm(hidden_size)\n    self.gdn = GatedDeltaNet(hidden_size=hidden_size, num_heads=num_heads, head_dim=head_dim)\n</code></pre>"},{"location":"api/#microfuel.model.LinearAttentionBlock.forward","title":"forward","text":"<pre><code>forward(\n    x: torch.Tensor, cu_seqlens: torch.Tensor\n) -&gt; torch.Tensor\n</code></pre> Source code in <code>src/microfuel/model.py</code> <pre><code>def forward(self, x: torch.Tensor, cu_seqlens: torch.Tensor) -&gt; torch.Tensor:\n    residual = x\n    x = self.norm(x)\n    x, _, _ = self.gdn(x.unsqueeze(0), cu_seqlens=cu_seqlens)\n    x = x.squeeze(0)\n    x = x + residual\n    return x\n</code></pre>"},{"location":"api/#microfuel.model.StaticHyperNet","title":"StaticHyperNet","text":"<p>               Bases: <code>nn.Module</code></p> <p>Creates a specialised feature extractor for each aircraft type, improving over feature conditioning (concatenating embeddings to input).</p> <p>See: https://arxiv.org/pdf/1609.09106#page=3 (Section 3.1).</p> Source code in <code>src/microfuel/model.py</code> <pre><code>class StaticHyperNet(nn.Module):\n    \"\"\"Creates a specialised feature extractor for each aircraft type, improving over\n    feature conditioning (concatenating embeddings to input).\n\n    See: https://arxiv.org/pdf/1609.09106#page=3 (Section 3.1).\"\"\"\n\n    def __init__(\n        self, num_aircraft_types: int, embedding_dim: int, input_dim: int, output_dim: int\n    ):\n        super().__init__()\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.embedding = nn.Embedding(num_aircraft_types, embedding_dim)\n        self.mlp = nn.Sequential(\n            nn.Linear(embedding_dim, 64),\n            nn.GELU(),\n            nn.Linear(64, (input_dim * output_dim) + output_dim),\n        )\n\n    def forward(\n        self, aircraft_type_idx: torch.Tensor\n    ) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        embeddings = self.embedding(aircraft_type_idx)\n        params = self.mlp(embeddings)\n        weights_flat = params[:, : self.input_dim * self.output_dim]\n        bias = params[:, self.input_dim * self.output_dim :]\n        weights = weights_flat.view(-1, self.output_dim, self.input_dim)\n        return weights, bias, embeddings\n</code></pre>"},{"location":"api/#microfuel.model.StaticHyperNet.input_dim","title":"input_dim  <code>instance-attribute</code>","text":"<pre><code>input_dim = input_dim\n</code></pre>"},{"location":"api/#microfuel.model.StaticHyperNet.output_dim","title":"output_dim  <code>instance-attribute</code>","text":"<pre><code>output_dim = output_dim\n</code></pre>"},{"location":"api/#microfuel.model.StaticHyperNet.embedding","title":"embedding  <code>instance-attribute</code>","text":"<pre><code>embedding = nn.Embedding(num_aircraft_types, embedding_dim)\n</code></pre>"},{"location":"api/#microfuel.model.StaticHyperNet.mlp","title":"mlp  <code>instance-attribute</code>","text":"<pre><code>mlp = nn.Sequential(\n    nn.Linear(embedding_dim, 64),\n    nn.GELU(),\n    nn.Linear(64, input_dim * output_dim + output_dim),\n)\n</code></pre>"},{"location":"api/#microfuel.model.StaticHyperNet.__init__","title":"__init__","text":"<pre><code>__init__(\n    num_aircraft_types: int,\n    embedding_dim: int,\n    input_dim: int,\n    output_dim: int,\n)\n</code></pre> Source code in <code>src/microfuel/model.py</code> <pre><code>def __init__(\n    self, num_aircraft_types: int, embedding_dim: int, input_dim: int, output_dim: int\n):\n    super().__init__()\n    self.input_dim = input_dim\n    self.output_dim = output_dim\n    self.embedding = nn.Embedding(num_aircraft_types, embedding_dim)\n    self.mlp = nn.Sequential(\n        nn.Linear(embedding_dim, 64),\n        nn.GELU(),\n        nn.Linear(64, (input_dim * output_dim) + output_dim),\n    )\n</code></pre>"},{"location":"api/#microfuel.model.StaticHyperNet.forward","title":"forward","text":"<pre><code>forward(\n    aircraft_type_idx: torch.Tensor,\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n</code></pre> Source code in <code>src/microfuel/model.py</code> <pre><code>def forward(\n    self, aircraft_type_idx: torch.Tensor\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    embeddings = self.embedding(aircraft_type_idx)\n    params = self.mlp(embeddings)\n    weights_flat = params[:, : self.input_dim * self.output_dim]\n    bias = params[:, self.input_dim * self.output_dim :]\n    weights = weights_flat.view(-1, self.output_dim, self.input_dim)\n    return weights, bias, embeddings\n</code></pre>"},{"location":"api/#microfuel.model.FuelBurnPredictorConfig","title":"FuelBurnPredictorConfig  <code>dataclass</code>","text":"Source code in <code>src/microfuel/model.py</code> <pre><code>@dataclass\nclass FuelBurnPredictorConfig:\n    input_dim: int\n    hidden_size: int\n    num_heads: int\n    num_aircraft_types: int\n    aircraft_embedding_dim: int\n    num_layers: int\n</code></pre>"},{"location":"api/#microfuel.model.FuelBurnPredictorConfig.input_dim","title":"input_dim  <code>instance-attribute</code>","text":"<pre><code>input_dim: int\n</code></pre>"},{"location":"api/#microfuel.model.FuelBurnPredictorConfig.hidden_size","title":"hidden_size  <code>instance-attribute</code>","text":"<pre><code>hidden_size: int\n</code></pre>"},{"location":"api/#microfuel.model.FuelBurnPredictorConfig.num_heads","title":"num_heads  <code>instance-attribute</code>","text":"<pre><code>num_heads: int\n</code></pre>"},{"location":"api/#microfuel.model.FuelBurnPredictorConfig.num_aircraft_types","title":"num_aircraft_types  <code>instance-attribute</code>","text":"<pre><code>num_aircraft_types: int\n</code></pre>"},{"location":"api/#microfuel.model.FuelBurnPredictorConfig.aircraft_embedding_dim","title":"aircraft_embedding_dim  <code>instance-attribute</code>","text":"<pre><code>aircraft_embedding_dim: int\n</code></pre>"},{"location":"api/#microfuel.model.FuelBurnPredictorConfig.num_layers","title":"num_layers  <code>instance-attribute</code>","text":"<pre><code>num_layers: int\n</code></pre>"},{"location":"api/#microfuel.model.FuelBurnPredictorConfig.__init__","title":"__init__","text":"<pre><code>__init__(\n    input_dim: int,\n    hidden_size: int,\n    num_heads: int,\n    num_aircraft_types: int,\n    aircraft_embedding_dim: int,\n    num_layers: int,\n) -&gt; None\n</code></pre>"},{"location":"api/#microfuel.model.TrainConfig","title":"TrainConfig  <code>dataclass</code>","text":"Source code in <code>src/microfuel/model.py</code> <pre><code>@dataclass\nclass TrainConfig:\n    partition: Partition\n    batch_size: int\n    epochs: int\n    lr: float\n    weight_decay: float\n    warmup_steps: int\n    seed: int\n    model_config: FuelBurnPredictorConfig\n</code></pre>"},{"location":"api/#microfuel.model.TrainConfig.partition","title":"partition  <code>instance-attribute</code>","text":"<pre><code>partition: Partition\n</code></pre>"},{"location":"api/#microfuel.model.TrainConfig.batch_size","title":"batch_size  <code>instance-attribute</code>","text":"<pre><code>batch_size: int\n</code></pre>"},{"location":"api/#microfuel.model.TrainConfig.epochs","title":"epochs  <code>instance-attribute</code>","text":"<pre><code>epochs: int\n</code></pre>"},{"location":"api/#microfuel.model.TrainConfig.lr","title":"lr  <code>instance-attribute</code>","text":"<pre><code>lr: float\n</code></pre>"},{"location":"api/#microfuel.model.TrainConfig.weight_decay","title":"weight_decay  <code>instance-attribute</code>","text":"<pre><code>weight_decay: float\n</code></pre>"},{"location":"api/#microfuel.model.TrainConfig.warmup_steps","title":"warmup_steps  <code>instance-attribute</code>","text":"<pre><code>warmup_steps: int\n</code></pre>"},{"location":"api/#microfuel.model.TrainConfig.seed","title":"seed  <code>instance-attribute</code>","text":"<pre><code>seed: int\n</code></pre>"},{"location":"api/#microfuel.model.TrainConfig.model_config","title":"model_config  <code>instance-attribute</code>","text":"<pre><code>model_config: FuelBurnPredictorConfig\n</code></pre>"},{"location":"api/#microfuel.model.TrainConfig.__init__","title":"__init__","text":"<pre><code>__init__(\n    partition: Partition,\n    batch_size: int,\n    epochs: int,\n    lr: float,\n    weight_decay: float,\n    warmup_steps: int,\n    seed: int,\n    model_config: FuelBurnPredictorConfig,\n) -&gt; None\n</code></pre>"},{"location":"api/#microfuel.model.TrainingState","title":"TrainingState  <code>dataclass</code>","text":"Source code in <code>src/microfuel/model.py</code> <pre><code>@dataclass\nclass TrainingState:\n    optimizer_state_dict: dict[str, Any]\n    scaler_state_dict: dict[str, Any]\n    global_step: int\n</code></pre>"},{"location":"api/#microfuel.model.TrainingState.optimizer_state_dict","title":"optimizer_state_dict  <code>instance-attribute</code>","text":"<pre><code>optimizer_state_dict: dict[str, Any]\n</code></pre>"},{"location":"api/#microfuel.model.TrainingState.scaler_state_dict","title":"scaler_state_dict  <code>instance-attribute</code>","text":"<pre><code>scaler_state_dict: dict[str, Any]\n</code></pre>"},{"location":"api/#microfuel.model.TrainingState.global_step","title":"global_step  <code>instance-attribute</code>","text":"<pre><code>global_step: int\n</code></pre>"},{"location":"api/#microfuel.model.TrainingState.__init__","title":"__init__","text":"<pre><code>__init__(\n    optimizer_state_dict: dict[str, Any],\n    scaler_state_dict: dict[str, Any],\n    global_step: int,\n) -&gt; None\n</code></pre>"},{"location":"api/#microfuel.model.FuelBurnPredictor","title":"FuelBurnPredictor","text":"<p>               Bases: <code>nn.Module</code></p> <p>Gated Delta Network for fuel burn estimation.</p> <p>It processes data at two resolutions to solve the mass identifiability problem:</p> <ol> <li>Processes the high-fidelity kinematics \\(x_{t:t+\\Delta}\\) for the specific query interval.</li> <li>Processes the entire trajectory \\(x_{0:T}\\) (takeoff to landing).</li> </ol> <p>Hypothesis:     The <code>pooled_flight</code> vector acts as a compressed context containing     implicit estimates of the aircraft's takeoff mass and degradation factors,     which are globally observable over the full flight but locally unobservable.</p> <p>NOTE: Instead of padding, sequences are tightly packed together in a long tensor, and FLA is informed of boundaries via the <code>cu_seqlens</code> tensor.</p> Source code in <code>src/microfuel/model.py</code> <pre><code>class FuelBurnPredictor(nn.Module):\n    r\"\"\"Gated Delta Network for fuel burn estimation.\n\n    It processes data at two resolutions to solve the mass identifiability problem:\n\n    1. Processes the high-fidelity kinematics $x_{t:t+\\Delta}$ for the specific query interval.\n    2. Processes the entire trajectory $x_{0:T}$ (takeoff to landing).\n\n    Hypothesis:\n        The `pooled_flight` vector acts as a compressed context containing\n        implicit estimates of the aircraft's takeoff mass and degradation factors,\n        which are globally observable over the full flight but locally unobservable.\n\n    NOTE: Instead of padding, sequences are tightly packed together in a long tensor, and\n    FLA is informed of boundaries via the `cu_seqlens` tensor.\n    \"\"\"\n\n    def __init__(self, cfg: FuelBurnPredictorConfig):\n        super().__init__()\n        self.config = cfg\n        key_dim = int(cfg.hidden_size * 0.75)  # as per docstring\n        assert key_dim % cfg.num_heads == 0, (\n            \"int(hidden_size * 0.75) must be divisible by num_heads (use_gate=True)\"\n        )\n        head_dim = key_dim // cfg.num_heads\n\n        # segment processing branch\n        self.hypernetwork_segment = StaticHyperNet(\n            num_aircraft_types=cfg.num_aircraft_types,\n            embedding_dim=cfg.aircraft_embedding_dim,\n            input_dim=cfg.input_dim,\n            output_dim=cfg.hidden_size,\n        )\n        self.layers_segment = nn.ModuleList(\n            [\n                LinearAttentionBlock(cfg.hidden_size, cfg.num_heads, head_dim)\n                for _ in range(cfg.num_layers)\n            ]\n        )\n        self.pooler_segment = Pooler()\n\n        # flight context processing branch\n        self.hypernetwork_flight = StaticHyperNet(\n            num_aircraft_types=cfg.num_aircraft_types,\n            embedding_dim=cfg.aircraft_embedding_dim,\n            input_dim=cfg.input_dim,\n            output_dim=cfg.hidden_size,\n        )\n        self.layers_flight = nn.ModuleList(\n            [\n                LinearAttentionBlock(cfg.hidden_size, cfg.num_heads, head_dim)\n                for _ in range(cfg.num_layers)\n            ]\n        )\n        self.pooler_flight = Pooler()\n\n        # share embedding layer between hypernetworks\n        self.hypernetwork_flight.embedding = self.hypernetwork_segment.embedding\n\n        self.regression_head = nn.Linear(\n            cfg.hidden_size + cfg.hidden_size + cfg.aircraft_embedding_dim, 1\n        )\n\n    def forward(\n        self,\n        x_flight: torch.Tensor,\n        cu_seqlens_flight: torch.Tensor,\n        x_segment: torch.Tensor,\n        cu_seqlens_segment: torch.Tensor,\n        aircraft_type_idx: torch.Tensor,\n    ) -&gt; torch.Tensor:\n        \"\"\":param x_flight: packed tensor of full flight trajectories\n        :param cu_seqlens_flight: cumulative sequence lengths for flight tensor\n        :param x_segment: packed tensor of trajectory segments for prediction\n        :param cu_seqlens_segment: cumulative sequence lengths for segment tensor\n        :param aircraft_type_idx: (B,) tensor of aircraft type indices\"\"\"\n        # segment processing\n        segment_lengths = cu_seqlens_segment[1:] - cu_seqlens_segment[:-1]\n        weights_s, bias_s, ac_embeddings = self.hypernetwork_segment(aircraft_type_idx)\n        weights_expanded_s = torch.repeat_interleave(weights_s, segment_lengths, dim=0)\n        bias_expanded_s = torch.repeat_interleave(bias_s, segment_lengths, dim=0)\n        x_s = torch.bmm(weights_expanded_s, x_segment.unsqueeze(-1)).squeeze(-1) + bias_expanded_s\n\n        for layer in self.layers_segment:\n            x_s = layer(x_s, cu_seqlens_segment)\n        pooled_segment = self.pooler_segment(x_s, cu_seqlens_segment)\n\n        # flight context processing\n        flight_lengths = cu_seqlens_flight[1:] - cu_seqlens_flight[:-1]\n        weights_f, bias_f, _ = self.hypernetwork_flight(aircraft_type_idx)\n        weights_expanded_f = torch.repeat_interleave(weights_f, flight_lengths, dim=0)\n        bias_expanded_f = torch.repeat_interleave(bias_f, flight_lengths, dim=0)\n        x_f = torch.bmm(weights_expanded_f, x_flight.unsqueeze(-1)).squeeze(-1) + bias_expanded_f\n\n        for layer in self.layers_flight:\n            x_f = layer(x_f, cu_seqlens_flight)\n        pooled_flight = self.pooler_flight(x_f, cu_seqlens_flight)\n\n        # final regression\n        combined_features = torch.cat([pooled_segment, pooled_flight, ac_embeddings], dim=1)\n        y_pred = self.regression_head(combined_features)\n        return y_pred\n</code></pre>"},{"location":"api/#microfuel.model.FuelBurnPredictor.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = cfg\n</code></pre>"},{"location":"api/#microfuel.model.FuelBurnPredictor.hypernetwork_segment","title":"hypernetwork_segment  <code>instance-attribute</code>","text":"<pre><code>hypernetwork_segment = StaticHyperNet(\n    num_aircraft_types=cfg.num_aircraft_types,\n    embedding_dim=cfg.aircraft_embedding_dim,\n    input_dim=cfg.input_dim,\n    output_dim=cfg.hidden_size,\n)\n</code></pre>"},{"location":"api/#microfuel.model.FuelBurnPredictor.layers_segment","title":"layers_segment  <code>instance-attribute</code>","text":"<pre><code>layers_segment = nn.ModuleList(\n    [\n        (\n            LinearAttentionBlock(\n                cfg.hidden_size, cfg.num_heads, head_dim\n            )\n        )\n        for _ in (range(cfg.num_layers))\n    ]\n)\n</code></pre>"},{"location":"api/#microfuel.model.FuelBurnPredictor.pooler_segment","title":"pooler_segment  <code>instance-attribute</code>","text":"<pre><code>pooler_segment = Pooler()\n</code></pre>"},{"location":"api/#microfuel.model.FuelBurnPredictor.hypernetwork_flight","title":"hypernetwork_flight  <code>instance-attribute</code>","text":"<pre><code>hypernetwork_flight = StaticHyperNet(\n    num_aircraft_types=cfg.num_aircraft_types,\n    embedding_dim=cfg.aircraft_embedding_dim,\n    input_dim=cfg.input_dim,\n    output_dim=cfg.hidden_size,\n)\n</code></pre>"},{"location":"api/#microfuel.model.FuelBurnPredictor.layers_flight","title":"layers_flight  <code>instance-attribute</code>","text":"<pre><code>layers_flight = nn.ModuleList(\n    [\n        (\n            LinearAttentionBlock(\n                cfg.hidden_size, cfg.num_heads, head_dim\n            )\n        )\n        for _ in (range(cfg.num_layers))\n    ]\n)\n</code></pre>"},{"location":"api/#microfuel.model.FuelBurnPredictor.pooler_flight","title":"pooler_flight  <code>instance-attribute</code>","text":"<pre><code>pooler_flight = Pooler()\n</code></pre>"},{"location":"api/#microfuel.model.FuelBurnPredictor.regression_head","title":"regression_head  <code>instance-attribute</code>","text":"<pre><code>regression_head = nn.Linear(\n    cfg.hidden_size\n    + cfg.hidden_size\n    + cfg.aircraft_embedding_dim,\n    1,\n)\n</code></pre>"},{"location":"api/#microfuel.model.FuelBurnPredictor.__init__","title":"__init__","text":"<pre><code>__init__(cfg: FuelBurnPredictorConfig)\n</code></pre> Source code in <code>src/microfuel/model.py</code> <pre><code>def __init__(self, cfg: FuelBurnPredictorConfig):\n    super().__init__()\n    self.config = cfg\n    key_dim = int(cfg.hidden_size * 0.75)  # as per docstring\n    assert key_dim % cfg.num_heads == 0, (\n        \"int(hidden_size * 0.75) must be divisible by num_heads (use_gate=True)\"\n    )\n    head_dim = key_dim // cfg.num_heads\n\n    # segment processing branch\n    self.hypernetwork_segment = StaticHyperNet(\n        num_aircraft_types=cfg.num_aircraft_types,\n        embedding_dim=cfg.aircraft_embedding_dim,\n        input_dim=cfg.input_dim,\n        output_dim=cfg.hidden_size,\n    )\n    self.layers_segment = nn.ModuleList(\n        [\n            LinearAttentionBlock(cfg.hidden_size, cfg.num_heads, head_dim)\n            for _ in range(cfg.num_layers)\n        ]\n    )\n    self.pooler_segment = Pooler()\n\n    # flight context processing branch\n    self.hypernetwork_flight = StaticHyperNet(\n        num_aircraft_types=cfg.num_aircraft_types,\n        embedding_dim=cfg.aircraft_embedding_dim,\n        input_dim=cfg.input_dim,\n        output_dim=cfg.hidden_size,\n    )\n    self.layers_flight = nn.ModuleList(\n        [\n            LinearAttentionBlock(cfg.hidden_size, cfg.num_heads, head_dim)\n            for _ in range(cfg.num_layers)\n        ]\n    )\n    self.pooler_flight = Pooler()\n\n    # share embedding layer between hypernetworks\n    self.hypernetwork_flight.embedding = self.hypernetwork_segment.embedding\n\n    self.regression_head = nn.Linear(\n        cfg.hidden_size + cfg.hidden_size + cfg.aircraft_embedding_dim, 1\n    )\n</code></pre>"},{"location":"api/#microfuel.model.FuelBurnPredictor.forward","title":"forward","text":"<pre><code>forward(\n    x_flight: torch.Tensor,\n    cu_seqlens_flight: torch.Tensor,\n    x_segment: torch.Tensor,\n    cu_seqlens_segment: torch.Tensor,\n    aircraft_type_idx: torch.Tensor,\n) -&gt; torch.Tensor\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>x_flight</code> <code>torch.Tensor</code> <p>packed tensor of full flight trajectories</p> required <code>cu_seqlens_flight</code> <code>torch.Tensor</code> <p>cumulative sequence lengths for flight tensor</p> required <code>x_segment</code> <code>torch.Tensor</code> <p>packed tensor of trajectory segments for prediction</p> required <code>cu_seqlens_segment</code> <code>torch.Tensor</code> <p>cumulative sequence lengths for segment tensor</p> required <code>aircraft_type_idx</code> <code>torch.Tensor</code> <p>(B,) tensor of aircraft type indices</p> required Source code in <code>src/microfuel/model.py</code> <pre><code>def forward(\n    self,\n    x_flight: torch.Tensor,\n    cu_seqlens_flight: torch.Tensor,\n    x_segment: torch.Tensor,\n    cu_seqlens_segment: torch.Tensor,\n    aircraft_type_idx: torch.Tensor,\n) -&gt; torch.Tensor:\n    \"\"\":param x_flight: packed tensor of full flight trajectories\n    :param cu_seqlens_flight: cumulative sequence lengths for flight tensor\n    :param x_segment: packed tensor of trajectory segments for prediction\n    :param cu_seqlens_segment: cumulative sequence lengths for segment tensor\n    :param aircraft_type_idx: (B,) tensor of aircraft type indices\"\"\"\n    # segment processing\n    segment_lengths = cu_seqlens_segment[1:] - cu_seqlens_segment[:-1]\n    weights_s, bias_s, ac_embeddings = self.hypernetwork_segment(aircraft_type_idx)\n    weights_expanded_s = torch.repeat_interleave(weights_s, segment_lengths, dim=0)\n    bias_expanded_s = torch.repeat_interleave(bias_s, segment_lengths, dim=0)\n    x_s = torch.bmm(weights_expanded_s, x_segment.unsqueeze(-1)).squeeze(-1) + bias_expanded_s\n\n    for layer in self.layers_segment:\n        x_s = layer(x_s, cu_seqlens_segment)\n    pooled_segment = self.pooler_segment(x_s, cu_seqlens_segment)\n\n    # flight context processing\n    flight_lengths = cu_seqlens_flight[1:] - cu_seqlens_flight[:-1]\n    weights_f, bias_f, _ = self.hypernetwork_flight(aircraft_type_idx)\n    weights_expanded_f = torch.repeat_interleave(weights_f, flight_lengths, dim=0)\n    bias_expanded_f = torch.repeat_interleave(bias_f, flight_lengths, dim=0)\n    x_f = torch.bmm(weights_expanded_f, x_flight.unsqueeze(-1)).squeeze(-1) + bias_expanded_f\n\n    for layer in self.layers_flight:\n        x_f = layer(x_f, cu_seqlens_flight)\n    pooled_flight = self.pooler_flight(x_f, cu_seqlens_flight)\n\n    # final regression\n    combined_features = torch.cat([pooled_segment, pooled_flight, ac_embeddings], dim=1)\n    y_pred = self.regression_head(combined_features)\n    return y_pred\n</code></pre>"},{"location":"api/#microfuel.plot","title":"plot","text":""},{"location":"api/#microfuel.plot.default_fig","title":"default_fig","text":"<pre><code>default_fig(*args, **kwargs) -&gt; Figure\n</code></pre> Source code in <code>src/microfuel/plot.py</code> <pre><code>def default_fig(*args, **kwargs) -&gt; Figure:\n    _init_style(dark=False)\n    if \"figsize\" not in kwargs:\n        kwargs[\"figsize\"] = (12, 7)\n    fig = plt.figure(*args, **kwargs)\n    fig.set_layout_engine(\"tight\")\n    return fig\n</code></pre>"},{"location":"comparison/","title":"Comparison","text":"<p>This section compares the architecture and feature engineering techniques against several SOTA models in the PRC Data Challenge 2025.</p>"},{"location":"comparison/#unique-guitar-this-repository","title":"unique-guitar (this repository)","text":"<p>Phase 2 Test RMSE: 222.54 kg (rank: 7)</p> <p>Source Code: https://github.com/abc8747/microfuel</p> <p>Architecture: gated delta network</p> <p>Extra Data/Libraries: --</p> <p>Inputs:</p> <ul> <li>Kinematic sequence: <code>altitude</code>, <code>groundspeed</code>, <code>vertical_rate</code> (smoothed via Kalman filter).</li> <li>Context: <code>flight_progress</code> (normalized time), <code>flight_duration</code>, <code>aircraft_type</code> (embedding index).</li> </ul> <p>Methodology:</p> <ul> <li>Signal processing: Kalman filter + RTS smoother for trajectory denoising (constant velocity assumption).</li> <li>Architecture: GDN (linear attention variant) with dual-stream processing:<ol> <li>Segment-level: Local kinematics.</li> <li>Flight-level: Full trajectory context (implicit mass identifiability).</li> </ol> </li> <li>Adaptation: Static Hypernetwork generates weights based on <code>aircraft_type</code> embedding.</li> <li>Loss function: Class-Balanced (CB) loss to upweight rare aircraft types; optimisation on RMSE of fuel rate, converted to total kg.</li> </ul>"},{"location":"comparison/#resourceful-quiver","title":"resourceful-quiver","text":"<p>Phase 2 Test RMSE: 199.91 kg (rank: 1)</p> <p>Source Code: https://github.com/meldorr/PRC-2025</p> <p>Architecture: LightGBM Regressor (two-stage: mass estimation -&gt; fuel prediction)</p> <p>Extra Data/Libraries: ERA5 weather (u/v wind, temp, humidity), <code>ac_tows.csv</code> (aircraft weights), <code>openap</code> (library).</p> <p>Inputs:</p> <ul> <li>Flight Context: <code>seg_duration</code>, <code>seg_dist</code>, <code>flight_duration</code>, <code>full_flight_dist</code>, <code>aircraft_type</code>, <code>phase</code>, <code>m_tow</code>, <code>oew</code>.</li> <li>Trajectory Aggregates: <code>mean</code>/<code>std</code> for <code>groundspeed</code>, <code>track</code>, <code>vertical_rate</code>, <code>mach</code>, <code>TAS</code>, <code>CAS</code>; <code>altitude_mean</code>, <code>vertical_rate_min</code>, <code>vertical_rate_max</code>.</li> <li>Mass Features: <code>tow_est_kg</code> (predicted via separate LGBM using climb profile/vertical acceleration), <code>mass_est_tf_mean</code>, <code>mass_est_tf_std</code> (time-flown based mass decay).</li> <li>Derived Physics: <code>ff_kgs_est_mass_tf_mean</code>, <code>ff_kgs_est_mass_tf_std</code> (OpenAP fuel flow estimate using decaying mass).</li> <li>Binned Statistics: <code>vertical_rate_mean_{0..9}</code>, <code>vertical_rate_std_{0..9}</code>.</li> </ul> <p>Methodology:</p> <ul> <li>Preprocessing: LCC projection resampling (1s), Savitzky-Golay filtering, custom phase detection, altitude gap filling.</li> <li>Aerodynamics: TAS calculated from Mach/CAS and ERA5 wind components; mass estimated via climb performance.</li> <li>Modelling: LightGBM on ~48 features, 5-fold CV, target <code>ff_kgs</code> (fuel flow kg/s) converted to total fuel.</li> </ul>"},{"location":"comparison/#bright-lobster","title":"bright-lobster","text":"<p>Phase 2 Test RMSE: 213.24 kg (rank: 2)</p> <p>Source Code: https://github.com/eeftychiou/PRCXGBoost</p> <p>Architecture: XGBoost Regressor ensemble (top 10 Models)</p> <p>Extra Data/Libraries: SkyVector (runway heading/length/elevation), IATA market reports (load factors), METAR weather, <code>openap</code> (library), <code>pygeomag</code> (library).</p> <p>Inputs:</p> <ul> <li>Airport/runway: <code>origin_</code> / <code>destination_</code> <code>longitude</code>, <code>latitude</code>, <code>elevation</code>, <code>RWY_{1..8}_{HEADING/LENGTH/ELEVATION}</code>.</li> <li>Aircraft Meta: <code>mfc</code>, <code>pax_high</code>, <code>fuselage_height</code>, <code>wing_mac</code>, <code>wing_t/c</code>, <code>flaps_{type/area/bf_b/Sf_S}</code>, <code>cruise_mach</code>, <code>engine_default</code>, <code>drag_{cd0/e/gears}</code>, <code>fuel_fuel_coef</code>, <code>limits_OEW</code>.</li> <li>Weather (METAR): <code>dep_</code> / <code>arr_</code> <code>tmpf</code>, <code>sknt</code>, <code>vsby</code>, <code>wx_intensity</code>; boolean flags for <code>thunderstorm</code>, <code>freezing</code>, <code>shower</code>, <code>rain</code>, <code>snow</code>, <code>fog_mist</code>, <code>haze_smoke</code>.</li> <li>Flight Context: <code>great_circle_distance_km</code>, <code>flight_duration_seconds</code>, <code>average_load_factor</code> (IATA), <code>estimated_payload_kg</code>, <code>estimated_takeoff_mass</code>, <code>estimated_total_fuel_kg</code>, <code>trip_fuel_kg</code>, <code>contingency_fuel_kg</code>, <code>final_reserve_fuel_kg</code>.</li> <li>Time: <code>seg_start_day_of_week</code>, <code>seg_{start/end}_time_decimal</code>, <code>flight_{start/end}_day_of_week</code>, <code>flight_{start/end}_time_decimal</code>, <code>seg_{end/start}_to_{landing/takeoff}</code>.</li> <li>Trajectory Aggregates: <code>min</code>/<code>max</code>/<code>mean</code>/<code>std</code>/<code>delta</code> for <code>latitude</code>, <code>longitude</code>, <code>altitude</code>, <code>groundspeed</code>, <code>track</code>, <code>vertical_rate</code>, <code>mach</code>, <code>TAS</code>, <code>CAS</code>, <code>calculated_speed</code>, <code>vertical_rate_change</code>, <code>dist_to_origin_km</code>, <code>dist_to_dest_km</code>; <code>start_alt_rev</code>, <code>end_alt_rev</code>, <code>alt_diff_rev</code>, <code>alt_diff_rev_std</code>, <code>mean_time_in_air</code>.</li> <li>Phases: <code>phase_fraction_{climb/cruise/descent/approach/gnd/level/na}</code>, <code>ee_phase_duration_{parked/taxi_out/takeoff/climb/cruise/descent/approach/landing/taxi_in}</code>.</li> <li>Derived Physics: <code>fuel_consumption_{gnd/cl/de/lvl/cr/na}</code>, <code>fuel_consumption</code> (sum), <code>seg_avg_burn_rate</code>.</li> <li>Interactions: <code>duration_x_{mass/altitude}</code>, <code>distance_x_mass</code>, <code>alt_x_mass</code>, <code>speed_x_mass</code>; polynomials (<code>segment_duration_{sq/cub}</code>, <code>phase_duration_cl_{sq/cub}</code>, <code>alt_diff_rev_sq</code>).</li> </ul> <p>Methodology:</p> <ul> <li>Data Enrichment: Web scraping SkyVector, mapping IATA load factors to routes.</li> <li>Augmentation: Generation of 25k synthetic widebody samples via Gaussian noise injection on long segments.</li> <li>Feature Selection: Sequential Feature Selection (SFS) with XGBoost base.</li> <li>Training: RandomizedSearchCV, ensemble of top 10 validation Models trained on 100% data (train+synthetic).</li> </ul>"},{"location":"comparison/#sincere-glacier","title":"sincere-glacier","text":"<p>Phase 2 Test RMSE: 214.36 kg (rank: 3)</p> <p>Source Code: https://github.com/johntad110/sincere-glacier-prc2025</p> <p>Architecture: Hybrid Stacking Ensemble (LightGBM + LSTM -&gt; Ridge Regression)</p> <p>Extra Data/Libraries: --</p> <p>Inputs:</p> <ul> <li>GBM features (aggregated): <code>duration</code>, <code>n_points</code>, <code>total_dist</code>, <code>time_since_takeoff</code>, <code>time_to_landing</code>, <code>relative_time</code>, <code>od_distance</code>; <code>origin_</code> / <code>dest_</code> <code>lat</code>, <code>lon</code>, <code>elev</code>; <code>aircraft_type</code>.</li> <li>Statistics (<code>avg</code>/<code>std</code> and <code>min</code>/<code>max</code> where applicable): <code>avg_alt</code>, <code>avg_speed</code>, <code>avg_vertical_rate</code>, <code>avg_acc</code>, <code>avg_energy_rate</code>.</li> <li>Aerodynamics: <code>avg_mach</code>, <code>avg_dynamic_pressure</code>, <code>avg_air_density</code>, <code>avg_parasitic_power</code>, <code>avg_induced_power</code>, <code>avg_climb_power</code>.</li> <li>Physics: <code>mass_proxy</code> (estimated from climb Newtonian dynamics).</li> <li>LSTM features (sequential): <code>altitude</code>, <code>groundspeed</code>, <code>vertical_rate</code>, <code>sin_track</code>, <code>cos_track</code>, <code>dist_step</code>, <code>acceleration</code>, <code>energy_rate</code> (sequence length 32).</li> <li>Static context: <code>aircraft_type</code> (embedding), <code>duration</code>, <code>total_dist</code>.</li> </ul> <p>Methodology:</p> <ul> <li>Preprocessing: Z-score normalisation, ISA standard atmosphere Modelling, outlier filtering based on fuel flow limits.</li> <li>Mass Estimation: Physics-based proxy derived using thrust-drag equation during climb.</li> <li>Stacking: Non-negative Ridge Regression combining Out-of-Fold (OOF) predictions from physics-aware GBM and sequence-aware LSTM.</li> </ul>"},{"location":"comparison/#wise-watermelon","title":"wise-watermelon","text":"<p>Phase 2 Test RMSE: 215.68 kg (rank: 4)</p> <p>Source Code: https://github.com/isaacOluwafemiOg/prc2025_wisewatermelon</p> <p>Architecture: CatBoost Regressor Ensemble</p> <p>Extra Data/Libraries: FAA aircraft characteristics, World Airports CSV, <code>openap</code> (library), <code>acropole</code> (library), <code>traffic</code> (library).</p> <p>Inputs:</p> <ul> <li>Scalars: <code>aircraft_type</code>, <code>engine_Model</code>, <code>wake_category</code>; <code>flight_fuel</code> (total estimated), <code>fl_max_alti</code>, <code>real_flight_dur</code>, <code>missing_segment</code>.</li> <li>Segment aggregates (<code>sum</code>, <code>mean</code>, <code>min</code>, <code>max</code>, <code>start</code>, <code>end</code>, <code>nancount</code>): <code>altitude</code>, <code>groundspeed</code>, <code>mach</code>, <code>CAS</code>, <code>fuel_flow</code> (OpenAP), <code>fuel</code> (OpenAP), <code>drag</code> (OpenAP), <code>thrust</code> (OpenAP), <code>cl_fuel</code>, <code>enr_fuel</code>, <code>dist_from_ades</code>, <code>acp_fuel</code> (Acropole), <code>acp_fuelflow</code> (Acropole).</li> <li>Derived: <code>total_climb_height</code>, <code>unscaled_approx_seg_fuel</code>, <code>resample_quality_score</code>.</li> <li>Phases: <code>seg_{phase}_dur</code> (for GND, CL, DE, LVL, CR, NA), <code>flight_{phase}_ct</code>, <code>all_seg_phase_dur</code>.</li> </ul> <p>Methodology:</p> <ul> <li>Preprocessing: Trajectory merging (handling duplicates), synthetic row injection for coverage, linear interpolation/resampling to 60s.</li> <li>Physics: Vectorised Haversine, ATM phase labelling, OpenAP/Acropole estimation per timestamp for fuel/drag/thrust.</li> <li>Validation: Stratified Group K-Fold (k=5) grouped by <code>flight_id</code>.</li> <li>Modelling: CatBoost with Optuna tuning and physics-constrained clipping (non-negative).</li> </ul>"},{"location":"data/","title":"Data","text":"<p>Summary statistics:</p> Dataset Partition Trajectory Rows Fuel Segments Flights File Size Phase 1 (Train) 124,094,050 133,984 11,088 3.2 GB Phase 1 (Rank) 24,499,924 24,972 1,929 616 MB Phase 2 (Rank) 37,877,494 61,745 2,839 943 MB"},{"location":"data/#first-party-data","title":"First party data","text":"<p>Identifier Uniqueness</p> <p>The segment identifier <code>idx</code> in the fuel files is not globally unique. It resets between competition phases/months.</p> <p>For example, <code>idx=0</code> exists in both Phase 1 (September) and Phase 2 (October). When performing joins or evaluations spanning multiple partitions, you should use the composite key <code>(idx, flight_id)</code> to avoid data misalignment.</p> <p>Phase 2 Data</p> <p>In Phase 2, fuel data <code>fuel_phase2_rank.parquet</code> contains both September and October data. But flight list data <code>flight_list_phase2_rank</code> and trajectory data <code>flights_phase2_rank/prc*.parquet</code> contains only October data.</p> <p>Take caution when joining data.</p> <p>Schema:</p> <ul> <li>Fuel</li> <li>Flight List</li> <li>Trajectories</li> <li>Airports</li> </ul> <p>The distribution of aircraft type, segment lengths are heavily tailed.</p> <p></p> <p>A visualisation of the fuel burn in a simple altitude/speed plot.</p> <p></p> <p>A visualisation of the preprocessed trajectory features. Notice that state vectors are irregularly sampled, often with significant time gaps.</p> <p></p>"},{"location":"data/#weather-data","title":"Weather Data","text":"<p>Note</p> <p>Weather data is unused in <code>v0.1</code> versions of the models. Future versions of the model (<code>v0.2</code> onwards) will allow optionally specifying the wind component for more accurate predictions.</p> <p>We augment the trajectory data with <code>u</code> and <code>v</code> wind components extracted from the ARCO ERA5 dataset. This requires installing <code>microfuel</code> with the <code>era5</code> optional depedency.</p> <ol> <li> <p>The weather data is massive (~565 GB). It is recommended to use an extenral HDD and symlink it to <code>data/raw/weather</code>:</p> <pre><code>mkdir -p /mnt/hdd/microfuel_era5\nln -s /mnt/hdd/microfuel_era5 data/raw/era5\n</code></pre> </li> <li> <p>Install the <code>gcloud</code> CLI and run the following to pull specific pressure level slices in NetCDF format.</p> <pre><code>uv run scripts/main.py download-era5\n</code></pre> <ul> <li>Months: <code>2025-04</code>..=<code>2025-10</code></li> <li>Variables: <code>u_component_of_wind</code>, <code>v_component_of_wind</code></li> <li>Levels: 28 levels (1000..=70 hPa)</li> </ul> </li> <li> <p>We interpolate the 4D weather grid (<code>time</code>, <code>level</code>, <code>lat</code>, <code>lon</code>) onto the 4D flight trajectory coordinates.</p> <pre><code>uv run scripts/main.py create-era5 --partition phase1\n</code></pre> </li> </ol>"},{"location":"problem/","title":"Problem Formulation","text":"<p>The objective is to estimate the total mass of fuel consumed, \\(m_f\\), over a specific flight segment defined by the time interval \\([t_{\\mathrm{start}}, t_{\\mathrm{end}}]\\).</p> <p>The fuel mass flow rate \\(\\dot{m}_f\\) is a function of the thrust required to maintain the aircraft's kinematic state. The simplified governing equation for longitudinal motion of a point-mass aircraft is: $$ m \\dot{V} = T - D - mg \\sin \\gamma, $$ where:</p> <ul> <li>\\(m\\): Aircraft mass (time-varying: \\(m(t) = m_0 - \\int_0^t \\dot{m}_f(\\tau) d\\tau\\)). Note that the initial takeoff mass is unknown.</li> <li>\\(\\dot{V}\\): Rate of change of the true airspeed along the flight path.</li> <li>\\(T\\): Thrust.</li> <li>\\(D\\): Drag (\\(D = \\frac{1}{2}\\rho V^2 S C_D\\)).</li> <li>\\(g\\): Gravitational acceleration.</li> <li>\\(\\gamma\\): Flight path angle.</li> </ul> <p>The instantaneous fuel flow is related to thrust via the thrust specific fuel consumption \\(c_T\\): \\(\\dot{m}_f = c_T T\\), which varies with the aircraft's altitude and mach number (Bartel and Young, 2008)</p>"},{"location":"problem/#identifiability","title":"Identifiability","text":"<p>A challenge in using public surveillance data (ADS-B) is the non-identifiability of the kinetic state.</p> \\[ T - D = m(\\dot{V} + g \\sin \\gamma) \\] <p>The RHS contains kinematic terms (\\(\\dot{V}, \\gamma\\)) which are observable from surveillance data (see <code>microfuel.datasets.raw.TrajectoryRecord</code>), and mass \\(m\\), which is unknown but bounded. However, the LHS represents the Excess Thrust. Without an aerodynamic model (e.g. <code>openap</code> or BADA) to determine \\(D\\), it is not possible to isolate \\(T\\) from \\(T - D\\) given only kinematic observations.</p> <p>Since we cannot explicitly solve for \\(T\\) to compute \\(\\dot{m}_f\\), we learn a mapping function \\(\\mathcal{F}_\\theta\\) parameterised by a neural network that maps a sequence of observable kinematic states to the cumulative fuel burn:</p> \\[ \\int_{t_\\mathrm{start}}^{t_\\mathrm{end}} \\dot{m}_f dt \\approx \\mathcal{F}_\\theta(X_{t_\\mathrm{start}:t_\\mathrm{end}} | \\text{type}_\\text{ac}) \\]"},{"location":"problem/#model-architecture","title":"Model Architecture","text":"<p>We utilise a Gated Delta Network (GDN) (Yang et al., 2024), a simplified state-space model (SSM) that offers the inference speed of RNNs with the expressive capacity of Transformers.</p> <p>GDN introduces an input-dependent state transition while maintaining \\(O(L)\\) linear complexity. The update rule for the memory state \\(S_t \\in \\mathbb{R}^{d_k \\times d_v}\\) is:</p> \\[ S_t = S_{t-1} \\underbrace{(\\alpha_t I - \\beta_t k_t k_t^\\top)}_{\\text{Decay \\&amp; Erase}} + \\underbrace{\\beta_t v_t k_t^\\top}_{\\text{Write}} \\]"},{"location":"problem/#control-theoretic-interpretation","title":"Control-theoretic interpretation","text":"<p>The GDN update rule can be rewritten to reveal its operation as an online gradient descent optimiser solving a least squares problem: $$ S_t = \\alpha_t S_{t-1} + \\beta_t \\underbrace{(v_t - S_{t-1} k_t)}_{\\text{Innovation / Error}} k_t^\\top $$</p> <ol> <li>The model queries the current memory state \\(S_{t-1}\\) with key \\(k_t\\) to predict a value \\(\\hat{v}_t = S_{t-1}k_t\\).</li> <li>It calculates the innovation, or the difference between the actual information \\(v_t\\) and the retrieved information.</li> <li>It updates the memory matrix \\(S\\) by moving it in the direction of the error, weighted by the step size \\(\\beta_t\\).</li> </ol> <p>This effectively makes the GDN a \"Fast Weight Programmer\" (Schlag et al. (2021)). The memory \\(S_t\\) is a dynamic linear model that is continuously re-optimised (trained) at every time step to minimise the reconstruction error of the incoming flight data stream.</p>"},{"location":"problem/#inference-modes-realtime-vs-offline","title":"Inference Modes: Realtime vs. Offline","text":"<p>We address the unknown mass \\(m(t)\\) by leveraging different temporal contexts depending on the inference scenario.</p>"},{"location":"problem/#realtime","title":"Realtime","text":"<p>In the realtime model, prediction depends strictly on causal information \\(P(y_t | x_{0:t})\\). The model must infer the current fuel burn rate based solely on the kinematic history up to the current moment.</p>"},{"location":"problem/#offline","title":"Offline","text":"<p>For post-flight analysis, we employ an offline model that utilizes the entire flight trajectory \\(x_{0:T}\\) to predict the fuel burn of a specific segment \\(y_{t:t+\\Delta}\\).</p> <p>The model processes two streams via <code>microfuel.model.FuelBurnPredictor</code>:</p> <ol> <li>Segment Stream: High-resolution kinematics of the query interval.</li> <li>Flight Context Stream: The full trajectory from takeoff to landing.</li> </ol> <p>We hypothesise that the flight context stream allows the model to implicitly learn a latent representation of the aircraft's mass and drag constraints. For example, the rate of climb at takeoff reveals the initial weight, while the total energy dissipation during descent constrains the drag polar. This global context corrects the local fuel burn estimates.</p>"},{"location":"problem/#hypernetworks-for-physics-adaptation","title":"Hypernetworks for Physics Adaptation","text":"<p>Aircraft performance characteristics vary drastically across different aircraft types (e.g. A380 vs A320). A standard embedding approach is insufficient because it only learns to shift the bias of the features.</p> <p>We employ a Static Hypernetwork (<code>microfuel.model.StaticHyperNet</code>, Ha et al., 2016) to address this. Instead of sharing weights across all aircraft, the model learns a manifold of parameters. Let \\(e_{ac} \\in \\mathbb{R}^{d_{emb}}\\) be the embedding for a specific aircraft type. A small MLP \\(\\mathcal{H}\\) generates the weights for the input projection layers of the main network:</p> \\[ \\begin{align*} \\theta_\\text{proj}^{\\text(ac)} &amp;= \\mathcal{H}(e_\\text{ac}) \\\\ h_t &amp;= W_\\text{proj}^{\\text(ac)} x_t + b_\\text{proj}^{\\text{(ac)}} \\end{align*} \\]"},{"location":"problem/#class-imbalance","title":"Class Imbalance","text":"<p>The aircraft type distribution follows a power law, causing the model to be biased towards common aircraft types (e.g., A320, A20N) while performing poorly on rare types (e.g., MD11, A318).</p> <p>We introduce an optional Class-Balanced (CB) loss (Cui et al., 2019), weighing the loss based on the \"effective number of samples\", which posits that the marginal benefit of new data diminishes as sample size increases due to information overlap.</p> <p>The effective number of samples \\(E_n\\) for a class with \\(n\\) samples is defined as \\(E_n = (1 - \\beta^n) / (1 - \\beta)\\), where \\(\\beta \\in [0, 1)\\) is a hyperparameter. The CB loss weights the loss function \\(\\mathcal{L}\\) for a sample of class \\(y\\) by the inverse of its effective frequency:</p> \\[ \\mathcal{L}_{\\text{CB}}(\\mathbf{p}, y) = \\frac{1 - \\beta}{1 - \\beta^{n_y}} \\mathcal{L}(\\mathbf{p}, y) \\] <p>This up-weights the loss for rare classes, encouraging the model to learn their physics effectively despite the data sparsity.</p>"},{"location":"quickstart/","title":"Quickstart","text":"<p>Warning</p> <p>The repository is in a pre-alpha state and not ready for production use.</p> <p>A convenient PyPI package containing inference-only code with slimmed down dependencies will be released in the future.</p>"},{"location":"quickstart/#prerequisites","title":"Prerequisites","text":"<ul> <li>Git</li> <li>Python 3.10+</li> <li><code>uv</code> (highly recommended), or <code>pip</code></li> <li>Modern GPU for running <code>triton</code> kernels (this requirement will be lifted in the future)</li> <li>~10GB disk space for data.</li> </ul>"},{"location":"quickstart/#installation","title":"Installation","text":"<p>Clone the repository and sync the environment. We use <code>uv</code> to manage the virtual environment and dependencies.</p> <pre><code>git clone https://github.com/abc8747/microfuel\ncd microfuel\nuv sync --extras cli\n</code></pre>"},{"location":"quickstart/#codebase-layout","title":"Codebase Layout","text":"<p>Understanding the structure will help you navigate the commands:</p> <ul> <li><code>src/microfuel/</code>: The core library.<ul> <li><code>datasets/</code>: Logic for loading raw parquet files (<code>raw.py</code>) and generating features (<code>preprocessed.py</code>).</li> <li><code>model.py</code>: The GDN, Hypernetwork, and Loss functions.</li> <li><code>hacks.py</code>: JIT-compiled kernel patches for <code>fla</code> to support variable-length sequences without recompilation.</li> </ul> </li> <li><code>scripts/</code>: CLI entry points.<ul> <li><code>main.py</code>: The primary interface for all tasks (downloading, processing, training).</li> <li><code>plots.py</code>: Visualization tools for debugging and analysis.</li> </ul> </li> </ul>"},{"location":"quickstart/#step-by-step-reproduction","title":"Step-by-Step Reproduction","text":""},{"location":"quickstart/#1-data-ingestion","title":"1. Data Ingestion","text":"<p>The PRC Data Challenge 2025 data is hosted on S3. You will need to request a team creation and configure your access credentials in <code>data/config.toml</code> (create this file if it doesn't exist, see <code>data/config.example.toml</code>).</p> <pre><code># downloads raw parquet files to data/raw/\nuv run scripts/main.py download-raw\n</code></pre> <p>For consistency and to avoid confusion, we rename the data partitions:</p> <pre><code>cd data/raw\n\nmv flightlist_train.parquet flight_list_phase1.parquet\nmv fuel_train.parquet fuel_phase1.parquet\nmv flights_train flights_phase1\n\nmv flightlist_rank.parquet flight_list_phase1_rank.parquet\nmv fuel_rank_submission.parquet fuel_phase1_rank.parquet\nmv flights_rank flights_phase1_rank\n\nmv flightlist_final.parquet flight_list_phase2_rank.parquet\nmv fuel_train.parquet fuel_phase2_rank.parquet\nmv flights_final flights_phase2_rank\n</code></pre>"},{"location":"quickstart/#2-preprocessing","title":"2. Preprocessing","text":"<p>This step applies the Kalman Filter/RTS Smoother to the raw ADS-B points to generate clean inputs. It also generates the train/validation splits based on stratified sampling of aircraft types and flight durations.</p> <pre><code># smoothed trajectory vectors (heavy CPU usage, ~30 minutes)\nuv run scripts/main.py create-dataset --partition phase1\n\n# generate normalisation statistics (mean/std, ~5 minutes)\nuv run scripts/main.py create-stats --partition phase1\n\n# create stratified splits\nuv run scripts/main.py create-splits --partition phase1\n</code></pre>"},{"location":"quickstart/#3-training","title":"3. Training","text":"<p>Launch the training loop. The model uses <code>wandb</code> for logging.</p> <pre><code>uv run scripts/main.py train \\\n    --partition phase1 \\\n    --exp-name \"quickstart-gdn-v1\" \\\n    --batch-size 64 \\\n    --lr 4e-4 \\\n    --epochs 20 \\\n    --beta 0.999 \\\n    --loss-type rmse_kg\n</code></pre> <ul> <li><code>--beta</code>: The Class-Balanced Loss hyperparameter. Higher values (e.g., 0.999) heavily upweight rare aircraft types.</li> <li><code>--loss-type</code>: To avoid long tail distributions, the model outputs the average fuel burn rate over the segment instead of the total fuel burnt in that segment. <code>rmse_kg</code> effectively optimises against both for training stability.</li> </ul>"},{"location":"quickstart/#4-evaluation","title":"4. Evaluation","text":"<p>To evaluate a specific checkpoint on the validation set:</p> <pre><code>uv run scripts/main.py evaluate \\\n    data/checkpoints/quickstart-gdn-v1/step00500_123.45.pt \\\n    --partition phase1 \\\n    --split validation\n</code></pre> <p>This will generate a parquet file in <code>data/predictions/</code> containing ground truth vs. predicted values for analysis.</p>"}]}